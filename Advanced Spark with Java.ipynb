{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiHktwSQo3gMpgxkM1ra39",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KAYALAIMMANUELRAJU/PySpark1/blob/main/Advanced%20Spark%20with%20Java.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Case Study on Advanced Spark with Java\n",
        "Task 1:"
      ],
      "metadata": {
        "id": "tSDgVP0NEShb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8viT6tro874",
        "outputId": "5aabfe4e-73a3-4bed-a40d-f947f14e0231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: spark-3.3.2-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ],
      "source": [
        "# Install Java 11\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install Spark 3.3.2\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "\n",
        "# Install findspark to link Python and Spark\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "q5Cj6etppNf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "MqaMWRbupkBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/project/src/DataIngestionSetup.java\n",
        "import org.apache.spark.sql.*;\n",
        "public class DataIngestionSetup {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "            .appName(\"DataIngestionSetup\")\n",
        "            .master(\"local\")\n",
        "            .getOrCreate();\n",
        "        String[] tables = { \"customers\",\"employees\",\"offices\",\"orderdetails\",\n",
        "                            \"orders\",\"payments\",\"products\",\"productlines\" };\n",
        "        for (String t : tables) {\n",
        "            spark.read().option(\"header\",\"true\").csv(\"/content/data/csv/\"+t+\".csv\")\n",
        "                 .write().mode(\"overwrite\").parquet(\"/content/data/parquet/\"+t);\n",
        "            System.out.println(\"✅ \"+t);\n",
        "        }\n",
        "        spark.stop();\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UbLLX8iFsOg",
        "outputId": "b5dab0c6-8ccb-47b0-f3fb-45e258892c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/project/src/DataIngestionSetup.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/project/src\n",
        "!javac -cp \"/content/spark-3.3.2-bin-hadoop3/jars/*\" DataIngestionSetup.java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOBTODX4FzmK",
        "outputId": "72d6b39b-630a-4c65-a398-b40bb15909ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/project/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/spark-3.3.2-bin-hadoop3/bin/spark-submit \\\n",
        "  --class DataIngestionSetup --master local \\\n",
        "  --conf \"spark.driver.extraClassPath=/content/project/src:/content/spark-3.3.2-bin-hadoop3/jars/*\" \\\n",
        "  --jars /content/spark-3.3.2-bin-hadoop3/jars/* \\\n",
        "  DataIngestionSetup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Yg7R0blBF219",
        "outputId": "4d646251-038c-4c1e-e24d-94698f1894c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/08/06 06:35:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 06:35:26 INFO SparkContext: Running Spark version 3.3.2\n",
            "25/08/06 06:35:27 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 06:35:27 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 06:35:27 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 06:35:27 INFO SparkContext: Submitted application: DataIngestionSetup\n",
            "25/08/06 06:35:27 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 06:35:27 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 06:35:27 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 06:35:27 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 06:35:27 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 06:35:27 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 06:35:27 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 06:35:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "25/08/06 06:35:27 INFO Utils: Successfully started service 'sparkDriver' on port 39887.\n",
            "25/08/06 06:35:27 INFO SparkEnv: Registering MapOutputTracker\n",
            "25/08/06 06:35:28 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 06:35:28 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 06:35:28 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 06:35:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 06:35:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e234d3e3-845c-4c97-9ff5-281aceab707d\n",
            "25/08/06 06:35:28 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "25/08/06 06:35:28 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 06:35:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/06 06:35:28 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/06 06:35:28 INFO SparkContext: Added JAR file:///content/spark-3.3.2-bin-hadoop3/jars/activation-1.1.1.jar at spark://95a0e3787e8d:39887/jars/activation-1.1.1.jar with timestamp 1754462126973\n",
            "25/08/06 06:35:28 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/aircompressor-0.21.jar at spark://95a0e3787e8d:39887/jars/aircompressor-0.21.jar with timestamp 1754462126973\n",
            "25/08/06 06:35:28 INFO Executor: Starting executor ID driver on host 95a0e3787e8d\n",
            "25/08/06 06:35:28 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 06:35:28 INFO Executor: Fetching spark://95a0e3787e8d:39887/jars/aircompressor-0.21.jar with timestamp 1754462126973\n",
            "25/08/06 06:35:29 INFO TransportClientFactory: Successfully created connection to 95a0e3787e8d/172.28.0.12:39887 after 77 ms (0 ms spent in bootstraps)\n",
            "25/08/06 06:35:29 INFO Utils: Fetching spark://95a0e3787e8d:39887/jars/aircompressor-0.21.jar to /tmp/spark-9f3530e3-64a8-4d28-9934-759ead0b2162/userFiles-ed46268a-aa46-4833-8d7a-73a5fa50daf6/fetchFileTemp11210151690174565419.tmp\n",
            "25/08/06 06:35:29 INFO Executor: Adding file:/tmp/spark-9f3530e3-64a8-4d28-9934-759ead0b2162/userFiles-ed46268a-aa46-4833-8d7a-73a5fa50daf6/aircompressor-0.21.jar to class loader\n",
            "25/08/06 06:35:29 INFO Executor: Fetching spark://95a0e3787e8d:39887/jars/activation-1.1.1.jar with timestamp 1754462126973\n",
            "25/08/06 06:35:29 INFO Utils: Fetching spark://95a0e3787e8d:39887/jars/activation-1.1.1.jar to /tmp/spark-9f3530e3-64a8-4d28-9934-759ead0b2162/userFiles-ed46268a-aa46-4833-8d7a-73a5fa50daf6/fetchFileTemp4069697775057612362.tmp\n",
            "25/08/06 06:35:29 INFO Executor: Adding file:/tmp/spark-9f3530e3-64a8-4d28-9934-759ead0b2162/userFiles-ed46268a-aa46-4833-8d7a-73a5fa50daf6/activation-1.1.1.jar to class loader\n",
            "25/08/06 06:35:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39661.\n",
            "25/08/06 06:35:29 INFO NettyBlockTransferService: Server created on 95a0e3787e8d:39661\n",
            "25/08/06 06:35:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 06:35:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 95a0e3787e8d, 39661, None)\n",
            "25/08/06 06:35:29 INFO BlockManagerMasterEndpoint: Registering block manager 95a0e3787e8d:39661 with 434.4 MiB RAM, BlockManagerId(driver, 95a0e3787e8d, 39661, None)\n",
            "25/08/06 06:35:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 95a0e3787e8d, 39661, None)\n",
            "25/08/06 06:35:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 95a0e3787e8d, 39661, None)\n",
            "25/08/06 06:35:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 06:35:30 INFO SharedState: Warehouse path is 'file:/content/project/src/spark-warehouse'.\n",
            "25/08/06 06:35:33 INFO InMemoryFileIndex: It took 208 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:35:33 INFO InMemoryFileIndex: It took 10 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:35:38 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:38 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
            "25/08/06 06:35:38 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
            "25/08/06 06:35:39 INFO CodeGenerator: Code generated in 319.331051 ms\n",
            "25/08/06 06:35:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.9 KiB, free 434.2 MiB)\n",
            "25/08/06 06:35:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 434.2 MiB)\n",
            "25/08/06 06:35:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 95a0e3787e8d:39661 (size: 34.3 KiB, free: 434.4 MiB)\n",
            "25/08/06 06:35:39 INFO SparkContext: Created broadcast 0 from csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4208227 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:39 INFO SparkContext: Starting job: csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:39 INFO DAGScheduler: Got job 0 (csv at DataIngestionSetup.java:11) with 1 output partitions\n",
            "25/08/06 06:35:39 INFO DAGScheduler: Final stage: ResultStage 0 (csv at DataIngestionSetup.java:11)\n",
            "25/08/06 06:35:39 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:35:39 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:35:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at DataIngestionSetup.java:11), which has no missing parents\n",
            "25/08/06 06:35:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.8 KiB, free 434.2 MiB)\n",
            "25/08/06 06:35:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.2 MiB)\n",
            "25/08/06 06:35:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 95a0e3787e8d:39661 (size: 5.9 KiB, free: 434.4 MiB)\n",
            "25/08/06 06:35:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:35:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at DataIngestionSetup.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:35:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:35:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4909 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:35:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 06:35:40 INFO FileScanRDD: Reading File path: file:///content/data/csv/customers.csv, range: 0-13923, partition values: [empty row]\n",
            "25/08/06 06:35:40 INFO CodeGenerator: Code generated in 29.620315 ms\n",
            "25/08/06 06:35:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1692 bytes result sent to driver\n",
            "25/08/06 06:35:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 499 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:35:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:35:40 INFO DAGScheduler: ResultStage 0 (csv at DataIngestionSetup.java:11) finished in 0.800 s\n",
            "25/08/06 06:35:40 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:35:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 06:35:40 INFO DAGScheduler: Job 0 finished: csv at DataIngestionSetup.java:11, took 0.939325 s\n",
            "25/08/06 06:35:40 INFO CodeGenerator: Code generated in 38.201888 ms\n",
            "25/08/06 06:35:41 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:41 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:35:41 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
            "25/08/06 06:35:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.9 KiB, free 434.0 MiB)\n",
            "25/08/06 06:35:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.9 MiB)\n",
            "25/08/06 06:35:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 95a0e3787e8d:39661 (size: 34.3 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:41 INFO SparkContext: Created broadcast 2 from csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4208227 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:41 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:41 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:35:41 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string, customerName: string, contactLastName: string, contactFirstName: string, phone: string ... 11 more fields>\n",
            "25/08/06 06:35:41 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:41 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 199.7 KiB, free 433.7 MiB)\n",
            "25/08/06 06:35:41 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 433.7 MiB)\n",
            "25/08/06 06:35:41 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 95a0e3787e8d:39661 (size: 34.2 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:41 INFO SparkContext: Created broadcast 3 from parquet at DataIngestionSetup.java:12\n",
            "25/08/06 06:35:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4208227 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:41 INFO SparkContext: Starting job: parquet at DataIngestionSetup.java:12\n",
            "25/08/06 06:35:41 INFO DAGScheduler: Got job 1 (parquet at DataIngestionSetup.java:12) with 1 output partitions\n",
            "25/08/06 06:35:41 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at DataIngestionSetup.java:12)\n",
            "25/08/06 06:35:41 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:35:41 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:35:41 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at parquet at DataIngestionSetup.java:12), which has no missing parents\n",
            "25/08/06 06:35:41 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 213.4 KiB, free 433.5 MiB)\n",
            "25/08/06 06:35:41 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 76.9 KiB, free 433.4 MiB)\n",
            "25/08/06 06:35:41 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 95a0e3787e8d:39661 (size: 76.9 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:35:41 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:35:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at parquet at DataIngestionSetup.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:35:41 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:35:41 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4909 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:35:41 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 06:35:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:41 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:35:41 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:35:41 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
            "25/08/06 06:35:41 INFO ParquetOutputFormat: Validation is off\n",
            "25/08/06 06:35:41 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "25/08/06 06:35:41 INFO ParquetOutputFormat: Parquet properties are:\n",
            "Parquet page size to 1048576\n",
            "Parquet dictionary page size to 1048576\n",
            "Dictionary is true\n",
            "Writer version is: PARQUET_1_0\n",
            "Page size checking is: estimated\n",
            "Min row count for page size check is: 100\n",
            "Max row count for page size check is: 10000\n",
            "Truncate length for column indexes is: 64\n",
            "Truncate length for statistics min/max  is: 2147483647\n",
            "Bloom filter enabled: false\n",
            "Max Bloom filter size for a column is 1048576\n",
            "Bloom filter expected number of distinct values are: null\n",
            "Page row count limit to 20000\n",
            "Writing page checksums is: on\n",
            "25/08/06 06:35:42 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"customerName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"contactLastName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"contactFirstName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"phone\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine1\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine2\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"state\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"postalCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"salesRepEmployeeNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"creditLimit\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary customerNumber (STRING);\n",
            "  optional binary customerName (STRING);\n",
            "  optional binary contactLastName (STRING);\n",
            "  optional binary contactFirstName (STRING);\n",
            "  optional binary phone (STRING);\n",
            "  optional binary addressLine1 (STRING);\n",
            "  optional binary addressLine2 (STRING);\n",
            "  optional binary city (STRING);\n",
            "  optional binary state (STRING);\n",
            "  optional binary postalCode (STRING);\n",
            "  optional binary country (STRING);\n",
            "  optional binary salesRepEmployeeNumber (STRING);\n",
            "  optional binary creditLimit (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:35:42 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 06:35:42 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 95a0e3787e8d:39661 in memory (size: 34.3 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:42 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 95a0e3787e8d:39661 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:42 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 95a0e3787e8d:39661 in memory (size: 34.3 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:42 INFO FileScanRDD: Reading File path: file:///content/data/csv/customers.csv, range: 0-13923, partition values: [empty row]\n",
            "25/08/06 06:35:42 INFO CodeGenerator: Code generated in 34.424407 ms\n",
            "25/08/06 06:35:43 INFO FileOutputCommitter: Saved output of task 'attempt_202508060635417099097967231521683_0001_m_000000_1' to file:/content/data/parquet/customers/_temporary/0/task_202508060635417099097967231521683_0001_m_000000\n",
            "25/08/06 06:35:43 INFO SparkHadoopMapRedUtil: attempt_202508060635417099097967231521683_0001_m_000000_1: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 06:35:43 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2541 bytes result sent to driver\n",
            "25/08/06 06:35:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1621 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:35:43 INFO DAGScheduler: ResultStage 1 (parquet at DataIngestionSetup.java:12) finished in 1.708 s\n",
            "25/08/06 06:35:43 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:35:43 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:35:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 06:35:43 INFO DAGScheduler: Job 1 finished: parquet at DataIngestionSetup.java:12, took 1.720093 s\n",
            "25/08/06 06:35:43 INFO FileFormatWriter: Start to commit write Job 5b0fef50-b5af-47eb-bd8a-10778e7ffa20.\n",
            "25/08/06 06:35:43 INFO FileFormatWriter: Write Job 5b0fef50-b5af-47eb-bd8a-10778e7ffa20 committed. Elapsed time: 29 ms.\n",
            "25/08/06 06:35:43 INFO FileFormatWriter: Finished processing stats for write job 5b0fef50-b5af-47eb-bd8a-10778e7ffa20.\n",
            "✅ customers\n",
            "25/08/06 06:35:43 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:35:43 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:35:43 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:43 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#69, None)) > 0)\n",
            "25/08/06 06:35:43 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
            "25/08/06 06:35:43 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 199.9 KiB, free 433.7 MiB)\n",
            "25/08/06 06:35:43 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.7 MiB)\n",
            "25/08/06 06:35:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 95a0e3787e8d:39661 (size: 34.3 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:43 INFO SparkContext: Created broadcast 5 from csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196085 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:43 INFO SparkContext: Starting job: csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:43 INFO DAGScheduler: Got job 2 (csv at DataIngestionSetup.java:11) with 1 output partitions\n",
            "25/08/06 06:35:43 INFO DAGScheduler: Final stage: ResultStage 2 (csv at DataIngestionSetup.java:11)\n",
            "25/08/06 06:35:43 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:35:43 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:35:43 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at csv at DataIngestionSetup.java:11), which has no missing parents\n",
            "25/08/06 06:35:43 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.8 KiB, free 433.6 MiB)\n",
            "25/08/06 06:35:43 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.6 MiB)\n",
            "25/08/06 06:35:43 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 95a0e3787e8d:39661 (size: 5.9 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:43 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:35:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at csv at DataIngestionSetup.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:35:43 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:35:43 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4909 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:35:43 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 06:35:43 INFO FileScanRDD: Reading File path: file:///content/data/csv/employees.csv, range: 0-1781, partition values: [empty row]\n",
            "25/08/06 06:35:43 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1595 bytes result sent to driver\n",
            "25/08/06 06:35:43 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 29 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:35:43 INFO DAGScheduler: ResultStage 2 (csv at DataIngestionSetup.java:11) finished in 0.050 s\n",
            "25/08/06 06:35:43 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:35:43 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:35:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 06:35:43 INFO DAGScheduler: Job 2 finished: csv at DataIngestionSetup.java:11, took 0.065048 s\n",
            "25/08/06 06:35:43 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:43 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:35:43 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
            "25/08/06 06:35:43 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 199.9 KiB, free 433.4 MiB)\n",
            "25/08/06 06:35:43 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.4 MiB)\n",
            "25/08/06 06:35:43 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 95a0e3787e8d:39661 (size: 34.3 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:35:43 INFO SparkContext: Created broadcast 7 from csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196085 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:44 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:44 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:35:44 INFO FileSourceStrategy: Output Data Schema: struct<employeeNumber: string, lastName: string, firstName: string, extension: string, email: string ... 6 more fields>\n",
            "25/08/06 06:35:44 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:44 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 199.7 KiB, free 433.2 MiB)\n",
            "25/08/06 06:35:44 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 433.2 MiB)\n",
            "25/08/06 06:35:44 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 95a0e3787e8d:39661 (size: 34.2 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:35:44 INFO SparkContext: Created broadcast 8 from parquet at DataIngestionSetup.java:12\n",
            "25/08/06 06:35:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196085 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:44 INFO SparkContext: Starting job: parquet at DataIngestionSetup.java:12\n",
            "25/08/06 06:35:44 INFO DAGScheduler: Got job 3 (parquet at DataIngestionSetup.java:12) with 1 output partitions\n",
            "25/08/06 06:35:44 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at DataIngestionSetup.java:12)\n",
            "25/08/06 06:35:44 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:35:44 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:35:44 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[23] at parquet at DataIngestionSetup.java:12), which has no missing parents\n",
            "25/08/06 06:35:44 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 212.2 KiB, free 433.0 MiB)\n",
            "25/08/06 06:35:44 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 76.6 KiB, free 432.9 MiB)\n",
            "25/08/06 06:35:44 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 95a0e3787e8d:39661 (size: 76.6 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:35:44 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:35:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[23] at parquet at DataIngestionSetup.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:35:44 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:35:44 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4909 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:35:44 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 06:35:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:44 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:35:44 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:35:44 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
            "25/08/06 06:35:44 INFO ParquetOutputFormat: Validation is off\n",
            "25/08/06 06:35:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "25/08/06 06:35:44 INFO ParquetOutputFormat: Parquet properties are:\n",
            "Parquet page size to 1048576\n",
            "Parquet dictionary page size to 1048576\n",
            "Dictionary is true\n",
            "Writer version is: PARQUET_1_0\n",
            "Page size checking is: estimated\n",
            "Min row count for page size check is: 100\n",
            "Max row count for page size check is: 10000\n",
            "Truncate length for column indexes is: 64\n",
            "Truncate length for statistics min/max  is: 2147483647\n",
            "Bloom filter enabled: false\n",
            "Max Bloom filter size for a column is 1048576\n",
            "Bloom filter expected number of distinct values are: null\n",
            "Page row count limit to 20000\n",
            "Writing page checksums is: on\n",
            "25/08/06 06:35:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"employeeNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"lastName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"firstName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"extension\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"email\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"reportsTo\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"jobTitle\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary employeeNumber (STRING);\n",
            "  optional binary lastName (STRING);\n",
            "  optional binary firstName (STRING);\n",
            "  optional binary extension (STRING);\n",
            "  optional binary email (STRING);\n",
            "  optional binary officeCode (STRING);\n",
            "  optional binary reportsTo (STRING);\n",
            "  optional binary jobTitle (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:35:44 INFO FileScanRDD: Reading File path: file:///content/data/csv/employees.csv, range: 0-1781, partition values: [empty row]\n",
            "25/08/06 06:35:44 INFO CodeGenerator: Code generated in 91.565476 ms\n",
            "25/08/06 06:35:44 INFO FileOutputCommitter: Saved output of task 'attempt_202508060635446028574972537247500_0003_m_000000_3' to file:/content/data/parquet/employees/_temporary/0/task_202508060635446028574972537247500_0003_m_000000\n",
            "25/08/06 06:35:44 INFO SparkHadoopMapRedUtil: attempt_202508060635446028574972537247500_0003_m_000000_3: Committed. Elapsed time: 4 ms.\n",
            "25/08/06 06:35:44 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2455 bytes result sent to driver\n",
            "25/08/06 06:35:44 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 362 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:35:44 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:35:44 INFO DAGScheduler: ResultStage 3 (parquet at DataIngestionSetup.java:12) finished in 0.431 s\n",
            "25/08/06 06:35:44 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:35:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 06:35:44 INFO DAGScheduler: Job 3 finished: parquet at DataIngestionSetup.java:12, took 0.455075 s\n",
            "25/08/06 06:35:44 INFO FileFormatWriter: Start to commit write Job 6bcbe73f-704b-4d73-a3c7-235a5e5f0d1b.\n",
            "25/08/06 06:35:44 INFO FileFormatWriter: Write Job 6bcbe73f-704b-4d73-a3c7-235a5e5f0d1b committed. Elapsed time: 38 ms.\n",
            "25/08/06 06:35:44 INFO FileFormatWriter: Finished processing stats for write job 6bcbe73f-704b-4d73-a3c7-235a5e5f0d1b.\n",
            "✅ employees\n",
            "25/08/06 06:35:44 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:35:44 INFO InMemoryFileIndex: It took 11 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:35:45 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:45 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#118, None)) > 0)\n",
            "25/08/06 06:35:45 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
            "25/08/06 06:35:45 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 199.9 KiB, free 432.7 MiB)\n",
            "25/08/06 06:35:45 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 432.7 MiB)\n",
            "25/08/06 06:35:45 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 95a0e3787e8d:39661 (size: 34.3 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:35:45 INFO SparkContext: Created broadcast 10 from csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194889 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:45 INFO SparkContext: Starting job: csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:45 INFO DAGScheduler: Got job 4 (csv at DataIngestionSetup.java:11) with 1 output partitions\n",
            "25/08/06 06:35:45 INFO DAGScheduler: Final stage: ResultStage 4 (csv at DataIngestionSetup.java:11)\n",
            "25/08/06 06:35:45 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:35:45 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:35:45 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[27] at csv at DataIngestionSetup.java:11), which has no missing parents\n",
            "25/08/06 06:35:45 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 11.8 KiB, free 432.7 MiB)\n",
            "25/08/06 06:35:45 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 432.7 MiB)\n",
            "25/08/06 06:35:45 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 95a0e3787e8d:39661 (size: 5.9 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:35:45 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:35:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[27] at csv at DataIngestionSetup.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:35:45 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:35:45 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4907 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:35:45 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 06:35:45 INFO FileScanRDD: Reading File path: file:///content/data/csv/offices.csv, range: 0-585, partition values: [empty row]\n",
            "25/08/06 06:35:45 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1640 bytes result sent to driver\n",
            "25/08/06 06:35:45 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 157 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:35:45 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:35:45 INFO DAGScheduler: ResultStage 4 (csv at DataIngestionSetup.java:11) finished in 0.213 s\n",
            "25/08/06 06:35:45 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:35:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/06 06:35:45 INFO DAGScheduler: Job 4 finished: csv at DataIngestionSetup.java:11, took 0.223178 s\n",
            "25/08/06 06:35:45 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 95a0e3787e8d:39661 in memory (size: 76.6 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:35:45 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 95a0e3787e8d:39661 in memory (size: 76.9 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:35:45 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:45 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:35:45 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
            "25/08/06 06:35:45 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 199.9 KiB, free 433.0 MiB)\n",
            "25/08/06 06:35:45 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 95a0e3787e8d:39661 in memory (size: 34.2 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:45 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.2 MiB)\n",
            "25/08/06 06:35:45 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 95a0e3787e8d:39661 (size: 34.3 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:35:45 INFO SparkContext: Created broadcast 12 from csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194889 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:45 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 95a0e3787e8d:39661 in memory (size: 34.3 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:45 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 95a0e3787e8d:39661 in memory (size: 34.3 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:45 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 95a0e3787e8d:39661 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:45 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 95a0e3787e8d:39661 in memory (size: 34.2 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:45 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:45 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:35:45 INFO FileSourceStrategy: Output Data Schema: struct<officeCode: string, city: string, phone: string, addressLine1: string, addressLine2: string ... 7 more fields>\n",
            "25/08/06 06:35:45 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:45 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 199.7 KiB, free 433.7 MiB)\n",
            "25/08/06 06:35:45 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 433.7 MiB)\n",
            "25/08/06 06:35:45 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 95a0e3787e8d:39661 (size: 34.2 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:45 INFO SparkContext: Created broadcast 13 from parquet at DataIngestionSetup.java:12\n",
            "25/08/06 06:35:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194889 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:45 INFO SparkContext: Starting job: parquet at DataIngestionSetup.java:12\n",
            "25/08/06 06:35:45 INFO DAGScheduler: Got job 5 (parquet at DataIngestionSetup.java:12) with 1 output partitions\n",
            "25/08/06 06:35:45 INFO DAGScheduler: Final stage: ResultStage 5 (parquet at DataIngestionSetup.java:12)\n",
            "25/08/06 06:35:45 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:35:45 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:35:45 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[35] at parquet at DataIngestionSetup.java:12), which has no missing parents\n",
            "25/08/06 06:35:46 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 212.4 KiB, free 433.5 MiB)\n",
            "25/08/06 06:35:46 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 76.7 KiB, free 433.4 MiB)\n",
            "25/08/06 06:35:46 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 95a0e3787e8d:39661 (size: 76.7 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:35:46 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:35:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[35] at parquet at DataIngestionSetup.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:35:46 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:35:46 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4907 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:35:46 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "25/08/06 06:35:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:46 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:35:46 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:35:46 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
            "25/08/06 06:35:46 INFO ParquetOutputFormat: Validation is off\n",
            "25/08/06 06:35:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "25/08/06 06:35:46 INFO ParquetOutputFormat: Parquet properties are:\n",
            "Parquet page size to 1048576\n",
            "Parquet dictionary page size to 1048576\n",
            "Dictionary is true\n",
            "Writer version is: PARQUET_1_0\n",
            "Page size checking is: estimated\n",
            "Min row count for page size check is: 100\n",
            "Max row count for page size check is: 10000\n",
            "Truncate length for column indexes is: 64\n",
            "Truncate length for statistics min/max  is: 2147483647\n",
            "Bloom filter enabled: false\n",
            "Max Bloom filter size for a column is 1048576\n",
            "Bloom filter expected number of distinct values are: null\n",
            "Page row count limit to 20000\n",
            "Writing page checksums is: on\n",
            "25/08/06 06:35:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"phone\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine1\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"addressLine2\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"state\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"postalCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"territory\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary officeCode (STRING);\n",
            "  optional binary city (STRING);\n",
            "  optional binary phone (STRING);\n",
            "  optional binary addressLine1 (STRING);\n",
            "  optional binary addressLine2 (STRING);\n",
            "  optional binary state (STRING);\n",
            "  optional binary country (STRING);\n",
            "  optional binary postalCode (STRING);\n",
            "  optional binary territory (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:35:46 INFO FileScanRDD: Reading File path: file:///content/data/csv/offices.csv, range: 0-585, partition values: [empty row]\n",
            "25/08/06 06:35:46 INFO CodeGenerator: Code generated in 68.047785 ms\n",
            "25/08/06 06:35:46 INFO FileOutputCommitter: Saved output of task 'attempt_202508060635458505096013991515318_0005_m_000000_5' to file:/content/data/parquet/offices/_temporary/0/task_202508060635458505096013991515318_0005_m_000000\n",
            "25/08/06 06:35:46 INFO SparkHadoopMapRedUtil: attempt_202508060635458505096013991515318_0005_m_000000_5: Committed. Elapsed time: 2 ms.\n",
            "25/08/06 06:35:46 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2455 bytes result sent to driver\n",
            "25/08/06 06:35:46 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 355 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:35:46 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:35:46 INFO DAGScheduler: ResultStage 5 (parquet at DataIngestionSetup.java:12) finished in 0.462 s\n",
            "25/08/06 06:35:46 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:35:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 06:35:46 INFO DAGScheduler: Job 5 finished: parquet at DataIngestionSetup.java:12, took 0.480655 s\n",
            "25/08/06 06:35:46 INFO FileFormatWriter: Start to commit write Job 096e10a6-45e4-4b80-af85-db903bfe6131.\n",
            "25/08/06 06:35:46 INFO FileFormatWriter: Write Job 096e10a6-45e4-4b80-af85-db903bfe6131 committed. Elapsed time: 26 ms.\n",
            "25/08/06 06:35:46 INFO FileFormatWriter: Finished processing stats for write job 096e10a6-45e4-4b80-af85-db903bfe6131.\n",
            "✅ offices\n",
            "25/08/06 06:35:46 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:35:46 INFO InMemoryFileIndex: It took 9 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:35:46 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:46 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#171, None)) > 0)\n",
            "25/08/06 06:35:46 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
            "25/08/06 06:35:46 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 199.9 KiB, free 433.2 MiB)\n",
            "25/08/06 06:35:46 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.2 MiB)\n",
            "25/08/06 06:35:46 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 95a0e3787e8d:39661 (size: 34.3 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:35:46 INFO SparkContext: Created broadcast 15 from csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4274007 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:47 INFO SparkContext: Starting job: csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:47 INFO DAGScheduler: Got job 6 (csv at DataIngestionSetup.java:11) with 1 output partitions\n",
            "25/08/06 06:35:47 INFO DAGScheduler: Final stage: ResultStage 6 (csv at DataIngestionSetup.java:11)\n",
            "25/08/06 06:35:47 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:35:47 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:35:47 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[39] at csv at DataIngestionSetup.java:11), which has no missing parents\n",
            "25/08/06 06:35:47 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 11.8 KiB, free 433.2 MiB)\n",
            "25/08/06 06:35:47 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.2 MiB)\n",
            "25/08/06 06:35:47 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 95a0e3787e8d:39661 (size: 5.9 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:35:47 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:35:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[39] at csv at DataIngestionSetup.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:35:47 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:35:47 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4912 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:35:47 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "25/08/06 06:35:47 INFO FileScanRDD: Reading File path: file:///content/data/csv/orderdetails.csv, range: 0-79703, partition values: [empty row]\n",
            "25/08/06 06:35:47 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1584 bytes result sent to driver\n",
            "25/08/06 06:35:47 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 56 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:35:47 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:35:47 INFO DAGScheduler: ResultStage 6 (csv at DataIngestionSetup.java:11) finished in 0.087 s\n",
            "25/08/06 06:35:47 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:35:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 06:35:47 INFO DAGScheduler: Job 6 finished: csv at DataIngestionSetup.java:11, took 0.097054 s\n",
            "25/08/06 06:35:47 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:47 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:35:47 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
            "25/08/06 06:35:47 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 199.9 KiB, free 433.0 MiB)\n",
            "25/08/06 06:35:47 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 432.9 MiB)\n",
            "25/08/06 06:35:47 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 95a0e3787e8d:39661 (size: 34.3 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:35:47 INFO SparkContext: Created broadcast 17 from csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4274007 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:47 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:47 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:35:47 INFO FileSourceStrategy: Output Data Schema: struct<orderNumber: string, productCode: string, quantityOrdered: string, priceEach: string, orderLineNumber: string ... 3 more fields>\n",
            "25/08/06 06:35:47 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:47 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 199.7 KiB, free 432.7 MiB)\n",
            "25/08/06 06:35:47 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 432.7 MiB)\n",
            "25/08/06 06:35:47 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 95a0e3787e8d:39661 (size: 34.2 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:35:47 INFO SparkContext: Created broadcast 18 from parquet at DataIngestionSetup.java:12\n",
            "25/08/06 06:35:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4274007 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:47 INFO SparkContext: Starting job: parquet at DataIngestionSetup.java:12\n",
            "25/08/06 06:35:47 INFO DAGScheduler: Got job 7 (parquet at DataIngestionSetup.java:12) with 1 output partitions\n",
            "25/08/06 06:35:47 INFO DAGScheduler: Final stage: ResultStage 7 (parquet at DataIngestionSetup.java:12)\n",
            "25/08/06 06:35:47 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:35:47 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:35:47 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[47] at parquet at DataIngestionSetup.java:12), which has no missing parents\n",
            "25/08/06 06:35:47 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 211.5 KiB, free 432.5 MiB)\n",
            "25/08/06 06:35:47 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 76.6 KiB, free 432.4 MiB)\n",
            "25/08/06 06:35:47 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 95a0e3787e8d:39661 (size: 76.6 KiB, free: 434.0 MiB)\n",
            "25/08/06 06:35:47 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:35:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[47] at parquet at DataIngestionSetup.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:35:47 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:35:47 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4912 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:35:47 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
            "25/08/06 06:35:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:47 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:35:47 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:35:47 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
            "25/08/06 06:35:47 INFO ParquetOutputFormat: Validation is off\n",
            "25/08/06 06:35:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "25/08/06 06:35:47 INFO ParquetOutputFormat: Parquet properties are:\n",
            "Parquet page size to 1048576\n",
            "Parquet dictionary page size to 1048576\n",
            "Dictionary is true\n",
            "Writer version is: PARQUET_1_0\n",
            "Page size checking is: estimated\n",
            "Min row count for page size check is: 100\n",
            "Max row count for page size check is: 10000\n",
            "Truncate length for column indexes is: 64\n",
            "Truncate length for statistics min/max  is: 2147483647\n",
            "Bloom filter enabled: false\n",
            "Max Bloom filter size for a column is 1048576\n",
            "Bloom filter expected number of distinct values are: null\n",
            "Page row count limit to 20000\n",
            "Writing page checksums is: on\n",
            "25/08/06 06:35:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"orderNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"quantityOrdered\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"priceEach\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"orderLineNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary orderNumber (STRING);\n",
            "  optional binary productCode (STRING);\n",
            "  optional binary quantityOrdered (STRING);\n",
            "  optional binary priceEach (STRING);\n",
            "  optional binary orderLineNumber (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:35:47 INFO FileScanRDD: Reading File path: file:///content/data/csv/orderdetails.csv, range: 0-79703, partition values: [empty row]\n",
            "25/08/06 06:35:47 INFO CodeGenerator: Code generated in 27.399671 ms\n",
            "25/08/06 06:35:48 INFO FileOutputCommitter: Saved output of task 'attempt_202508060635473411473673723636627_0007_m_000000_7' to file:/content/data/parquet/orderdetails/_temporary/0/task_202508060635473411473673723636627_0007_m_000000\n",
            "25/08/06 06:35:48 INFO SparkHadoopMapRedUtil: attempt_202508060635473411473673723636627_0007_m_000000_7: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 06:35:48 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2455 bytes result sent to driver\n",
            "25/08/06 06:35:48 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 441 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:35:48 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:35:48 INFO DAGScheduler: ResultStage 7 (parquet at DataIngestionSetup.java:12) finished in 0.525 s\n",
            "25/08/06 06:35:48 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:35:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/08/06 06:35:48 INFO DAGScheduler: Job 7 finished: parquet at DataIngestionSetup.java:12, took 0.529454 s\n",
            "25/08/06 06:35:48 INFO FileFormatWriter: Start to commit write Job e8e27866-443c-4954-a986-105b52d444f6.\n",
            "25/08/06 06:35:48 INFO FileFormatWriter: Write Job e8e27866-443c-4954-a986-105b52d444f6 committed. Elapsed time: 42 ms.\n",
            "25/08/06 06:35:48 INFO FileFormatWriter: Finished processing stats for write job e8e27866-443c-4954-a986-105b52d444f6.\n",
            "✅ orderdetails\n",
            "25/08/06 06:35:48 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:35:48 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:35:48 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:48 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#208, None)) > 0)\n",
            "25/08/06 06:35:48 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
            "25/08/06 06:35:48 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 199.9 KiB, free 432.2 MiB)\n",
            "25/08/06 06:35:48 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 432.2 MiB)\n",
            "25/08/06 06:35:48 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 95a0e3787e8d:39661 (size: 34.3 KiB, free: 434.0 MiB)\n",
            "25/08/06 06:35:48 INFO SparkContext: Created broadcast 20 from csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4217852 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:48 INFO SparkContext: Starting job: csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:48 INFO DAGScheduler: Got job 8 (csv at DataIngestionSetup.java:11) with 1 output partitions\n",
            "25/08/06 06:35:48 INFO DAGScheduler: Final stage: ResultStage 8 (csv at DataIngestionSetup.java:11)\n",
            "25/08/06 06:35:48 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:35:48 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:35:48 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[51] at csv at DataIngestionSetup.java:11), which has no missing parents\n",
            "25/08/06 06:35:48 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 11.8 KiB, free 432.2 MiB)\n",
            "25/08/06 06:35:48 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 432.2 MiB)\n",
            "25/08/06 06:35:48 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 95a0e3787e8d:39661 (size: 5.9 KiB, free: 434.0 MiB)\n",
            "25/08/06 06:35:48 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:35:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[51] at csv at DataIngestionSetup.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:35:48 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:35:48 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4906 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:35:48 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
            "25/08/06 06:35:48 INFO FileScanRDD: Reading File path: file:///content/data/csv/orders.csv, range: 0-23548, partition values: [empty row]\n",
            "25/08/06 06:35:48 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 1585 bytes result sent to driver\n",
            "25/08/06 06:35:48 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 46 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:35:48 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:35:48 INFO DAGScheduler: ResultStage 8 (csv at DataIngestionSetup.java:11) finished in 0.092 s\n",
            "25/08/06 06:35:48 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:35:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
            "25/08/06 06:35:48 INFO DAGScheduler: Job 8 finished: csv at DataIngestionSetup.java:11, took 0.104577 s\n",
            "25/08/06 06:35:48 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:48 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:35:48 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
            "25/08/06 06:35:48 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 199.9 KiB, free 432.0 MiB)\n",
            "25/08/06 06:35:48 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 432.0 MiB)\n",
            "25/08/06 06:35:48 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 95a0e3787e8d:39661 (size: 34.3 KiB, free: 434.0 MiB)\n",
            "25/08/06 06:35:48 INFO SparkContext: Created broadcast 22 from csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4217852 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:48 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:48 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:35:48 INFO FileSourceStrategy: Output Data Schema: struct<orderNumber: string, orderDate: string, requiredDate: string, shippedDate: string, status: string ... 5 more fields>\n",
            "25/08/06 06:35:48 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:48 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 199.7 KiB, free 431.8 MiB)\n",
            "25/08/06 06:35:48 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 431.7 MiB)\n",
            "25/08/06 06:35:48 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 95a0e3787e8d:39661 (size: 34.2 KiB, free: 433.9 MiB)\n",
            "25/08/06 06:35:48 INFO SparkContext: Created broadcast 23 from parquet at DataIngestionSetup.java:12\n",
            "25/08/06 06:35:48 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 95a0e3787e8d:39661 in memory (size: 34.2 KiB, free: 434.0 MiB)\n",
            "25/08/06 06:35:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4217852 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:49 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 95a0e3787e8d:39661 in memory (size: 5.9 KiB, free: 434.0 MiB)\n",
            "25/08/06 06:35:49 INFO SparkContext: Starting job: parquet at DataIngestionSetup.java:12\n",
            "25/08/06 06:35:49 INFO DAGScheduler: Got job 9 (parquet at DataIngestionSetup.java:12) with 1 output partitions\n",
            "25/08/06 06:35:49 INFO DAGScheduler: Final stage: ResultStage 9 (parquet at DataIngestionSetup.java:12)\n",
            "25/08/06 06:35:49 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:35:49 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:35:49 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[59] at parquet at DataIngestionSetup.java:12), which has no missing parents\n",
            "25/08/06 06:35:49 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 95a0e3787e8d:39661 in memory (size: 34.3 KiB, free: 434.0 MiB)\n",
            "25/08/06 06:35:49 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 212.0 KiB, free 432.0 MiB)\n",
            "25/08/06 06:35:49 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 76.5 KiB, free 431.9 MiB)\n",
            "25/08/06 06:35:49 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 95a0e3787e8d:39661 (size: 76.5 KiB, free: 433.9 MiB)\n",
            "25/08/06 06:35:49 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:35:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[59] at parquet at DataIngestionSetup.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:35:49 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:35:49 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4906 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:35:49 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
            "25/08/06 06:35:49 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 95a0e3787e8d:39661 in memory (size: 76.7 KiB, free: 434.0 MiB)\n",
            "25/08/06 06:35:49 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 95a0e3787e8d:39661 in memory (size: 34.3 KiB, free: 434.0 MiB)\n",
            "25/08/06 06:35:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:49 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:35:49 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:35:49 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
            "25/08/06 06:35:49 INFO ParquetOutputFormat: Validation is off\n",
            "25/08/06 06:35:49 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "25/08/06 06:35:49 INFO ParquetOutputFormat: Parquet properties are:\n",
            "Parquet page size to 1048576\n",
            "Parquet dictionary page size to 1048576\n",
            "Dictionary is true\n",
            "Writer version is: PARQUET_1_0\n",
            "Page size checking is: estimated\n",
            "Min row count for page size check is: 100\n",
            "Max row count for page size check is: 10000\n",
            "Truncate length for column indexes is: 64\n",
            "Truncate length for statistics min/max  is: 2147483647\n",
            "Bloom filter enabled: false\n",
            "Max Bloom filter size for a column is 1048576\n",
            "Bloom filter expected number of distinct values are: null\n",
            "Page row count limit to 20000\n",
            "Writing page checksums is: on\n",
            "25/08/06 06:35:49 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"orderNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"orderDate\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"requiredDate\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"shippedDate\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"status\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"comments\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary orderNumber (STRING);\n",
            "  optional binary orderDate (STRING);\n",
            "  optional binary requiredDate (STRING);\n",
            "  optional binary shippedDate (STRING);\n",
            "  optional binary status (STRING);\n",
            "  optional binary comments (STRING);\n",
            "  optional binary customerNumber (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:35:49 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 95a0e3787e8d:39661 in memory (size: 76.6 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:35:49 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 95a0e3787e8d:39661 in memory (size: 34.3 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:35:49 INFO FileScanRDD: Reading File path: file:///content/data/csv/orders.csv, range: 0-23548, partition values: [empty row]\n",
            "25/08/06 06:35:49 INFO CodeGenerator: Code generated in 17.809877 ms\n",
            "25/08/06 06:35:49 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 95a0e3787e8d:39661 in memory (size: 5.9 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:35:49 INFO FileOutputCommitter: Saved output of task 'attempt_202508060635488784526738209811194_0009_m_000000_9' to file:/content/data/parquet/orders/_temporary/0/task_202508060635488784526738209811194_0009_m_000000\n",
            "25/08/06 06:35:49 INFO SparkHadoopMapRedUtil: attempt_202508060635488784526738209811194_0009_m_000000_9: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 06:35:49 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2498 bytes result sent to driver\n",
            "25/08/06 06:35:49 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 213 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:35:49 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:35:49 INFO DAGScheduler: ResultStage 9 (parquet at DataIngestionSetup.java:12) finished in 0.320 s\n",
            "25/08/06 06:35:49 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:35:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "25/08/06 06:35:49 INFO DAGScheduler: Job 9 finished: parquet at DataIngestionSetup.java:12, took 0.329711 s\n",
            "25/08/06 06:35:49 INFO FileFormatWriter: Start to commit write Job f66bb281-c866-4447-be71-9e6d2a07480c.\n",
            "25/08/06 06:35:49 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 95a0e3787e8d:39661 in memory (size: 34.3 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:35:49 INFO FileFormatWriter: Write Job f66bb281-c866-4447-be71-9e6d2a07480c committed. Elapsed time: 17 ms.\n",
            "25/08/06 06:35:49 INFO FileFormatWriter: Finished processing stats for write job f66bb281-c866-4447-be71-9e6d2a07480c.\n",
            "✅ orders\n",
            "25/08/06 06:35:49 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 95a0e3787e8d:39661 in memory (size: 34.3 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:35:49 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:35:49 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 95a0e3787e8d:39661 in memory (size: 34.3 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:49 INFO InMemoryFileIndex: It took 12 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:35:49 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 95a0e3787e8d:39661 in memory (size: 34.2 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:49 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 95a0e3787e8d:39661 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:49 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:49 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#253, None)) > 0)\n",
            "25/08/06 06:35:49 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
            "25/08/06 06:35:49 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 199.9 KiB, free 433.7 MiB)\n",
            "25/08/06 06:35:49 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.7 MiB)\n",
            "25/08/06 06:35:49 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 95a0e3787e8d:39661 (size: 34.3 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:49 INFO SparkContext: Created broadcast 25 from csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203272 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:49 INFO SparkContext: Starting job: csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:49 INFO DAGScheduler: Got job 10 (csv at DataIngestionSetup.java:11) with 1 output partitions\n",
            "25/08/06 06:35:49 INFO DAGScheduler: Final stage: ResultStage 10 (csv at DataIngestionSetup.java:11)\n",
            "25/08/06 06:35:49 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:35:49 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:35:49 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[63] at csv at DataIngestionSetup.java:11), which has no missing parents\n",
            "25/08/06 06:35:49 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 11.8 KiB, free 433.6 MiB)\n",
            "25/08/06 06:35:49 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.6 MiB)\n",
            "25/08/06 06:35:49 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 95a0e3787e8d:39661 (size: 5.9 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:49 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:35:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[63] at csv at DataIngestionSetup.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:35:49 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:35:49 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:35:49 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)\n",
            "25/08/06 06:35:49 INFO FileScanRDD: Reading File path: file:///content/data/csv/payments.csv, range: 0-8968, partition values: [empty row]\n",
            "25/08/06 06:35:49 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 1563 bytes result sent to driver\n",
            "25/08/06 06:35:49 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 17 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:35:49 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:35:49 INFO DAGScheduler: ResultStage 10 (csv at DataIngestionSetup.java:11) finished in 0.042 s\n",
            "25/08/06 06:35:49 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:35:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
            "25/08/06 06:35:49 INFO DAGScheduler: Job 10 finished: csv at DataIngestionSetup.java:11, took 0.047619 s\n",
            "25/08/06 06:35:49 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:49 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:35:49 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
            "25/08/06 06:35:49 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 199.9 KiB, free 433.4 MiB)\n",
            "25/08/06 06:35:49 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.4 MiB)\n",
            "25/08/06 06:35:49 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 95a0e3787e8d:39661 (size: 34.3 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:35:49 INFO SparkContext: Created broadcast 27 from csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203272 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:49 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:49 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:35:49 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string, checkNumber: string, paymentDate: string, amount: string ... 2 more fields>\n",
            "25/08/06 06:35:49 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:49 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 199.7 KiB, free 433.2 MiB)\n",
            "25/08/06 06:35:49 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 433.2 MiB)\n",
            "25/08/06 06:35:49 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 95a0e3787e8d:39661 (size: 34.2 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:35:49 INFO SparkContext: Created broadcast 28 from parquet at DataIngestionSetup.java:12\n",
            "25/08/06 06:35:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203272 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:50 INFO SparkContext: Starting job: parquet at DataIngestionSetup.java:12\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Got job 11 (parquet at DataIngestionSetup.java:12) with 1 output partitions\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Final stage: ResultStage 11 (parquet at DataIngestionSetup.java:12)\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[71] at parquet at DataIngestionSetup.java:12), which has no missing parents\n",
            "25/08/06 06:35:50 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 211.3 KiB, free 433.0 MiB)\n",
            "25/08/06 06:35:50 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 76.5 KiB, free 432.9 MiB)\n",
            "25/08/06 06:35:50 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 95a0e3787e8d:39661 (size: 76.5 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:35:50 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[71] at parquet at DataIngestionSetup.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:35:50 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:35:50 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:35:50 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)\n",
            "25/08/06 06:35:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:50 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:35:50 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:35:50 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
            "25/08/06 06:35:50 INFO ParquetOutputFormat: Validation is off\n",
            "25/08/06 06:35:50 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "25/08/06 06:35:50 INFO ParquetOutputFormat: Parquet properties are:\n",
            "Parquet page size to 1048576\n",
            "Parquet dictionary page size to 1048576\n",
            "Dictionary is true\n",
            "Writer version is: PARQUET_1_0\n",
            "Page size checking is: estimated\n",
            "Min row count for page size check is: 100\n",
            "Max row count for page size check is: 10000\n",
            "Truncate length for column indexes is: 64\n",
            "Truncate length for statistics min/max  is: 2147483647\n",
            "Bloom filter enabled: false\n",
            "Max Bloom filter size for a column is 1048576\n",
            "Bloom filter expected number of distinct values are: null\n",
            "Page row count limit to 20000\n",
            "Writing page checksums is: on\n",
            "25/08/06 06:35:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"checkNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"paymentDate\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"amount\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary customerNumber (STRING);\n",
            "  optional binary checkNumber (STRING);\n",
            "  optional binary paymentDate (STRING);\n",
            "  optional binary amount (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:35:50 INFO FileScanRDD: Reading File path: file:///content/data/csv/payments.csv, range: 0-8968, partition values: [empty row]\n",
            "25/08/06 06:35:50 INFO CodeGenerator: Code generated in 23.639421 ms\n",
            "25/08/06 06:35:50 INFO FileOutputCommitter: Saved output of task 'attempt_202508060635507616090948065150845_0011_m_000000_11' to file:/content/data/parquet/payments/_temporary/0/task_202508060635507616090948065150845_0011_m_000000\n",
            "25/08/06 06:35:50 INFO SparkHadoopMapRedUtil: attempt_202508060635507616090948065150845_0011_m_000000_11: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 06:35:50 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 2455 bytes result sent to driver\n",
            "25/08/06 06:35:50 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 141 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:35:50 INFO DAGScheduler: ResultStage 11 (parquet at DataIngestionSetup.java:12) finished in 0.187 s\n",
            "25/08/06 06:35:50 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:35:50 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:35:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Job 11 finished: parquet at DataIngestionSetup.java:12, took 0.195225 s\n",
            "25/08/06 06:35:50 INFO FileFormatWriter: Start to commit write Job d3e6623a-6b2f-4a8b-8929-a3ad5e4d910f.\n",
            "25/08/06 06:35:50 INFO FileFormatWriter: Write Job d3e6623a-6b2f-4a8b-8929-a3ad5e4d910f committed. Elapsed time: 15 ms.\n",
            "25/08/06 06:35:50 INFO FileFormatWriter: Finished processing stats for write job d3e6623a-6b2f-4a8b-8929-a3ad5e4d910f.\n",
            "✅ payments\n",
            "25/08/06 06:35:50 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:35:50 INFO InMemoryFileIndex: It took 9 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:35:50 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:50 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#286, None)) > 0)\n",
            "25/08/06 06:35:50 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
            "25/08/06 06:35:50 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 199.9 KiB, free 432.7 MiB)\n",
            "25/08/06 06:35:50 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 432.7 MiB)\n",
            "25/08/06 06:35:50 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 95a0e3787e8d:39661 (size: 34.3 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:35:50 INFO SparkContext: Created broadcast 30 from csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4223613 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:50 INFO SparkContext: Starting job: csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Got job 12 (csv at DataIngestionSetup.java:11) with 1 output partitions\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Final stage: ResultStage 12 (csv at DataIngestionSetup.java:11)\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[75] at csv at DataIngestionSetup.java:11), which has no missing parents\n",
            "25/08/06 06:35:50 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 11.8 KiB, free 432.7 MiB)\n",
            "25/08/06 06:35:50 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 432.7 MiB)\n",
            "25/08/06 06:35:50 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 95a0e3787e8d:39661 (size: 5.9 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:35:50 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[75] at csv at DataIngestionSetup.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:35:50 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:35:50 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:35:50 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)\n",
            "25/08/06 06:35:50 INFO FileScanRDD: Reading File path: file:///content/data/csv/products.csv, range: 0-29309, partition values: [empty row]\n",
            "25/08/06 06:35:50 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 1604 bytes result sent to driver\n",
            "25/08/06 06:35:50 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 23 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:35:50 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:35:50 INFO DAGScheduler: ResultStage 12 (csv at DataIngestionSetup.java:11) finished in 0.049 s\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:35:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Job 12 finished: csv at DataIngestionSetup.java:11, took 0.057335 s\n",
            "25/08/06 06:35:50 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:50 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:35:50 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
            "25/08/06 06:35:50 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 199.9 KiB, free 432.5 MiB)\n",
            "25/08/06 06:35:50 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 432.4 MiB)\n",
            "25/08/06 06:35:50 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 95a0e3787e8d:39661 (size: 34.3 KiB, free: 434.0 MiB)\n",
            "25/08/06 06:35:50 INFO SparkContext: Created broadcast 32 from csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4223613 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:50 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:50 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:35:50 INFO FileSourceStrategy: Output Data Schema: struct<productCode: string, productName: string, productLine: string, productScale: string, productVendor: string ... 7 more fields>\n",
            "25/08/06 06:35:50 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:50 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 199.7 KiB, free 432.2 MiB)\n",
            "25/08/06 06:35:50 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 432.2 MiB)\n",
            "25/08/06 06:35:50 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 95a0e3787e8d:39661 (size: 34.2 KiB, free: 434.0 MiB)\n",
            "25/08/06 06:35:50 INFO SparkContext: Created broadcast 33 from parquet at DataIngestionSetup.java:12\n",
            "25/08/06 06:35:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4223613 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:50 INFO SparkContext: Starting job: parquet at DataIngestionSetup.java:12\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Got job 13 (parquet at DataIngestionSetup.java:12) with 1 output partitions\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at DataIngestionSetup.java:12)\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[83] at parquet at DataIngestionSetup.java:12), which has no missing parents\n",
            "25/08/06 06:35:50 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 212.5 KiB, free 432.0 MiB)\n",
            "25/08/06 06:35:50 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 76.7 KiB, free 431.9 MiB)\n",
            "25/08/06 06:35:50 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 95a0e3787e8d:39661 (size: 76.7 KiB, free: 433.9 MiB)\n",
            "25/08/06 06:35:50 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[83] at parquet at DataIngestionSetup.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:35:50 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:35:50 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:35:50 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)\n",
            "25/08/06 06:35:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:50 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:35:50 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:35:50 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
            "25/08/06 06:35:50 INFO ParquetOutputFormat: Validation is off\n",
            "25/08/06 06:35:50 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "25/08/06 06:35:50 INFO ParquetOutputFormat: Parquet properties are:\n",
            "Parquet page size to 1048576\n",
            "Parquet dictionary page size to 1048576\n",
            "Dictionary is true\n",
            "Writer version is: PARQUET_1_0\n",
            "Page size checking is: estimated\n",
            "Min row count for page size check is: 100\n",
            "Max row count for page size check is: 10000\n",
            "Truncate length for column indexes is: 64\n",
            "Truncate length for statistics min/max  is: 2147483647\n",
            "Bloom filter enabled: false\n",
            "Max Bloom filter size for a column is 1048576\n",
            "Bloom filter expected number of distinct values are: null\n",
            "Page row count limit to 20000\n",
            "Writing page checksums is: on\n",
            "25/08/06 06:35:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productLine\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productScale\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productVendor\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productDescription\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"quantityInStock\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"buyPrice\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"MSRP\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productCode (STRING);\n",
            "  optional binary productName (STRING);\n",
            "  optional binary productLine (STRING);\n",
            "  optional binary productScale (STRING);\n",
            "  optional binary productVendor (STRING);\n",
            "  optional binary productDescription (STRING);\n",
            "  optional binary quantityInStock (STRING);\n",
            "  optional binary buyPrice (STRING);\n",
            "  optional binary MSRP (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:35:50 INFO FileScanRDD: Reading File path: file:///content/data/csv/products.csv, range: 0-29309, partition values: [empty row]\n",
            "25/08/06 06:35:50 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 95a0e3787e8d:39661 in memory (size: 5.9 KiB, free: 433.9 MiB)\n",
            "25/08/06 06:35:50 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 95a0e3787e8d:39661 in memory (size: 34.2 KiB, free: 434.0 MiB)\n",
            "25/08/06 06:35:50 INFO FileOutputCommitter: Saved output of task 'attempt_202508060635508009950295944065328_0013_m_000000_13' to file:/content/data/parquet/products/_temporary/0/task_202508060635508009950295944065328_0013_m_000000\n",
            "25/08/06 06:35:50 INFO SparkHadoopMapRedUtil: attempt_202508060635508009950295944065328_0013_m_000000_13: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 06:35:50 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 2498 bytes result sent to driver\n",
            "25/08/06 06:35:50 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 173 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:35:50 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:35:50 INFO DAGScheduler: ResultStage 13 (parquet at DataIngestionSetup.java:12) finished in 0.220 s\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:35:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
            "25/08/06 06:35:50 INFO DAGScheduler: Job 13 finished: parquet at DataIngestionSetup.java:12, took 0.228635 s\n",
            "25/08/06 06:35:50 INFO FileFormatWriter: Start to commit write Job 9bec487e-a2ee-48ea-8337-a47a26539196.\n",
            "25/08/06 06:35:51 INFO FileFormatWriter: Write Job 9bec487e-a2ee-48ea-8337-a47a26539196 committed. Elapsed time: 50 ms.\n",
            "25/08/06 06:35:51 INFO FileFormatWriter: Finished processing stats for write job 9bec487e-a2ee-48ea-8337-a47a26539196.\n",
            "✅ products\n",
            "25/08/06 06:35:51 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 95a0e3787e8d:39661 in memory (size: 34.3 KiB, free: 434.0 MiB)\n",
            "25/08/06 06:35:51 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:35:51 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:35:51 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 95a0e3787e8d:39661 in memory (size: 34.3 KiB, free: 434.0 MiB)\n",
            "25/08/06 06:35:51 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 95a0e3787e8d:39661 in memory (size: 34.3 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:35:51 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 95a0e3787e8d:39661 in memory (size: 34.2 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:35:51 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 95a0e3787e8d:39661 in memory (size: 5.9 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:35:51 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 95a0e3787e8d:39661 in memory (size: 76.5 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:35:51 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:51 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#339, None)) > 0)\n",
            "25/08/06 06:35:51 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
            "25/08/06 06:35:51 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 95a0e3787e8d:39661 in memory (size: 34.3 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:35:51 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 199.9 KiB, free 433.4 MiB)\n",
            "25/08/06 06:35:51 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 95a0e3787e8d:39661 in memory (size: 76.5 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:51 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.7 MiB)\n",
            "25/08/06 06:35:51 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 95a0e3787e8d:39661 (size: 34.3 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:51 INFO SparkContext: Created broadcast 35 from csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197750 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:51 INFO SparkContext: Starting job: csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:51 INFO DAGScheduler: Got job 14 (csv at DataIngestionSetup.java:11) with 1 output partitions\n",
            "25/08/06 06:35:51 INFO DAGScheduler: Final stage: ResultStage 14 (csv at DataIngestionSetup.java:11)\n",
            "25/08/06 06:35:51 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:35:51 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:35:51 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[87] at csv at DataIngestionSetup.java:11), which has no missing parents\n",
            "25/08/06 06:35:51 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 11.8 KiB, free 433.6 MiB)\n",
            "25/08/06 06:35:51 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.6 MiB)\n",
            "25/08/06 06:35:51 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 95a0e3787e8d:39661 (size: 5.9 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:35:51 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:35:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[87] at csv at DataIngestionSetup.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:35:51 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:35:51 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4912 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:35:51 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)\n",
            "25/08/06 06:35:51 INFO FileScanRDD: Reading File path: file:///content/data/csv/productlines.csv, range: 0-3446, partition values: [empty row]\n",
            "25/08/06 06:35:51 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 1566 bytes result sent to driver\n",
            "25/08/06 06:35:51 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 26 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:35:51 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:35:51 INFO DAGScheduler: ResultStage 14 (csv at DataIngestionSetup.java:11) finished in 0.044 s\n",
            "25/08/06 06:35:51 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:35:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
            "25/08/06 06:35:51 INFO DAGScheduler: Job 14 finished: csv at DataIngestionSetup.java:11, took 0.051564 s\n",
            "25/08/06 06:35:51 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:51 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:35:51 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
            "25/08/06 06:35:51 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 199.9 KiB, free 433.4 MiB)\n",
            "25/08/06 06:35:51 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.4 MiB)\n",
            "25/08/06 06:35:51 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 95a0e3787e8d:39661 (size: 34.3 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:35:51 INFO SparkContext: Created broadcast 37 from csv at DataIngestionSetup.java:11\n",
            "25/08/06 06:35:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197750 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:51 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 06:35:51 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 06:35:51 INFO FileSourceStrategy: Output Data Schema: struct<productLine: string, textDescription: string, htmlDescription: string, image: string ... 2 more fields>\n",
            "25/08/06 06:35:51 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:51 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 199.7 KiB, free 433.2 MiB)\n",
            "25/08/06 06:35:51 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 433.2 MiB)\n",
            "25/08/06 06:35:51 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 95a0e3787e8d:39661 (size: 34.2 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:35:51 INFO SparkContext: Created broadcast 38 from parquet at DataIngestionSetup.java:12\n",
            "25/08/06 06:35:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197750 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:35:51 INFO SparkContext: Starting job: parquet at DataIngestionSetup.java:12\n",
            "25/08/06 06:35:51 INFO DAGScheduler: Got job 15 (parquet at DataIngestionSetup.java:12) with 1 output partitions\n",
            "25/08/06 06:35:51 INFO DAGScheduler: Final stage: ResultStage 15 (parquet at DataIngestionSetup.java:12)\n",
            "25/08/06 06:35:51 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:35:51 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:35:51 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[95] at parquet at DataIngestionSetup.java:12), which has no missing parents\n",
            "25/08/06 06:35:51 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 211.4 KiB, free 433.0 MiB)\n",
            "25/08/06 06:35:51 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 76.5 KiB, free 432.9 MiB)\n",
            "25/08/06 06:35:51 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 95a0e3787e8d:39661 (size: 76.5 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:35:51 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:35:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[95] at parquet at DataIngestionSetup.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:35:51 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:35:51 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4912 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:35:51 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)\n",
            "25/08/06 06:35:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:35:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:35:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:35:51 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:35:51 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:35:51 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
            "25/08/06 06:35:51 INFO ParquetOutputFormat: Validation is off\n",
            "25/08/06 06:35:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "25/08/06 06:35:51 INFO ParquetOutputFormat: Parquet properties are:\n",
            "Parquet page size to 1048576\n",
            "Parquet dictionary page size to 1048576\n",
            "Dictionary is true\n",
            "Writer version is: PARQUET_1_0\n",
            "Page size checking is: estimated\n",
            "Min row count for page size check is: 100\n",
            "Max row count for page size check is: 10000\n",
            "Truncate length for column indexes is: 64\n",
            "Truncate length for statistics min/max  is: 2147483647\n",
            "Bloom filter enabled: false\n",
            "Max Bloom filter size for a column is 1048576\n",
            "Bloom filter expected number of distinct values are: null\n",
            "Page row count limit to 20000\n",
            "Writing page checksums is: on\n",
            "25/08/06 06:35:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productLine\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"textDescription\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"htmlDescription\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"image\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productLine (STRING);\n",
            "  optional binary textDescription (STRING);\n",
            "  optional binary htmlDescription (STRING);\n",
            "  optional binary image (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:35:51 INFO FileScanRDD: Reading File path: file:///content/data/csv/productlines.csv, range: 0-3446, partition values: [empty row]\n",
            "25/08/06 06:35:51 INFO FileOutputCommitter: Saved output of task 'attempt_202508060635511625959873792631936_0015_m_000000_15' to file:/content/data/parquet/productlines/_temporary/0/task_202508060635511625959873792631936_0015_m_000000\n",
            "25/08/06 06:35:51 INFO SparkHadoopMapRedUtil: attempt_202508060635511625959873792631936_0015_m_000000_15: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 06:35:51 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 2455 bytes result sent to driver\n",
            "25/08/06 06:35:51 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 99 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:35:51 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:35:51 INFO DAGScheduler: ResultStage 15 (parquet at DataIngestionSetup.java:12) finished in 0.164 s\n",
            "25/08/06 06:35:51 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:35:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
            "25/08/06 06:35:51 INFO DAGScheduler: Job 15 finished: parquet at DataIngestionSetup.java:12, took 0.170122 s\n",
            "25/08/06 06:35:51 INFO FileFormatWriter: Start to commit write Job 08c36c45-0cdf-4eb8-bbd8-e8670301ead0.\n",
            "25/08/06 06:35:51 INFO FileFormatWriter: Write Job 08c36c45-0cdf-4eb8-bbd8-e8670301ead0 committed. Elapsed time: 12 ms.\n",
            "25/08/06 06:35:51 INFO FileFormatWriter: Finished processing stats for write job 08c36c45-0cdf-4eb8-bbd8-e8670301ead0.\n",
            "✅ productlines\n",
            "25/08/06 06:35:51 INFO SparkUI: Stopped Spark web UI at http://95a0e3787e8d:4041\n",
            "25/08/06 06:35:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 06:35:51 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 06:35:51 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 06:35:51 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 06:35:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 06:35:51 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 06:35:51 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 06:35:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-9f3530e3-64a8-4d28-9934-759ead0b2162\n",
            "25/08/06 06:35:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-54e64d51-f51c-4f2e-8b87-337badde4bd4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "spark.read.parquet(\"/content/data/parquet/customers\").show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_01sSZEGGWZO",
        "outputId": "4e040ca0-fe53-4052-d050-8568667ea86e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------------+---------------+----------------+------------+-----------------+------------+---------+--------+----------+---------+----------------------+-----------+\n",
            "|customerNumber|        customerName|contactLastName|contactFirstName|       phone|     addressLine1|addressLine2|     city|   state|postalCode|  country|salesRepEmployeeNumber|creditLimit|\n",
            "+--------------+--------------------+---------------+----------------+------------+-----------------+------------+---------+--------+----------+---------+----------------------+-----------+\n",
            "|           103|   Atelier graphique|        Schmitt|         Carine |  40.32.2555|   54, rue Royale|        null|   Nantes|    null|     44000|   France|                1370.0|    21000.0|\n",
            "|           112|  Signal Gift Stores|           King|            Jean|  7025551838|  8489 Strong St.|        null|Las Vegas|      NV|     83030|      USA|                1166.0|    71800.0|\n",
            "|           114|Australian Collec...|       Ferguson|           Peter|03 9520 4555|636 St Kilda Road|     Level 3|Melbourne|Victoria|      3004|Australia|                1611.0|   117300.0|\n",
            "+--------------+--------------------+---------------+----------------+------------+-----------------+------------+---------+--------+----------+---------+----------------------+-----------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2:"
      ],
      "metadata": {
        "id": "nkRSRGz8GY6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/project/src/OrderRevenueAnalysis.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class OrderRevenueAnalysis {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "            .appName(\"OrderRevenueAnalysis\")\n",
        "            .master(\"local\")\n",
        "            .getOrCreate();\n",
        "\n",
        "        Dataset<Row> orders = spark.read().parquet(\"/content/data/parquet/orders\");\n",
        "        Dataset<Row> orderDetails = spark.read().parquet(\"/content/data/parquet/orderdetails\");\n",
        "        Dataset<Row> products = spark.read().parquet(\"/content/data/parquet/products\");\n",
        "\n",
        "        // Top 10 products by quantity sold\n",
        "        Dataset<Row> topProducts = orderDetails.groupBy(\"productCode\")\n",
        "            .agg(functions.sum(\"quantityOrdered\").alias(\"totalSold\"))\n",
        "            .join(products, \"productCode\")\n",
        "            .select(\"productName\", \"totalSold\")\n",
        "            .orderBy(functions.col(\"totalSold\").desc())\n",
        "            .limit(10);\n",
        "\n",
        "        topProducts.write().mode(\"overwrite\").parquet(\"/output/processed/top_10_products.parquet\");\n",
        "\n",
        "        // Product-wise revenue\n",
        "        Dataset<Row> productRevenue = orderDetails.withColumn(\"revenue\",\n",
        "                functions.expr(\"quantityOrdered * priceEach\"))\n",
        "            .groupBy(\"productCode\")\n",
        "            .agg(functions.sum(\"revenue\").alias(\"totalRevenue\"))\n",
        "            .join(products, \"productCode\")\n",
        "            .select(\"productName\", \"totalRevenue\")\n",
        "            .orderBy(functions.col(\"totalRevenue\").desc());\n",
        "\n",
        "        productRevenue.write().mode(\"overwrite\").parquet(\"/output/processed/product_revenue.parquet\");\n",
        "\n",
        "        // Average order value per customer\n",
        "        Dataset<Row> ordersWithTotal = orderDetails.withColumn(\"lineTotal\",\n",
        "                functions.expr(\"quantityOrdered * priceEach\"))\n",
        "            .groupBy(\"orderNumber\")\n",
        "            .agg(functions.sum(\"lineTotal\").alias(\"orderTotal\"));\n",
        "\n",
        "        Dataset<Row> ordersWithCustomer = orders.join(ordersWithTotal, \"orderNumber\")\n",
        "            .groupBy(\"customerNumber\")\n",
        "            .agg(functions.avg(\"orderTotal\").alias(\"avgOrderValue\"));\n",
        "\n",
        "        ordersWithCustomer.write().mode(\"overwrite\").parquet(\"/output/processed/avg_order_value.parquet\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJBuda0pHyZI",
        "outputId": "469f53e0-a359-4115-d6b2-5b048ac72019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/project/src/OrderRevenueAnalysis.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/project/src\n",
        "!javac -cp \"/content/spark-3.3.2-bin-hadoop3/jars/*\" OrderRevenueAnalysis.java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtQwIpn9H1lk",
        "outputId": "86577147-9eaf-4a20-a210-f9c3365b896c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/project/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/spark-3.3.2-bin-hadoop3/bin/spark-submit \\\n",
        "  --class OrderRevenueAnalysis \\\n",
        "  --master local \\\n",
        "  --conf \"spark.driver.extraClassPath=/content/project/src:/content/spark-3.3.2-bin-hadoop3/jars/*\" \\\n",
        "  --jars /content/spark-3.3.2-bin-hadoop3/jars/* \\\n",
        "  OrderRevenueAnalysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WLs9KNtAH5du",
        "outputId": "afba5eda-8d6b-400e-ca90-d8e48f6f3e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/08/06 06:44:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 06:44:23 INFO SparkContext: Running Spark version 3.3.2\n",
            "25/08/06 06:44:23 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 06:44:23 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 06:44:23 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 06:44:23 INFO SparkContext: Submitted application: OrderRevenueAnalysis\n",
            "25/08/06 06:44:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 06:44:23 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 06:44:23 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 06:44:23 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 06:44:23 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 06:44:23 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 06:44:23 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 06:44:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "25/08/06 06:44:23 INFO Utils: Successfully started service 'sparkDriver' on port 42847.\n",
            "25/08/06 06:44:23 INFO SparkEnv: Registering MapOutputTracker\n",
            "25/08/06 06:44:24 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 06:44:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 06:44:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 06:44:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 06:44:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5ad87df8-3bc9-4095-bb3b-ccb17f82544f\n",
            "25/08/06 06:44:24 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "25/08/06 06:44:24 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 06:44:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/06 06:44:24 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/06 06:44:24 INFO SparkContext: Added JAR file:///content/spark-3.3.2-bin-hadoop3/jars/activation-1.1.1.jar at spark://95a0e3787e8d:42847/jars/activation-1.1.1.jar with timestamp 1754462663163\n",
            "25/08/06 06:44:24 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/aircompressor-0.21.jar at spark://95a0e3787e8d:42847/jars/aircompressor-0.21.jar with timestamp 1754462663163\n",
            "25/08/06 06:44:24 INFO Executor: Starting executor ID driver on host 95a0e3787e8d\n",
            "25/08/06 06:44:24 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 06:44:24 INFO Executor: Fetching spark://95a0e3787e8d:42847/jars/aircompressor-0.21.jar with timestamp 1754462663163\n",
            "25/08/06 06:44:25 INFO TransportClientFactory: Successfully created connection to 95a0e3787e8d/172.28.0.12:42847 after 74 ms (0 ms spent in bootstraps)\n",
            "25/08/06 06:44:25 INFO Utils: Fetching spark://95a0e3787e8d:42847/jars/aircompressor-0.21.jar to /tmp/spark-074a6688-65be-4a3e-9475-8295885d734e/userFiles-0fd730fb-4bad-40f7-a93d-1d2b1e17a0fc/fetchFileTemp3573131981060450720.tmp\n",
            "25/08/06 06:44:25 INFO Executor: Adding file:/tmp/spark-074a6688-65be-4a3e-9475-8295885d734e/userFiles-0fd730fb-4bad-40f7-a93d-1d2b1e17a0fc/aircompressor-0.21.jar to class loader\n",
            "25/08/06 06:44:25 INFO Executor: Fetching spark://95a0e3787e8d:42847/jars/activation-1.1.1.jar with timestamp 1754462663163\n",
            "25/08/06 06:44:25 INFO Utils: Fetching spark://95a0e3787e8d:42847/jars/activation-1.1.1.jar to /tmp/spark-074a6688-65be-4a3e-9475-8295885d734e/userFiles-0fd730fb-4bad-40f7-a93d-1d2b1e17a0fc/fetchFileTemp3091861963939207507.tmp\n",
            "25/08/06 06:44:25 INFO Executor: Adding file:/tmp/spark-074a6688-65be-4a3e-9475-8295885d734e/userFiles-0fd730fb-4bad-40f7-a93d-1d2b1e17a0fc/activation-1.1.1.jar to class loader\n",
            "25/08/06 06:44:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33689.\n",
            "25/08/06 06:44:25 INFO NettyBlockTransferService: Server created on 95a0e3787e8d:33689\n",
            "25/08/06 06:44:25 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 06:44:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 95a0e3787e8d, 33689, None)\n",
            "25/08/06 06:44:25 INFO BlockManagerMasterEndpoint: Registering block manager 95a0e3787e8d:33689 with 434.4 MiB RAM, BlockManagerId(driver, 95a0e3787e8d, 33689, None)\n",
            "25/08/06 06:44:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 95a0e3787e8d, 33689, None)\n",
            "25/08/06 06:44:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 95a0e3787e8d, 33689, None)\n",
            "25/08/06 06:44:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 06:44:25 INFO SharedState: Warehouse path is 'file:/content/project/src/spark-warehouse'.\n",
            "25/08/06 06:44:27 INFO InMemoryFileIndex: It took 82 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:44:28 INFO SparkContext: Starting job: parquet at OrderRevenueAnalysis.java:10\n",
            "25/08/06 06:44:28 INFO DAGScheduler: Got job 0 (parquet at OrderRevenueAnalysis.java:10) with 1 output partitions\n",
            "25/08/06 06:44:28 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at OrderRevenueAnalysis.java:10)\n",
            "25/08/06 06:44:28 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:44:28 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:44:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at OrderRevenueAnalysis.java:10), which has no missing parents\n",
            "25/08/06 06:44:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 103.1 KiB, free 434.3 MiB)\n",
            "25/08/06 06:44:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.1 KiB, free 434.3 MiB)\n",
            "25/08/06 06:44:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 95a0e3787e8d:33689 (size: 37.1 KiB, free: 434.4 MiB)\n",
            "25/08/06 06:44:29 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:44:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at OrderRevenueAnalysis.java:10) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:44:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:44:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:44:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 06:44:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1913 bytes result sent to driver\n",
            "25/08/06 06:44:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1252 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:44:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:44:30 INFO DAGScheduler: ResultStage 0 (parquet at OrderRevenueAnalysis.java:10) finished in 1.685 s\n",
            "25/08/06 06:44:30 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:44:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 06:44:30 INFO DAGScheduler: Job 0 finished: parquet at OrderRevenueAnalysis.java:10, took 1.843650 s\n",
            "25/08/06 06:44:31 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 95a0e3787e8d:33689 in memory (size: 37.1 KiB, free: 434.4 MiB)\n",
            "25/08/06 06:44:35 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:44:35 INFO SparkContext: Starting job: parquet at OrderRevenueAnalysis.java:11\n",
            "25/08/06 06:44:35 INFO DAGScheduler: Got job 1 (parquet at OrderRevenueAnalysis.java:11) with 1 output partitions\n",
            "25/08/06 06:44:35 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at OrderRevenueAnalysis.java:11)\n",
            "25/08/06 06:44:35 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:44:35 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:44:35 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at OrderRevenueAnalysis.java:11), which has no missing parents\n",
            "25/08/06 06:44:35 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 103.1 KiB, free 434.3 MiB)\n",
            "25/08/06 06:44:35 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 37.1 KiB, free 434.3 MiB)\n",
            "25/08/06 06:44:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 95a0e3787e8d:33689 (size: 37.1 KiB, free: 434.4 MiB)\n",
            "25/08/06 06:44:35 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:44:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at OrderRevenueAnalysis.java:11) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:44:35 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:44:35 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:44:35 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 06:44:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1796 bytes result sent to driver\n",
            "25/08/06 06:44:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 61 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:44:35 INFO DAGScheduler: ResultStage 1 (parquet at OrderRevenueAnalysis.java:11) finished in 0.097 s\n",
            "25/08/06 06:44:35 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:44:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:44:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 06:44:35 INFO DAGScheduler: Job 1 finished: parquet at OrderRevenueAnalysis.java:11, took 0.109268 s\n",
            "25/08/06 06:44:35 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "25/08/06 06:44:35 INFO SparkContext: Starting job: parquet at OrderRevenueAnalysis.java:12\n",
            "25/08/06 06:44:35 INFO DAGScheduler: Got job 2 (parquet at OrderRevenueAnalysis.java:12) with 1 output partitions\n",
            "25/08/06 06:44:35 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at OrderRevenueAnalysis.java:12)\n",
            "25/08/06 06:44:35 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:44:35 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:44:35 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at OrderRevenueAnalysis.java:12), which has no missing parents\n",
            "25/08/06 06:44:35 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 103.1 KiB, free 434.2 MiB)\n",
            "25/08/06 06:44:35 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 37.1 KiB, free 434.1 MiB)\n",
            "25/08/06 06:44:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 95a0e3787e8d:33689 (size: 37.1 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:44:35 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:44:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at OrderRevenueAnalysis.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:44:35 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:44:35 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4651 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:44:35 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 06:44:35 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1964 bytes result sent to driver\n",
            "25/08/06 06:44:35 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 68 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:44:35 INFO DAGScheduler: ResultStage 2 (parquet at OrderRevenueAnalysis.java:12) finished in 0.106 s\n",
            "25/08/06 06:44:35 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:44:35 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:44:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 06:44:35 INFO DAGScheduler: Job 2 finished: parquet at OrderRevenueAnalysis.java:12, took 0.117693 s\n",
            "25/08/06 06:44:37 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 06:44:37 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#15)\n",
            "25/08/06 06:44:37 INFO FileSourceStrategy: Output Data Schema: struct<productCode: string, quantityOrdered: string>\n",
            "25/08/06 06:44:37 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 06:44:37 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#24)\n",
            "25/08/06 06:44:37 INFO FileSourceStrategy: Output Data Schema: struct<productCode: string, productName: string>\n",
            "25/08/06 06:44:37 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:44:38 INFO CodeGenerator: Code generated in 433.943615 ms\n",
            "25/08/06 06:44:38 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 201.3 KiB, free 433.9 MiB)\n",
            "25/08/06 06:44:38 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 433.9 MiB)\n",
            "25/08/06 06:44:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 95a0e3787e8d:33689 (size: 34.9 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:44:38 INFO SparkContext: Created broadcast 3 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:44:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4210971 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:44:38 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 95a0e3787e8d:33689 in memory (size: 37.1 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:44:38 INFO CodeGenerator: Code generated in 509.095026 ms\n",
            "25/08/06 06:44:38 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 433.8 MiB)\n",
            "25/08/06 06:44:38 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 95a0e3787e8d:33689 in memory (size: 37.1 KiB, free: 434.4 MiB)\n",
            "25/08/06 06:44:38 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 433.9 MiB)\n",
            "25/08/06 06:44:38 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 95a0e3787e8d:33689 (size: 34.8 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:44:38 INFO SparkContext: Created broadcast 4 from parquet at OrderRevenueAnalysis.java:22\n",
            "25/08/06 06:44:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4219095 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:44:38 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:44:38 INFO DAGScheduler: Got job 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 06:44:38 INFO DAGScheduler: Final stage: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 06:44:38 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:44:38 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:44:38 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 06:44:38 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 13.6 KiB, free 433.9 MiB)\n",
            "25/08/06 06:44:38 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 433.9 MiB)\n",
            "25/08/06 06:44:38 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 95a0e3787e8d:33689 (size: 6.1 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:44:38 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:44:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:44:38 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:44:38 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4976 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:44:38 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 06:44:38 INFO DAGScheduler: Registering RDD 13 (parquet at OrderRevenueAnalysis.java:22) as input to shuffle 0\n",
            "25/08/06 06:44:38 INFO DAGScheduler: Got map stage job 4 (parquet at OrderRevenueAnalysis.java:22) with 1 output partitions\n",
            "25/08/06 06:44:39 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (parquet at OrderRevenueAnalysis.java:22)\n",
            "25/08/06 06:44:39 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:44:39 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:44:39 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[13] at parquet at OrderRevenueAnalysis.java:22), which has no missing parents\n",
            "25/08/06 06:44:39 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 40.9 KiB, free 433.9 MiB)\n",
            "25/08/06 06:44:39 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 18.3 KiB, free 433.9 MiB)\n",
            "25/08/06 06:44:39 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 95a0e3787e8d:33689 (size: 18.3 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:44:39 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:44:39 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[13] at parquet at OrderRevenueAnalysis.java:22) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:44:39 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:44:39 INFO FileScanRDD: Reading File path: file:///content/data/parquet/products/part-00000-6f591ce5-8ce6-4705-a661-9754a49f5377-c000.snappy.parquet, range: 0-16667, partition values: [empty row]\n",
            "25/08/06 06:44:39 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 06:44:39 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 06:44:39 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 5350 bytes result sent to driver\n",
            "25/08/06 06:44:39 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:44:39 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 06:44:39 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 949 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:44:39 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:44:39 INFO DAGScheduler: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.993 s\n",
            "25/08/06 06:44:39 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:44:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 06:44:39 INFO DAGScheduler: Job 3 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.011923 s\n",
            "25/08/06 06:44:39 INFO CodeGenerator: Code generated in 28.425041 ms\n",
            "25/08/06 06:44:39 INFO CodeGenerator: Code generated in 46.051016 ms\n",
            "25/08/06 06:44:39 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 16.0 MiB, free 417.9 MiB)\n",
            "25/08/06 06:44:39 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 417.9 MiB)\n",
            "25/08/06 06:44:39 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 95a0e3787e8d:33689 (size: 4.7 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:44:39 INFO SparkContext: Created broadcast 7 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:44:39 INFO CodeGenerator: Code generated in 13.115051 ms\n",
            "25/08/06 06:44:40 INFO CodeGenerator: Code generated in 26.729787 ms\n",
            "25/08/06 06:44:40 INFO CodeGenerator: Code generated in 22.154464 ms\n",
            "25/08/06 06:44:40 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails/part-00000-d0518666-5369-4332-9ddc-d7a9dd010872-c000.snappy.parquet, range: 0-24791, partition values: [empty row]\n",
            "25/08/06 06:44:40 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 06:44:40 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 3034 bytes result sent to driver\n",
            "25/08/06 06:44:40 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 554 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:44:40 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:44:40 INFO DAGScheduler: ShuffleMapStage 4 (parquet at OrderRevenueAnalysis.java:22) finished in 1.307 s\n",
            "25/08/06 06:44:40 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 06:44:40 INFO DAGScheduler: running: Set()\n",
            "25/08/06 06:44:40 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 06:44:40 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 06:44:40 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 06:44:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:44:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:44:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:44:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:44:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:44:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:44:40 INFO CodeGenerator: Code generated in 36.344424 ms\n",
            "25/08/06 06:44:40 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 06:44:40 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 95a0e3787e8d:33689 in memory (size: 6.1 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:44:40 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 95a0e3787e8d:33689 in memory (size: 18.3 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:44:40 INFO CodeGenerator: Code generated in 140.059287 ms\n",
            "25/08/06 06:44:40 INFO SparkContext: Starting job: parquet at OrderRevenueAnalysis.java:22\n",
            "25/08/06 06:44:40 INFO DAGScheduler: Got job 5 (parquet at OrderRevenueAnalysis.java:22) with 1 output partitions\n",
            "25/08/06 06:44:40 INFO DAGScheduler: Final stage: ResultStage 6 (parquet at OrderRevenueAnalysis.java:22)\n",
            "25/08/06 06:44:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "25/08/06 06:44:40 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:44:40 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[16] at parquet at OrderRevenueAnalysis.java:22), which has no missing parents\n",
            "25/08/06 06:44:41 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 247.8 KiB, free 417.7 MiB)\n",
            "25/08/06 06:44:41 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 92.1 KiB, free 417.6 MiB)\n",
            "25/08/06 06:44:41 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 95a0e3787e8d:33689 (size: 92.1 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:44:41 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:44:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[16] at parquet at OrderRevenueAnalysis.java:22) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:44:41 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:44:41 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (95a0e3787e8d, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:44:41 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)\n",
            "25/08/06 06:44:41 INFO ShuffleBlockFetcherIterator: Getting 1 (6.3 KiB) non-empty blocks including 1 (6.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 06:44:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms\n",
            "25/08/06 06:44:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:44:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:44:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:44:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:44:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:44:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:44:41 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:44:41 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:44:41 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
            "25/08/06 06:44:41 INFO ParquetOutputFormat: Validation is off\n",
            "25/08/06 06:44:41 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "25/08/06 06:44:41 INFO ParquetOutputFormat: Parquet properties are:\n",
            "Parquet page size to 1048576\n",
            "Parquet dictionary page size to 1048576\n",
            "Dictionary is true\n",
            "Writer version is: PARQUET_1_0\n",
            "Page size checking is: estimated\n",
            "Min row count for page size check is: 100\n",
            "Max row count for page size check is: 10000\n",
            "Truncate length for column indexes is: 64\n",
            "Truncate length for statistics min/max  is: 2147483647\n",
            "Bloom filter enabled: false\n",
            "Max Bloom filter size for a column is 1048576\n",
            "Bloom filter expected number of distinct values are: null\n",
            "Page row count limit to 20000\n",
            "Writing page checksums is: on\n",
            "25/08/06 06:44:41 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalSold\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productName (STRING);\n",
            "  optional double totalSold;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:44:41 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 06:44:41 INFO FileOutputCommitter: Saved output of task 'attempt_202508060644405630459013037923210_0006_m_000000_5' to file:/output/processed/top_10_products.parquet/_temporary/0/task_202508060644405630459013037923210_0006_m_000000\n",
            "25/08/06 06:44:41 INFO SparkHadoopMapRedUtil: attempt_202508060644405630459013037923210_0006_m_000000_5: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 06:44:41 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 6618 bytes result sent to driver\n",
            "25/08/06 06:44:41 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 741 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:44:41 INFO DAGScheduler: ResultStage 6 (parquet at OrderRevenueAnalysis.java:22) finished in 0.804 s\n",
            "25/08/06 06:44:41 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:44:41 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:44:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 06:44:41 INFO DAGScheduler: Job 5 finished: parquet at OrderRevenueAnalysis.java:22, took 0.858336 s\n",
            "25/08/06 06:44:41 INFO FileFormatWriter: Start to commit write Job 3e6fc4a5-c797-414e-bdd8-3db5ad64a8da.\n",
            "25/08/06 06:44:41 INFO FileFormatWriter: Write Job 3e6fc4a5-c797-414e-bdd8-3db5ad64a8da committed. Elapsed time: 39 ms.\n",
            "25/08/06 06:44:41 INFO FileFormatWriter: Finished processing stats for write job 3e6fc4a5-c797-414e-bdd8-3db5ad64a8da.\n",
            "25/08/06 06:44:42 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 06:44:42 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#15)\n",
            "25/08/06 06:44:42 INFO FileSourceStrategy: Output Data Schema: struct<productCode: string, quantityOrdered: string, priceEach: string ... 1 more fields>\n",
            "25/08/06 06:44:42 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 06:44:42 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#24)\n",
            "25/08/06 06:44:42 INFO FileSourceStrategy: Output Data Schema: struct<productCode: string, productName: string>\n",
            "25/08/06 06:44:42 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:44:42 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 201.3 KiB, free 417.4 MiB)\n",
            "25/08/06 06:44:42 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 417.4 MiB)\n",
            "25/08/06 06:44:42 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 95a0e3787e8d:33689 (size: 34.9 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:44:42 INFO SparkContext: Created broadcast 9 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:44:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4210971 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:44:42 INFO CodeGenerator: Code generated in 124.15204 ms\n",
            "25/08/06 06:44:42 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:44:42 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.4 KiB, free 417.2 MiB)\n",
            "25/08/06 06:44:42 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 06:44:42 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 06:44:42 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:44:42 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:44:42 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[20] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 06:44:42 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 13.6 KiB, free 417.2 MiB)\n",
            "25/08/06 06:44:42 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 417.1 MiB)\n",
            "25/08/06 06:44:42 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 95a0e3787e8d:33689 (size: 34.8 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:44:42 INFO SparkContext: Created broadcast 10 from parquet at OrderRevenueAnalysis.java:33\n",
            "25/08/06 06:44:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4219095 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:44:42 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 417.1 MiB)\n",
            "25/08/06 06:44:42 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 95a0e3787e8d:33689 (size: 6.1 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:44:42 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:44:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[20] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:44:42 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:44:42 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4976 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:44:42 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)\n",
            "25/08/06 06:44:42 INFO FileScanRDD: Reading File path: file:///content/data/parquet/products/part-00000-6f591ce5-8ce6-4705-a661-9754a49f5377-c000.snappy.parquet, range: 0-16667, partition values: [empty row]\n",
            "25/08/06 06:44:42 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 06:44:42 INFO DAGScheduler: Registering RDD 24 (parquet at OrderRevenueAnalysis.java:33) as input to shuffle 1\n",
            "25/08/06 06:44:42 INFO DAGScheduler: Got map stage job 7 (parquet at OrderRevenueAnalysis.java:33) with 1 output partitions\n",
            "25/08/06 06:44:42 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (parquet at OrderRevenueAnalysis.java:33)\n",
            "25/08/06 06:44:42 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:44:42 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:44:42 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[24] at parquet at OrderRevenueAnalysis.java:33), which has no missing parents\n",
            "25/08/06 06:44:42 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 43.5 KiB, free 417.1 MiB)\n",
            "25/08/06 06:44:42 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 417.1 MiB)\n",
            "25/08/06 06:44:42 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 95a0e3787e8d:33689 (size: 19.1 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:44:42 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:44:42 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[24] at parquet at OrderRevenueAnalysis.java:33) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:44:42 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:44:42 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 5350 bytes result sent to driver\n",
            "25/08/06 06:44:42 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:44:42 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 91 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:44:42 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:44:42 INFO DAGScheduler: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.119 s\n",
            "25/08/06 06:44:42 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:44:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/08/06 06:44:42 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.136175 s\n",
            "25/08/06 06:44:42 INFO Executor: Running task 0.0 in stage 8.0 (TID 7)\n",
            "25/08/06 06:44:42 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 16.0 MiB, free 401.1 MiB)\n",
            "25/08/06 06:44:42 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 401.0 MiB)\n",
            "25/08/06 06:44:42 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 95a0e3787e8d:33689 (size: 4.7 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:44:42 INFO SparkContext: Created broadcast 13 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:44:42 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 95a0e3787e8d:33689 in memory (size: 6.1 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:44:42 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails/part-00000-d0518666-5369-4332-9ddc-d7a9dd010872-c000.snappy.parquet, range: 0-24791, partition values: [empty row]\n",
            "25/08/06 06:44:42 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 06:44:42 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 95a0e3787e8d:33689 in memory (size: 92.1 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:44:42 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 95a0e3787e8d:33689 in memory (size: 4.7 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:44:42 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 95a0e3787e8d:33689 in memory (size: 34.9 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:44:42 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 95a0e3787e8d:33689 in memory (size: 34.8 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:44:43 INFO Executor: Finished task 0.0 in stage 8.0 (TID 7). 3034 bytes result sent to driver\n",
            "25/08/06 06:44:43 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 439 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:44:43 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:44:43 INFO DAGScheduler: ShuffleMapStage 8 (parquet at OrderRevenueAnalysis.java:33) finished in 0.495 s\n",
            "25/08/06 06:44:43 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 06:44:43 INFO DAGScheduler: running: Set()\n",
            "25/08/06 06:44:43 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 06:44:43 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 06:44:43 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 06:44:43 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 06:44:43 INFO CodeGenerator: Code generated in 27.957176 ms\n",
            "25/08/06 06:44:43 INFO SparkContext: Starting job: parquet at OrderRevenueAnalysis.java:33\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Got job 8 (parquet at OrderRevenueAnalysis.java:33) with 1 output partitions\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Final stage: ResultStage 10 (parquet at OrderRevenueAnalysis.java:33)\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[29] at parquet at OrderRevenueAnalysis.java:33), which has no missing parents\n",
            "25/08/06 06:44:43 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 48.6 KiB, free 417.8 MiB)\n",
            "25/08/06 06:44:43 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 22.1 KiB, free 417.8 MiB)\n",
            "25/08/06 06:44:43 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 95a0e3787e8d:33689 (size: 22.1 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:44:43 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[29] at parquet at OrderRevenueAnalysis.java:33) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:44:43 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:44:43 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (95a0e3787e8d, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:44:43 INFO Executor: Running task 0.0 in stage 10.0 (TID 8)\n",
            "25/08/06 06:44:43 INFO ShuffleBlockFetcherIterator: Getting 1 (7.0 KiB) non-empty blocks including 1 (7.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 06:44:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
            "25/08/06 06:44:43 INFO CodeGenerator: Code generated in 23.274129 ms\n",
            "25/08/06 06:44:43 INFO Executor: Finished task 0.0 in stage 10.0 (TID 8). 8758 bytes result sent to driver\n",
            "25/08/06 06:44:43 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 137 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:44:43 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:44:43 INFO DAGScheduler: ResultStage 10 (parquet at OrderRevenueAnalysis.java:33) finished in 0.162 s\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:44:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Job 8 finished: parquet at OrderRevenueAnalysis.java:33, took 0.193137 s\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Registering RDD 30 (parquet at OrderRevenueAnalysis.java:33) as input to shuffle 2\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Got map stage job 9 (parquet at OrderRevenueAnalysis.java:33) with 1 output partitions\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (parquet at OrderRevenueAnalysis.java:33)\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[30] at parquet at OrderRevenueAnalysis.java:33), which has no missing parents\n",
            "25/08/06 06:44:43 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 52.1 KiB, free 417.7 MiB)\n",
            "25/08/06 06:44:43 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 23.3 KiB, free 417.7 MiB)\n",
            "25/08/06 06:44:43 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 95a0e3787e8d:33689 (size: 23.3 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:44:43 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[30] at parquet at OrderRevenueAnalysis.java:33) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:44:43 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:44:43 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 9) (95a0e3787e8d, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:44:43 INFO Executor: Running task 0.0 in stage 12.0 (TID 9)\n",
            "25/08/06 06:44:43 INFO ShuffleBlockFetcherIterator: Getting 1 (7.0 KiB) non-empty blocks including 1 (7.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 06:44:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 06:44:43 INFO Executor: Finished task 0.0 in stage 12.0 (TID 9). 5245 bytes result sent to driver\n",
            "25/08/06 06:44:43 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 9) in 202 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:44:43 INFO DAGScheduler: ShuffleMapStage 12 (parquet at OrderRevenueAnalysis.java:33) finished in 0.246 s\n",
            "25/08/06 06:44:43 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 06:44:43 INFO DAGScheduler: running: Set()\n",
            "25/08/06 06:44:43 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 06:44:43 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 06:44:43 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:44:43 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 06:44:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:44:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:44:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:44:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:44:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:44:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:44:43 INFO CodeGenerator: Code generated in 35.314036 ms\n",
            "25/08/06 06:44:43 INFO SparkContext: Starting job: parquet at OrderRevenueAnalysis.java:33\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Got job 10 (parquet at OrderRevenueAnalysis.java:33) with 1 output partitions\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Final stage: ResultStage 15 (parquet at OrderRevenueAnalysis.java:33)\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[32] at parquet at OrderRevenueAnalysis.java:33), which has no missing parents\n",
            "25/08/06 06:44:43 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 243.1 KiB, free 417.5 MiB)\n",
            "25/08/06 06:44:43 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 90.8 KiB, free 417.4 MiB)\n",
            "25/08/06 06:44:43 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 95a0e3787e8d:33689 (size: 90.8 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:44:43 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:44:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[32] at parquet at OrderRevenueAnalysis.java:33) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:44:43 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:44:43 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 95a0e3787e8d:33689 in memory (size: 22.1 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:44:43 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 10) (95a0e3787e8d, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:44:43 INFO Executor: Running task 0.0 in stage 15.0 (TID 10)\n",
            "25/08/06 06:44:44 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 95a0e3787e8d:33689 in memory (size: 19.1 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:44:44 INFO ShuffleBlockFetcherIterator: Getting 1 (10.6 KiB) non-empty blocks including 1 (10.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 06:44:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\n",
            "25/08/06 06:44:44 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 95a0e3787e8d:33689 in memory (size: 23.3 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:44:44 INFO CodeGenerator: Code generated in 82.036259 ms\n",
            "25/08/06 06:44:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:44:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:44:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:44:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:44:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:44:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:44:44 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:44:44 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:44:44 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
            "25/08/06 06:44:44 INFO ParquetOutputFormat: Validation is off\n",
            "25/08/06 06:44:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "25/08/06 06:44:44 INFO ParquetOutputFormat: Parquet properties are:\n",
            "Parquet page size to 1048576\n",
            "Parquet dictionary page size to 1048576\n",
            "Dictionary is true\n",
            "Writer version is: PARQUET_1_0\n",
            "Page size checking is: estimated\n",
            "Min row count for page size check is: 100\n",
            "Max row count for page size check is: 10000\n",
            "Truncate length for column indexes is: 64\n",
            "Truncate length for statistics min/max  is: 2147483647\n",
            "Bloom filter enabled: false\n",
            "Max Bloom filter size for a column is 1048576\n",
            "Bloom filter expected number of distinct values are: null\n",
            "Page row count limit to 20000\n",
            "Writing page checksums is: on\n",
            "25/08/06 06:44:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalRevenue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productName (STRING);\n",
            "  optional double totalRevenue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:44:44 INFO FileOutputCommitter: Saved output of task 'attempt_202508060644432815547696618631034_0015_m_000000_10' to file:/output/processed/product_revenue.parquet/_temporary/0/task_202508060644432815547696618631034_0015_m_000000\n",
            "25/08/06 06:44:44 INFO SparkHadoopMapRedUtil: attempt_202508060644432815547696618631034_0015_m_000000_10: Committed. Elapsed time: 10 ms.\n",
            "25/08/06 06:44:44 INFO Executor: Finished task 0.0 in stage 15.0 (TID 10). 6881 bytes result sent to driver\n",
            "25/08/06 06:44:44 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 10) in 608 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:44:44 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:44:44 INFO DAGScheduler: ResultStage 15 (parquet at OrderRevenueAnalysis.java:33) finished in 0.710 s\n",
            "25/08/06 06:44:44 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:44:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
            "25/08/06 06:44:44 INFO DAGScheduler: Job 10 finished: parquet at OrderRevenueAnalysis.java:33, took 0.723320 s\n",
            "25/08/06 06:44:44 INFO FileFormatWriter: Start to commit write Job 11d04bb8-6109-4bad-a25d-7001ccf0a52b.\n",
            "25/08/06 06:44:44 INFO FileFormatWriter: Write Job 11d04bb8-6109-4bad-a25d-7001ccf0a52b committed. Elapsed time: 37 ms.\n",
            "25/08/06 06:44:44 INFO FileFormatWriter: Finished processing stats for write job 11d04bb8-6109-4bad-a25d-7001ccf0a52b.\n",
            "25/08/06 06:44:45 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber)\n",
            "25/08/06 06:44:45 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#0)\n",
            "25/08/06 06:44:45 INFO FileSourceStrategy: Output Data Schema: struct<orderNumber: string, customerNumber: string>\n",
            "25/08/06 06:44:45 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber)\n",
            "25/08/06 06:44:45 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#14)\n",
            "25/08/06 06:44:45 INFO FileSourceStrategy: Output Data Schema: struct<orderNumber: string, quantityOrdered: string, priceEach: string ... 1 more fields>\n",
            "25/08/06 06:44:45 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:44:45 INFO CodeGenerator: Code generated in 67.566642 ms\n",
            "25/08/06 06:44:45 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 201.3 KiB, free 417.4 MiB)\n",
            "25/08/06 06:44:45 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 417.4 MiB)\n",
            "25/08/06 06:44:45 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 95a0e3787e8d:33689 (size: 34.8 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:44:45 INFO SparkContext: Created broadcast 17 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:44:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4205959 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:44:45 INFO CodeGenerator: Code generated in 203.953089 ms\n",
            "25/08/06 06:44:45 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:44:45 INFO DAGScheduler: Got job 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 06:44:45 INFO DAGScheduler: Final stage: ResultStage 16 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 06:44:45 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:44:45 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:44:45 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[36] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 06:44:45 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 13.6 KiB, free 417.4 MiB)\n",
            "25/08/06 06:44:45 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 417.4 MiB)\n",
            "25/08/06 06:44:45 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 95a0e3787e8d:33689 (size: 6.1 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:44:45 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 201.4 KiB, free 417.2 MiB)\n",
            "25/08/06 06:44:45 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:44:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[36] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:44:45 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:44:45 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 11) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4974 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:44:45 INFO Executor: Running task 0.0 in stage 16.0 (TID 11)\n",
            "25/08/06 06:44:45 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orders/part-00000-14514ce3-7c29-43db-bcb9-4c7b047139dd-c000.snappy.parquet, range: 0-11655, partition values: [empty row]\n",
            "25/08/06 06:44:45 INFO FilterCompat: Filtering using predicate: noteq(orderNumber, null)\n",
            "25/08/06 06:44:45 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 417.1 MiB)\n",
            "25/08/06 06:44:45 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 95a0e3787e8d:33689 (size: 34.8 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:44:45 INFO SparkContext: Created broadcast 18 from parquet at OrderRevenueAnalysis.java:45\n",
            "25/08/06 06:44:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4219095 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 06:44:45 INFO Executor: Finished task 0.0 in stage 16.0 (TID 11). 5059 bytes result sent to driver\n",
            "25/08/06 06:44:45 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 11) in 106 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:44:45 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:44:45 INFO DAGScheduler: ResultStage 16 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.133 s\n",
            "25/08/06 06:44:45 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:44:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished\n",
            "25/08/06 06:44:45 INFO DAGScheduler: Job 11 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.150995 s\n",
            "25/08/06 06:44:45 INFO DAGScheduler: Registering RDD 40 (parquet at OrderRevenueAnalysis.java:45) as input to shuffle 3\n",
            "25/08/06 06:44:45 INFO DAGScheduler: Got map stage job 12 (parquet at OrderRevenueAnalysis.java:45) with 1 output partitions\n",
            "25/08/06 06:44:45 INFO DAGScheduler: Final stage: ShuffleMapStage 17 (parquet at OrderRevenueAnalysis.java:45)\n",
            "25/08/06 06:44:45 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 06:44:45 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:44:45 INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[40] at parquet at OrderRevenueAnalysis.java:45), which has no missing parents\n",
            "25/08/06 06:44:45 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 43.5 KiB, free 417.1 MiB)\n",
            "25/08/06 06:44:45 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 16.0 MiB, free 401.0 MiB)\n",
            "25/08/06 06:44:45 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 401.0 MiB)\n",
            "25/08/06 06:44:45 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 95a0e3787e8d:33689 (size: 19.1 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:44:45 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:44:45 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 401.0 MiB)\n",
            "25/08/06 06:44:45 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[40] at parquet at OrderRevenueAnalysis.java:45) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:44:45 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:44:45 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 12) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:44:45 INFO Executor: Running task 0.0 in stage 17.0 (TID 12)\n",
            "25/08/06 06:44:45 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 95a0e3787e8d:33689 (size: 6.0 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:44:45 INFO SparkContext: Created broadcast 21 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 06:44:45 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 95a0e3787e8d:33689 in memory (size: 4.7 KiB, free: 434.1 MiB)\n",
            "25/08/06 06:44:46 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 95a0e3787e8d:33689 in memory (size: 34.9 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:44:46 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails/part-00000-d0518666-5369-4332-9ddc-d7a9dd010872-c000.snappy.parquet, range: 0-24791, partition values: [empty row]\n",
            "25/08/06 06:44:46 INFO FilterCompat: Filtering using predicate: noteq(orderNumber, null)\n",
            "25/08/06 06:44:46 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 95a0e3787e8d:33689 in memory (size: 90.8 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:44:46 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 95a0e3787e8d:33689 in memory (size: 34.8 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:44:46 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 95a0e3787e8d:33689 in memory (size: 6.1 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:44:46 INFO Executor: Finished task 0.0 in stage 17.0 (TID 12). 2991 bytes result sent to driver\n",
            "25/08/06 06:44:46 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 12) in 533 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:44:46 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:44:46 INFO DAGScheduler: ShuffleMapStage 17 (parquet at OrderRevenueAnalysis.java:45) finished in 0.666 s\n",
            "25/08/06 06:44:46 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 06:44:46 INFO DAGScheduler: running: Set()\n",
            "25/08/06 06:44:46 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 06:44:46 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 06:44:46 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 06:44:46 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 06:44:46 INFO CodeGenerator: Code generated in 130.311859 ms\n",
            "25/08/06 06:44:46 INFO DAGScheduler: Registering RDD 43 (parquet at OrderRevenueAnalysis.java:45) as input to shuffle 4\n",
            "25/08/06 06:44:46 INFO DAGScheduler: Got map stage job 13 (parquet at OrderRevenueAnalysis.java:45) with 1 output partitions\n",
            "25/08/06 06:44:46 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (parquet at OrderRevenueAnalysis.java:45)\n",
            "25/08/06 06:44:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\n",
            "25/08/06 06:44:46 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:44:46 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[43] at parquet at OrderRevenueAnalysis.java:45), which has no missing parents\n",
            "25/08/06 06:44:46 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 60.7 KiB, free 417.8 MiB)\n",
            "25/08/06 06:44:46 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 417.8 MiB)\n",
            "25/08/06 06:44:46 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 95a0e3787e8d:33689 (size: 25.4 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:44:46 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:44:46 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[43] at parquet at OrderRevenueAnalysis.java:45) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:44:46 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:44:46 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 13) (95a0e3787e8d, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:44:46 INFO Executor: Running task 0.0 in stage 19.0 (TID 13)\n",
            "25/08/06 06:44:46 INFO ShuffleBlockFetcherIterator: Getting 1 (15.5 KiB) non-empty blocks including 1 (15.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 06:44:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "25/08/06 06:44:46 INFO CodeGenerator: Code generated in 23.639136 ms\n",
            "25/08/06 06:44:47 INFO CodeGenerator: Code generated in 15.834635 ms\n",
            "25/08/06 06:44:47 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 95a0e3787e8d:33689 in memory (size: 19.1 KiB, free: 434.3 MiB)\n",
            "25/08/06 06:44:47 INFO Executor: Finished task 0.0 in stage 19.0 (TID 13). 5718 bytes result sent to driver\n",
            "25/08/06 06:44:47 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 13) in 380 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:44:47 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:44:47 INFO DAGScheduler: ShuffleMapStage 19 (parquet at OrderRevenueAnalysis.java:45) finished in 0.415 s\n",
            "25/08/06 06:44:47 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 06:44:47 INFO DAGScheduler: running: Set()\n",
            "25/08/06 06:44:47 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 06:44:47 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 06:44:47 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 06:44:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:44:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:44:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:44:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:44:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:44:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:44:47 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 06:44:47 INFO CodeGenerator: Code generated in 79.852131 ms\n",
            "25/08/06 06:44:47 INFO SparkContext: Starting job: parquet at OrderRevenueAnalysis.java:45\n",
            "25/08/06 06:44:47 INFO DAGScheduler: Got job 14 (parquet at OrderRevenueAnalysis.java:45) with 1 output partitions\n",
            "25/08/06 06:44:47 INFO DAGScheduler: Final stage: ResultStage 22 (parquet at OrderRevenueAnalysis.java:45)\n",
            "25/08/06 06:44:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\n",
            "25/08/06 06:44:47 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 06:44:47 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[45] at parquet at OrderRevenueAnalysis.java:45), which has no missing parents\n",
            "25/08/06 06:44:47 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 252.6 KiB, free 417.6 MiB)\n",
            "25/08/06 06:44:47 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 93.3 KiB, free 417.5 MiB)\n",
            "25/08/06 06:44:47 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 95a0e3787e8d:33689 (size: 93.3 KiB, free: 434.2 MiB)\n",
            "25/08/06 06:44:47 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 06:44:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[45] at parquet at OrderRevenueAnalysis.java:45) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 06:44:47 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
            "25/08/06 06:44:47 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 14) (95a0e3787e8d, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "25/08/06 06:44:47 INFO Executor: Running task 0.0 in stage 22.0 (TID 14)\n",
            "25/08/06 06:44:47 INFO ShuffleBlockFetcherIterator: Getting 1 (6.3 KiB) non-empty blocks including 1 (6.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 06:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
            "25/08/06 06:44:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:44:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:44:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:44:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 06:44:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 06:44:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 06:44:47 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:44:47 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 06:44:47 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
            "25/08/06 06:44:47 INFO ParquetOutputFormat: Validation is off\n",
            "25/08/06 06:44:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "25/08/06 06:44:47 INFO ParquetOutputFormat: Parquet properties are:\n",
            "Parquet page size to 1048576\n",
            "Parquet dictionary page size to 1048576\n",
            "Dictionary is true\n",
            "Writer version is: PARQUET_1_0\n",
            "Page size checking is: estimated\n",
            "Min row count for page size check is: 100\n",
            "Max row count for page size check is: 10000\n",
            "Truncate length for column indexes is: 64\n",
            "Truncate length for statistics min/max  is: 2147483647\n",
            "Bloom filter enabled: false\n",
            "Max Bloom filter size for a column is 1048576\n",
            "Bloom filter expected number of distinct values are: null\n",
            "Page row count limit to 20000\n",
            "Writing page checksums is: on\n",
            "25/08/06 06:44:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"customerNumber\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"avgOrderValue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary customerNumber (STRING);\n",
            "  optional double avgOrderValue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 06:44:48 INFO FileOutputCommitter: Saved output of task 'attempt_202508060644472327545372747704341_0022_m_000000_14' to file:/output/processed/avg_order_value.parquet/_temporary/0/task_202508060644472327545372747704341_0022_m_000000\n",
            "25/08/06 06:44:48 INFO SparkHadoopMapRedUtil: attempt_202508060644472327545372747704341_0022_m_000000_14: Committed. Elapsed time: 2 ms.\n",
            "25/08/06 06:44:48 INFO Executor: Finished task 0.0 in stage 22.0 (TID 14). 7372 bytes result sent to driver\n",
            "25/08/06 06:44:48 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 14) in 201 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 06:44:48 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
            "25/08/06 06:44:48 INFO DAGScheduler: ResultStage 22 (parquet at OrderRevenueAnalysis.java:45) finished in 0.309 s\n",
            "25/08/06 06:44:48 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 06:44:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
            "25/08/06 06:44:48 INFO DAGScheduler: Job 14 finished: parquet at OrderRevenueAnalysis.java:45, took 0.330011 s\n",
            "25/08/06 06:44:48 INFO FileFormatWriter: Start to commit write Job 12154f53-3244-4747-bfbb-7e65ca303104.\n",
            "25/08/06 06:44:48 INFO FileFormatWriter: Write Job 12154f53-3244-4747-bfbb-7e65ca303104 committed. Elapsed time: 43 ms.\n",
            "25/08/06 06:44:48 INFO FileFormatWriter: Finished processing stats for write job 12154f53-3244-4747-bfbb-7e65ca303104.\n",
            "25/08/06 06:44:48 INFO SparkUI: Stopped Spark web UI at http://95a0e3787e8d:4041\n",
            "25/08/06 06:44:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 06:44:48 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 06:44:48 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 06:44:48 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 06:44:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 06:44:48 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 06:44:48 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 06:44:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-074a6688-65be-4a3e-9475-8295885d734e\n",
            "25/08/06 06:44:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-1df38ed1-ade7-4bc8-9397-28d4d5983623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "spark.read.parquet(\"/output/processed/top_10_products.parquet\").show()\n",
        "spark.read.parquet(\"/output/processed/product_revenue.parquet\").show()\n",
        "spark.read.parquet(\"/output/processed/avg_order_value.parquet\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b58q5W5wIDiP",
        "outputId": "aae47cd4-654e-4930-8f2f-8c998fd5cb24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------+\n",
            "|         productName|totalSold|\n",
            "+--------------------+---------+\n",
            "|1992 Ferrari 360 ...|   1808.0|\n",
            "|1937 Lincoln Berline|   1111.0|\n",
            "|American Airlines...|   1085.0|\n",
            "|1941 Chevrolet Sp...|   1076.0|\n",
            "|1930 Buick Marque...|   1074.0|\n",
            "|    1940s Ford truck|   1061.0|\n",
            "|1969 Harley David...|   1057.0|\n",
            "|   1957 Chevy Pickup|   1056.0|\n",
            "|1964 Mercedes Tou...|   1053.0|\n",
            "|1956 Porsche 356A...|   1052.0|\n",
            "+--------------------+---------+\n",
            "\n",
            "+--------------------+------------------+\n",
            "|         productName|      totalRevenue|\n",
            "+--------------------+------------------+\n",
            "|1992 Ferrari 360 ...|         276839.98|\n",
            "|   2001 Ferrari Enzo|         190755.86|\n",
            "|1952 Alpine Renau...|190017.95999999996|\n",
            "|2003 Harley-David...|170685.99999999997|\n",
            "|   1968 Ford Mustang|161531.47999999992|\n",
            "|    1969 Ford Falcon|         152543.02|\n",
            "|1980s Black Hawk ...|144959.90999999997|\n",
            "|1998 Chrysler Ply...|142530.62999999998|\n",
            "|1917 Grand Tourin...|140535.60000000003|\n",
            "|    2002 Suzuki XREO|135767.03000000003|\n",
            "|1956 Porsche 356A...|         134240.71|\n",
            "|  1969 Corvair Monza|132363.78999999998|\n",
            "|1928 Mercedes-Ben...|132275.97999999998|\n",
            "|1957 Corvette Con...|130749.31000000001|\n",
            "| 1972 Alfa Romeo GTA|127924.31999999999|\n",
            "|1962 LanciaA Delt...|123123.00999999998|\n",
            "|1970 Triumph Spit...|         122254.75|\n",
            "|1976 Ford Gran To...|          121890.6|\n",
            "|1948 Porsche Type...|         121653.46|\n",
            "|      1958 Setra Bus|119085.24999999999|\n",
            "+--------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------+------------------+\n",
            "|customerNumber|     avgOrderValue|\n",
            "+--------------+------------------+\n",
            "|           124|34813.372941176465|\n",
            "|           447|16655.926666666666|\n",
            "|           475|          21874.36|\n",
            "|           205|31267.766666666663|\n",
            "|           334| 34632.24666666667|\n",
            "|           462|29542.496666666673|\n",
            "|           448|          40314.51|\n",
            "|           282|          33476.78|\n",
            "|           495|          32770.87|\n",
            "|           415|31310.089999999997|\n",
            "|           323|30924.416000000005|\n",
            "|           112|26726.993333333336|\n",
            "|           424|23071.443333333333|\n",
            "|           406|28812.323333333334|\n",
            "|           250|22553.063333333335|\n",
            "|           146| 43435.11666666667|\n",
            "|           379|24511.216666666664|\n",
            "|           171|30890.849999999995|\n",
            "|           187| 49470.02999999999|\n",
            "|           276|         34258.555|\n",
            "+--------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3:"
      ],
      "metadata": {
        "id": "mSzPG6dGJ5UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "java_code = \"\"\"\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "import org.apache.spark.sql.functions;\n",
        "import static org.apache.spark.sql.functions.*;\n",
        "\n",
        "public class RegionalSalesInsights {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "            .appName(\"RegionalSalesInsights\")\n",
        "            .master(\"local[*]\")\n",
        "            .getOrCreate();\n",
        "\n",
        "        // Load data from Parquet\n",
        "        Dataset<Row> customers = spark.read().parquet(\"/content/data/parquet/customers\");\n",
        "        Dataset<Row> payments = spark.read().parquet(\"/content/data/parquet/payments\");\n",
        "        Dataset<Row> employees = spark.read().parquet(\"/content/data/parquet/employees\");\n",
        "        Dataset<Row> offices = spark.read().parquet(\"/content/data/parquet/offices\");\n",
        "\n",
        "        // === 1. Total Revenue by Country ===\n",
        "        Dataset<Row> customerPayments = customers.alias(\"c\")\n",
        "            .join(payments.alias(\"p\"), col(\"c.customerNumber\").equalTo(col(\"p.customerNumber\")))\n",
        "            .groupBy(col(\"c.country\").alias(\"country\"))\n",
        "            .agg(round(sum(\"p.amount\"), 2).alias(\"totalRevenue\"))\n",
        "            .orderBy(desc(\"totalRevenue\"));\n",
        "\n",
        "        customerPayments.write().mode(\"overwrite\").parquet(\"/output/processed/revenue_by_country.parquet\");\n",
        "\n",
        "        // === 2. Sales per Region (Customer Sales by Office) ===\n",
        "        // Use left_outer join to keep all customers, even those without a sales rep\n",
        "        Dataset<Row> customerOffices = customers.alias(\"c\")\n",
        "            .join(employees.alias(\"e\"), col(\"c.salesRepEmployeeNumber\").equalTo(col(\"e.employeeNumber\")), \"left_outer\")\n",
        "            .join(offices.alias(\"o\"), col(\"e.officeCode\").equalTo(col(\"o.officeCode\")), \"left_outer\")\n",
        "            .join(payments.alias(\"p\"), col(\"c.customerNumber\").equalTo(col(\"p.customerNumber\"))); // Inner join with payments as we only care about customers with payments\n",
        "\n",
        "        Dataset<Row> salesByOffice = customerOffices\n",
        "            .groupBy(\n",
        "                coalesce(col(\"o.officeCode\"), lit(\"Unknown Office\")).alias(\"officeCode\"),\n",
        "                coalesce(col(\"o.city\"), lit(\"Unknown City\")).alias(\"city\"),\n",
        "                coalesce(col(\"o.country\"), lit(\"Unknown Country\")).alias(\"country\")\n",
        "            )\n",
        "            .agg(\n",
        "                countDistinct(\"c.customerNumber\").alias(\"customerCount\"),\n",
        "                round(sum(\"p.amount\"), 2).alias(\"totalSales\")\n",
        "            )\n",
        "            .orderBy(desc(\"totalSales\"));\n",
        "\n",
        "        salesByOffice.write().mode(\"overwrite\").parquet(\"/output/processed/customer_sales_by_office.parquet\");\n",
        "\n",
        "        // === 3. Top Performing Offices ===\n",
        "        Dataset<Row> topOffices = salesByOffice\n",
        "            .select(\"officeCode\", \"city\", \"country\", \"totalSales\")\n",
        "            .orderBy(desc(\"totalSales\"))\n",
        "            .limit(5);\n",
        "\n",
        "        topOffices.write().mode(\"overwrite\").parquet(\"/output/processed/top_offices.parquet\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/project/src/RegionalSalesInsights.java\", \"w\") as f:\n",
        "    f.write(java_code)"
      ],
      "metadata": {
        "id": "OtMeSOzoJ8y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"/content/spark-3.3.2-bin-hadoop3/jars/*\" /content/project/src/RegionalSalesInsights.java"
      ],
      "metadata": {
        "id": "zwTzBuOFl7-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/spark-3.3.2-bin-hadoop3/bin/spark-submit \\\n",
        "  --class RegionalSalesInsights \\\n",
        "  --master local \\\n",
        "  --conf \"spark.driver.extraClassPath=/content/project/src:/content/spark-3.3.2-bin-hadoop3/jars/*\" \\\n",
        "  --conf \"spark.executor.extraClassPath=/content/project/src:/content/spark-3.3.2-bin-hadoop3/jars/*\" \\\n",
        "  --jars /content/spark-3.3.2-bin-hadoop3/jars/* \\\n",
        "  RegionalSalesInsights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5lup3lj2l8gH",
        "outputId": "89cb304e-167e-4d9c-99d7-9c6f3be7ec49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/08/06 09:09:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 09:09:07 INFO SparkContext: Running Spark version 3.3.2\n",
            "25/08/06 09:09:07 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 09:09:07 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 09:09:07 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 09:09:07 INFO SparkContext: Submitted application: RegionalSalesInsights\n",
            "25/08/06 09:09:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 09:09:07 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 09:09:07 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 09:09:07 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 09:09:07 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 09:09:07 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 09:09:07 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 09:09:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "25/08/06 09:09:08 INFO Utils: Successfully started service 'sparkDriver' on port 45615.\n",
            "25/08/06 09:09:08 INFO SparkEnv: Registering MapOutputTracker\n",
            "25/08/06 09:09:08 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 09:09:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 09:09:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 09:09:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 09:09:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9530d3af-6368-4c1b-ac90-251814636e3d\n",
            "25/08/06 09:09:08 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "25/08/06 09:09:08 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 09:09:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/06 09:09:09 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/06 09:09:09 INFO SparkContext: Added JAR file:///content/spark-3.3.2-bin-hadoop3/jars/activation-1.1.1.jar at spark://95a0e3787e8d:45615/jars/activation-1.1.1.jar with timestamp 1754471347493\n",
            "25/08/06 09:09:09 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/aircompressor-0.21.jar at spark://95a0e3787e8d:45615/jars/aircompressor-0.21.jar with timestamp 1754471347493\n",
            "25/08/06 09:09:09 INFO Executor: Starting executor ID driver on host 95a0e3787e8d\n",
            "25/08/06 09:09:09 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/content/project/src/,file:/content/spark-3.3.2-bin-hadoop3/jars/*,file:/content/project/src/src,file:/content/project/src/*'\n",
            "25/08/06 09:09:09 INFO Executor: Fetching spark://95a0e3787e8d:45615/jars/activation-1.1.1.jar with timestamp 1754471347493\n",
            "25/08/06 09:09:09 INFO TransportClientFactory: Successfully created connection to 95a0e3787e8d/172.28.0.12:45615 after 63 ms (0 ms spent in bootstraps)\n",
            "25/08/06 09:09:09 INFO Utils: Fetching spark://95a0e3787e8d:45615/jars/activation-1.1.1.jar to /tmp/spark-cbb98b14-b7d9-4014-9bb8-b99f7a41e81e/userFiles-6223cdab-e34f-4411-bcce-2c2b57efdd56/fetchFileTemp17875378631883056711.tmp\n",
            "25/08/06 09:09:09 INFO Executor: Adding file:/tmp/spark-cbb98b14-b7d9-4014-9bb8-b99f7a41e81e/userFiles-6223cdab-e34f-4411-bcce-2c2b57efdd56/activation-1.1.1.jar to class loader\n",
            "25/08/06 09:09:09 INFO Executor: Fetching spark://95a0e3787e8d:45615/jars/aircompressor-0.21.jar with timestamp 1754471347493\n",
            "25/08/06 09:09:09 INFO Utils: Fetching spark://95a0e3787e8d:45615/jars/aircompressor-0.21.jar to /tmp/spark-cbb98b14-b7d9-4014-9bb8-b99f7a41e81e/userFiles-6223cdab-e34f-4411-bcce-2c2b57efdd56/fetchFileTemp12484001252894547038.tmp\n",
            "25/08/06 09:09:09 INFO Executor: Adding file:/tmp/spark-cbb98b14-b7d9-4014-9bb8-b99f7a41e81e/userFiles-6223cdab-e34f-4411-bcce-2c2b57efdd56/aircompressor-0.21.jar to class loader\n",
            "25/08/06 09:09:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33875.\n",
            "25/08/06 09:09:09 INFO NettyBlockTransferService: Server created on 95a0e3787e8d:33875\n",
            "25/08/06 09:09:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 09:09:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 95a0e3787e8d, 33875, None)\n",
            "25/08/06 09:09:09 INFO BlockManagerMasterEndpoint: Registering block manager 95a0e3787e8d:33875 with 434.4 MiB RAM, BlockManagerId(driver, 95a0e3787e8d, 33875, None)\n",
            "25/08/06 09:09:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 95a0e3787e8d, 33875, None)\n",
            "25/08/06 09:09:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 95a0e3787e8d, 33875, None)\n",
            "25/08/06 09:09:10 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 09:09:10 INFO SharedState: Warehouse path is 'file:/content/project/src/spark-warehouse'.\n",
            "25/08/06 09:09:12 INFO InMemoryFileIndex: It took 110 ms to list leaf files for 1 paths.\n",
            "25/08/06 09:09:13 INFO SparkContext: Starting job: parquet at RegionalSalesInsights.java:16\n",
            "25/08/06 09:09:13 INFO DAGScheduler: Got job 0 (parquet at RegionalSalesInsights.java:16) with 1 output partitions\n",
            "25/08/06 09:09:13 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at RegionalSalesInsights.java:16)\n",
            "25/08/06 09:09:13 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:09:13 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at RegionalSalesInsights.java:16), which has no missing parents\n",
            "25/08/06 09:09:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 103.2 KiB, free 434.3 MiB)\n",
            "25/08/06 09:09:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 434.3 MiB)\n",
            "25/08/06 09:09:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 95a0e3787e8d:33875 (size: 37.2 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:09:13 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at RegionalSalesInsights.java:16) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4652 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 09:09:15 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2176 bytes result sent to driver\n",
            "25/08/06 09:09:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1275 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:15 INFO DAGScheduler: ResultStage 0 (parquet at RegionalSalesInsights.java:16) finished in 1.927 s\n",
            "25/08/06 09:09:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:09:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 09:09:15 INFO DAGScheduler: Job 0 finished: parquet at RegionalSalesInsights.java:16, took 2.081888 s\n",
            "25/08/06 09:09:15 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 95a0e3787e8d:33875 in memory (size: 37.2 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:09:19 INFO InMemoryFileIndex: It took 19 ms to list leaf files for 1 paths.\n",
            "25/08/06 09:09:19 INFO SparkContext: Starting job: parquet at RegionalSalesInsights.java:17\n",
            "25/08/06 09:09:19 INFO DAGScheduler: Got job 1 (parquet at RegionalSalesInsights.java:17) with 1 output partitions\n",
            "25/08/06 09:09:19 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at RegionalSalesInsights.java:17)\n",
            "25/08/06 09:09:19 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:09:19 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:19 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at RegionalSalesInsights.java:17), which has no missing parents\n",
            "25/08/06 09:09:19 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 103.2 KiB, free 434.3 MiB)\n",
            "25/08/06 09:09:19 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 434.3 MiB)\n",
            "25/08/06 09:09:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 95a0e3787e8d:33875 (size: 37.2 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:09:19 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at RegionalSalesInsights.java:17) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:19 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:19 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4651 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:19 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 09:09:19 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1746 bytes result sent to driver\n",
            "25/08/06 09:09:19 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 61 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:19 INFO DAGScheduler: ResultStage 1 (parquet at RegionalSalesInsights.java:17) finished in 0.095 s\n",
            "25/08/06 09:09:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:09:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 09:09:19 INFO DAGScheduler: Job 1 finished: parquet at RegionalSalesInsights.java:17, took 0.106042 s\n",
            "25/08/06 09:09:19 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.\n",
            "25/08/06 09:09:19 INFO SparkContext: Starting job: parquet at RegionalSalesInsights.java:18\n",
            "25/08/06 09:09:19 INFO DAGScheduler: Got job 2 (parquet at RegionalSalesInsights.java:18) with 1 output partitions\n",
            "25/08/06 09:09:19 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at RegionalSalesInsights.java:18)\n",
            "25/08/06 09:09:19 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:09:19 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:19 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at RegionalSalesInsights.java:18), which has no missing parents\n",
            "25/08/06 09:09:19 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 103.2 KiB, free 434.2 MiB)\n",
            "25/08/06 09:09:19 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 434.1 MiB)\n",
            "25/08/06 09:09:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 95a0e3787e8d:33875 (size: 37.2 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:09:19 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at RegionalSalesInsights.java:18) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:19 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:19 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4652 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:19 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 09:09:19 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1902 bytes result sent to driver\n",
            "25/08/06 09:09:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 60 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:19 INFO DAGScheduler: ResultStage 2 (parquet at RegionalSalesInsights.java:18) finished in 0.108 s\n",
            "25/08/06 09:09:19 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:09:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 09:09:19 INFO DAGScheduler: Job 2 finished: parquet at RegionalSalesInsights.java:18, took 0.125203 s\n",
            "25/08/06 09:09:20 INFO InMemoryFileIndex: It took 18 ms to list leaf files for 1 paths.\n",
            "25/08/06 09:09:20 INFO SparkContext: Starting job: parquet at RegionalSalesInsights.java:19\n",
            "25/08/06 09:09:20 INFO DAGScheduler: Got job 3 (parquet at RegionalSalesInsights.java:19) with 1 output partitions\n",
            "25/08/06 09:09:20 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at RegionalSalesInsights.java:19)\n",
            "25/08/06 09:09:20 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:09:20 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:20 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[7] at parquet at RegionalSalesInsights.java:19), which has no missing parents\n",
            "25/08/06 09:09:20 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 103.2 KiB, free 434.0 MiB)\n",
            "25/08/06 09:09:20 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 434.0 MiB)\n",
            "25/08/06 09:09:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 95a0e3787e8d:33875 (size: 37.2 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:09:20 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at parquet at RegionalSalesInsights.java:19) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:20 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:20 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4650 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:20 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 09:09:20 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1935 bytes result sent to driver\n",
            "25/08/06 09:09:20 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 55 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:20 INFO DAGScheduler: ResultStage 3 (parquet at RegionalSalesInsights.java:19) finished in 0.099 s\n",
            "25/08/06 09:09:20 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:09:20 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 09:09:20 INFO DAGScheduler: Job 3 finished: parquet at RegionalSalesInsights.java:19, took 0.125152 s\n",
            "25/08/06 09:09:21 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 09:09:21 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#0)\n",
            "25/08/06 09:09:21 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string, country: string>\n",
            "25/08/06 09:09:21 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 09:09:21 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#26)\n",
            "25/08/06 09:09:21 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string, amount: string>\n",
            "25/08/06 09:09:22 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:09:22 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 95a0e3787e8d:33875 in memory (size: 37.2 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:09:22 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 95a0e3787e8d:33875 in memory (size: 37.2 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:09:22 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 95a0e3787e8d:33875 in memory (size: 37.2 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:09:22 INFO CodeGenerator: Code generated in 409.887717 ms\n",
            "25/08/06 09:09:22 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.5 KiB, free 434.2 MiB)\n",
            "25/08/06 09:09:22 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 434.2 MiB)\n",
            "25/08/06 09:09:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 95a0e3787e8d:33875 (size: 34.9 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:09:22 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:09:22 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:22 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 09:09:22 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 09:09:22 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:09:22 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:22 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[11] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 09:09:22 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 13.6 KiB, free 434.2 MiB)\n",
            "25/08/06 09:09:22 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.2 MiB)\n",
            "25/08/06 09:09:22 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 95a0e3787e8d:33875 (size: 6.1 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:09:22 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[11] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:22 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:22 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4977 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:22 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 09:09:23 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-b524bbe9-c037-49cb-aeb1-7f1d63d317e5-c000.snappy.parquet, range: 0-15479, partition values: [empty row]\n",
            "25/08/06 09:09:23 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 09:09:23 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 09:09:23 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 3378 bytes result sent to driver\n",
            "25/08/06 09:09:23 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 829 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:23 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:23 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.857 s\n",
            "25/08/06 09:09:23 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:09:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/06 09:09:23 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.885681 s\n",
            "25/08/06 09:09:23 INFO CodeGenerator: Code generated in 37.975823 ms\n",
            "25/08/06 09:09:23 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 8.0 MiB, free 426.1 MiB)\n",
            "25/08/06 09:09:23 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.8 KiB, free 426.1 MiB)\n",
            "25/08/06 09:09:23 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 95a0e3787e8d:33875 (size: 2.8 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:09:23 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:23 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 09:09:23 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#26)\n",
            "25/08/06 09:09:23 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string, amount: string>\n",
            "25/08/06 09:09:24 INFO CodeGenerator: Code generated in 210.769645 ms\n",
            "25/08/06 09:09:24 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 201.5 KiB, free 425.9 MiB)\n",
            "25/08/06 09:09:24 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 425.9 MiB)\n",
            "25/08/06 09:09:24 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 95a0e3787e8d:33875 (size: 34.9 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:09:24 INFO SparkContext: Created broadcast 7 from parquet at RegionalSalesInsights.java:28\n",
            "25/08/06 09:09:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:09:24 INFO DAGScheduler: Registering RDD 15 (parquet at RegionalSalesInsights.java:28) as input to shuffle 0\n",
            "25/08/06 09:09:24 INFO DAGScheduler: Got map stage job 5 (parquet at RegionalSalesInsights.java:28) with 1 output partitions\n",
            "25/08/06 09:09:24 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (parquet at RegionalSalesInsights.java:28)\n",
            "25/08/06 09:09:24 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:09:24 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:24 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[15] at parquet at RegionalSalesInsights.java:28), which has no missing parents\n",
            "25/08/06 09:09:24 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 48.3 KiB, free 425.9 MiB)\n",
            "25/08/06 09:09:24 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 21.4 KiB, free 425.8 MiB)\n",
            "25/08/06 09:09:24 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 95a0e3787e8d:33875 (size: 21.4 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:09:24 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:24 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[15] at parquet at RegionalSalesInsights.java:28) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:24 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:24 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4965 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:24 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "25/08/06 09:09:24 INFO CodeGenerator: Code generated in 20.934803 ms\n",
            "25/08/06 09:09:24 INFO CodeGenerator: Code generated in 20.75996 ms\n",
            "25/08/06 09:09:24 INFO CodeGenerator: Code generated in 27.852292 ms\n",
            "25/08/06 09:09:24 INFO CodeGenerator: Code generated in 32.645404 ms\n",
            "25/08/06 09:09:24 INFO FileScanRDD: Reading File path: file:///content/data/parquet/payments/part-00000-45dbd226-5b39-42cf-a89c-38bd90d91eec-c000.snappy.parquet, range: 0-8248, partition values: [empty row]\n",
            "25/08/06 09:09:24 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 09:09:25 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3931 bytes result sent to driver\n",
            "25/08/06 09:09:25 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 459 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:25 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:25 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 95a0e3787e8d:33875 in memory (size: 6.1 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:09:25 INFO DAGScheduler: ShuffleMapStage 5 (parquet at RegionalSalesInsights.java:28) finished in 0.534 s\n",
            "25/08/06 09:09:25 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 09:09:25 INFO DAGScheduler: running: Set()\n",
            "25/08/06 09:09:25 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 09:09:25 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 09:09:25 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 09:09:25 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 09:09:25 INFO CodeGenerator: Code generated in 75.297534 ms\n",
            "25/08/06 09:09:25 INFO CodeGenerator: Code generated in 48.976626 ms\n",
            "25/08/06 09:09:25 INFO SparkContext: Starting job: parquet at RegionalSalesInsights.java:28\n",
            "25/08/06 09:09:25 INFO DAGScheduler: Got job 6 (parquet at RegionalSalesInsights.java:28) with 1 output partitions\n",
            "25/08/06 09:09:25 INFO DAGScheduler: Final stage: ResultStage 7 (parquet at RegionalSalesInsights.java:28)\n",
            "25/08/06 09:09:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
            "25/08/06 09:09:25 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:25 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[20] at parquet at RegionalSalesInsights.java:28), which has no missing parents\n",
            "25/08/06 09:09:25 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 47.3 KiB, free 425.8 MiB)\n",
            "25/08/06 09:09:25 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 21.7 KiB, free 425.8 MiB)\n",
            "25/08/06 09:09:25 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 95a0e3787e8d:33875 (size: 21.7 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:09:25 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[20] at parquet at RegionalSalesInsights.java:28) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:25 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:25 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (95a0e3787e8d, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:25 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)\n",
            "25/08/06 09:09:25 INFO ShuffleBlockFetcherIterator: Getting 1 (1808.0 B) non-empty blocks including 1 (1808.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 09:09:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms\n",
            "25/08/06 09:09:25 INFO CodeGenerator: Code generated in 16.714191 ms\n",
            "25/08/06 09:09:25 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 5874 bytes result sent to driver\n",
            "25/08/06 09:09:25 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 185 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:25 INFO DAGScheduler: ResultStage 7 (parquet at RegionalSalesInsights.java:28) finished in 0.214 s\n",
            "25/08/06 09:09:25 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:09:25 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/08/06 09:09:25 INFO DAGScheduler: Job 6 finished: parquet at RegionalSalesInsights.java:28, took 0.286071 s\n",
            "25/08/06 09:09:25 INFO DAGScheduler: Registering RDD 21 (parquet at RegionalSalesInsights.java:28) as input to shuffle 1\n",
            "25/08/06 09:09:25 INFO DAGScheduler: Got map stage job 7 (parquet at RegionalSalesInsights.java:28) with 1 output partitions\n",
            "25/08/06 09:09:25 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (parquet at RegionalSalesInsights.java:28)\n",
            "25/08/06 09:09:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
            "25/08/06 09:09:25 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:25 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[21] at parquet at RegionalSalesInsights.java:28), which has no missing parents\n",
            "25/08/06 09:09:25 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 48.0 KiB, free 425.7 MiB)\n",
            "25/08/06 09:09:25 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 22.1 KiB, free 425.7 MiB)\n",
            "25/08/06 09:09:25 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 95a0e3787e8d:33875 (size: 22.1 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:09:25 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:25 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[21] at parquet at RegionalSalesInsights.java:28) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:25 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:25 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (95a0e3787e8d, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:25 INFO Executor: Running task 0.0 in stage 9.0 (TID 7)\n",
            "25/08/06 09:09:26 INFO ShuffleBlockFetcherIterator: Getting 1 (1808.0 B) non-empty blocks including 1 (1808.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 09:09:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
            "25/08/06 09:09:26 INFO Executor: Finished task 0.0 in stage 9.0 (TID 7). 5158 bytes result sent to driver\n",
            "25/08/06 09:09:26 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 111 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:26 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:26 INFO DAGScheduler: ShuffleMapStage 9 (parquet at RegionalSalesInsights.java:28) finished in 0.149 s\n",
            "25/08/06 09:09:26 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 09:09:26 INFO DAGScheduler: running: Set()\n",
            "25/08/06 09:09:26 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 09:09:26 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 09:09:26 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 09:09:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:09:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:09:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:09:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:09:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:09:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:09:26 INFO CodeGenerator: Code generated in 31.908975 ms\n",
            "25/08/06 09:09:26 INFO SparkContext: Starting job: parquet at RegionalSalesInsights.java:28\n",
            "25/08/06 09:09:26 INFO DAGScheduler: Got job 8 (parquet at RegionalSalesInsights.java:28) with 1 output partitions\n",
            "25/08/06 09:09:26 INFO DAGScheduler: Final stage: ResultStage 12 (parquet at RegionalSalesInsights.java:28)\n",
            "25/08/06 09:09:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
            "25/08/06 09:09:26 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:26 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[23] at parquet at RegionalSalesInsights.java:28), which has no missing parents\n",
            "25/08/06 09:09:26 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 243.2 KiB, free 425.5 MiB)\n",
            "25/08/06 09:09:26 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 90.7 KiB, free 425.4 MiB)\n",
            "25/08/06 09:09:26 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 95a0e3787e8d:33875 (size: 90.7 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:09:26 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[23] at parquet at RegionalSalesInsights.java:28) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:26 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 95a0e3787e8d:33875 in memory (size: 21.7 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:09:26 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:26 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 8) (95a0e3787e8d, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:26 INFO Executor: Running task 0.0 in stage 12.0 (TID 8)\n",
            "25/08/06 09:09:26 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 95a0e3787e8d:33875 in memory (size: 21.4 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:09:26 INFO ShuffleBlockFetcherIterator: Getting 1 (1808.0 B) non-empty blocks including 1 (1808.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 09:09:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
            "25/08/06 09:09:26 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 95a0e3787e8d:33875 in memory (size: 22.1 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:09:26 INFO CodeGenerator: Code generated in 19.663837 ms\n",
            "25/08/06 09:09:26 INFO CodeGenerator: Code generated in 26.924802 ms\n",
            "25/08/06 09:09:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:09:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:09:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:09:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:09:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:09:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:09:26 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 09:09:26 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 09:09:26 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
            "25/08/06 09:09:26 INFO ParquetOutputFormat: Validation is off\n",
            "25/08/06 09:09:26 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "25/08/06 09:09:26 INFO ParquetOutputFormat: Parquet properties are:\n",
            "Parquet page size to 1048576\n",
            "Parquet dictionary page size to 1048576\n",
            "Dictionary is true\n",
            "Writer version is: PARQUET_1_0\n",
            "Page size checking is: estimated\n",
            "Min row count for page size check is: 100\n",
            "Max row count for page size check is: 10000\n",
            "Truncate length for column indexes is: 64\n",
            "Truncate length for statistics min/max  is: 2147483647\n",
            "Bloom filter enabled: false\n",
            "Max Bloom filter size for a column is 1048576\n",
            "Bloom filter expected number of distinct values are: null\n",
            "Page row count limit to 20000\n",
            "Writing page checksums is: on\n",
            "25/08/06 09:09:26 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalRevenue\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary country (STRING);\n",
            "  optional double totalRevenue;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 09:09:26 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 09:09:26 INFO FileOutputCommitter: Saved output of task 'attempt_202508060909266608550808688944162_0012_m_000000_8' to file:/output/processed/revenue_by_country.parquet/_temporary/0/task_202508060909266608550808688944162_0012_m_000000\n",
            "25/08/06 09:09:26 INFO SparkHadoopMapRedUtil: attempt_202508060909266608550808688944162_0012_m_000000_8: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 09:09:26 INFO Executor: Finished task 0.0 in stage 12.0 (TID 8). 6924 bytes result sent to driver\n",
            "25/08/06 09:09:26 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 8) in 462 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:26 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:26 INFO DAGScheduler: ResultStage 12 (parquet at RegionalSalesInsights.java:28) finished in 0.534 s\n",
            "25/08/06 09:09:26 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:09:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
            "25/08/06 09:09:26 INFO DAGScheduler: Job 8 finished: parquet at RegionalSalesInsights.java:28, took 0.550743 s\n",
            "25/08/06 09:09:26 INFO FileFormatWriter: Start to commit write Job 52340f00-01e2-4fbc-9584-25da465ef39a.\n",
            "25/08/06 09:09:26 INFO FileFormatWriter: Write Job 52340f00-01e2-4fbc-9584-25da465ef39a committed. Elapsed time: 29 ms.\n",
            "25/08/06 09:09:26 INFO FileFormatWriter: Finished processing stats for write job 52340f00-01e2-4fbc-9584-25da465ef39a.\n",
            "25/08/06 09:09:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 09:09:27 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#0)\n",
            "25/08/06 09:09:27 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string, salesRepEmployeeNumber: string>\n",
            "25/08/06 09:09:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(employeeNumber)\n",
            "25/08/06 09:09:27 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(employeeNumber#34)\n",
            "25/08/06 09:09:27 INFO FileSourceStrategy: Output Data Schema: struct<employeeNumber: string, officeCode: string>\n",
            "25/08/06 09:09:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 09:09:27 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#50)\n",
            "25/08/06 09:09:27 INFO FileSourceStrategy: Output Data Schema: struct<officeCode: string, city: string, country: string ... 1 more fields>\n",
            "25/08/06 09:09:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 09:09:27 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#26)\n",
            "25/08/06 09:09:27 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string, amount: string>\n",
            "25/08/06 09:09:27 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:09:27 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.5 KiB, free 425.4 MiB)\n",
            "25/08/06 09:09:27 INFO CodeGenerator: Code generated in 35.716636 ms\n",
            "25/08/06 09:09:27 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 201.6 KiB, free 425.2 MiB)\n",
            "25/08/06 09:09:27 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 425.2 MiB)\n",
            "25/08/06 09:09:27 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 95a0e3787e8d:33875 (size: 34.8 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:09:27 INFO SparkContext: Created broadcast 12 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:09:27 INFO CodeGenerator: Code generated in 49.130447 ms\n",
            "25/08/06 09:09:27 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:27 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 424.9 MiB)\n",
            "25/08/06 09:09:27 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.5 KiB, free 425.0 MiB)\n",
            "25/08/06 09:09:27 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 95a0e3787e8d:33875 in memory (size: 90.7 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:09:27 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 95a0e3787e8d:33875 (size: 34.9 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Got job 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Final stage: ResultStage 13 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 09:09:27 INFO SparkContext: Created broadcast 13 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:09:27 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 425.2 MiB)\n",
            "25/08/06 09:09:27 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 95a0e3787e8d:33875 (size: 34.9 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:09:27 INFO SparkContext: Created broadcast 14 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[27] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 09:09:27 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 13.6 KiB, free 425.2 MiB)\n",
            "25/08/06 09:09:27 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 425.2 MiB)\n",
            "25/08/06 09:09:27 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 95a0e3787e8d:33875 (size: 6.1 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:09:27 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:27 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:27 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 9) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4977 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:27 INFO Executor: Running task 0.0 in stage 13.0 (TID 9)\n",
            "25/08/06 09:09:27 INFO FileScanRDD: Reading File path: file:///content/data/parquet/employees/part-00000-f344b70f-71cc-46aa-9cc7-d45bd5c2daa2-c000.snappy.parquet, range: 0-3574, partition values: [empty row]\n",
            "25/08/06 09:09:27 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Got job 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Final stage: ResultStage 14 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[31] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 09:09:27 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 14.2 KiB, free 425.2 MiB)\n",
            "25/08/06 09:09:27 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 425.2 MiB)\n",
            "25/08/06 09:09:27 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 95a0e3787e8d:33875 (size: 6.1 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:09:27 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:27 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[31] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:27 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:27 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 10) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4975 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Got job 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Final stage: ResultStage 15 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:27 INFO FilterCompat: Filtering using predicate: noteq(employeeNumber, null)\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[35] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 09:09:27 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 13.6 KiB, free 425.2 MiB)\n",
            "25/08/06 09:09:27 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 425.2 MiB)\n",
            "25/08/06 09:09:27 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 95a0e3787e8d:33875 (size: 6.1 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:09:27 INFO Executor: Running task 0.0 in stage 14.0 (TID 10)\n",
            "25/08/06 09:09:27 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[35] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:27 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:27 INFO Executor: Finished task 0.0 in stage 13.0 (TID 9). 2019 bytes result sent to driver\n",
            "25/08/06 09:09:27 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 11) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4976 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:27 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 9) in 102 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:27 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:27 INFO DAGScheduler: ResultStage 13 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.136 s\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:09:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
            "25/08/06 09:09:27 INFO Executor: Running task 0.0 in stage 15.0 (TID 11)\n",
            "25/08/06 09:09:27 INFO FileScanRDD: Reading File path: file:///content/data/parquet/offices/part-00000-6297b028-46cf-4dae-aa50-43a65d6f0ad4-c000.snappy.parquet, range: 0-3077, partition values: [empty row]\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Job 9 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.170383 s\n",
            "25/08/06 09:09:27 INFO FilterCompat: Filtering using predicate: noteq(officeCode, null)\n",
            "25/08/06 09:09:27 INFO FileScanRDD: Reading File path: file:///content/data/parquet/payments/part-00000-45dbd226-5b39-42cf-a89c-38bd90d91eec-c000.snappy.parquet, range: 0-8248, partition values: [empty row]\n",
            "25/08/06 09:09:27 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 8.0 MiB, free 417.2 MiB)\n",
            "25/08/06 09:09:27 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 582.0 B, free 417.2 MiB)\n",
            "25/08/06 09:09:27 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 95a0e3787e8d:33875 (size: 582.0 B, free: 434.2 MiB)\n",
            "25/08/06 09:09:27 INFO SparkContext: Created broadcast 18 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:27 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 09:09:27 INFO Executor: Finished task 0.0 in stage 14.0 (TID 10). 2006 bytes result sent to driver\n",
            "25/08/06 09:09:27 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 10) in 154 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:27 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:27 INFO DAGScheduler: ResultStage 14 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.169 s\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:09:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Job 10 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.190013 s\n",
            "25/08/06 09:09:27 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 8.0 MiB, free 409.2 MiB)\n",
            "25/08/06 09:09:27 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 450.0 B, free 409.2 MiB)\n",
            "25/08/06 09:09:27 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 95a0e3787e8d:33875 (size: 450.0 B, free: 434.2 MiB)\n",
            "25/08/06 09:09:27 INFO SparkContext: Created broadcast 19 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:27 INFO Executor: Finished task 0.0 in stage 15.0 (TID 11). 5770 bytes result sent to driver\n",
            "25/08/06 09:09:27 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 11) in 163 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:27 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 09:09:27 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#0)\n",
            "25/08/06 09:09:27 INFO DAGScheduler: ResultStage 15 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.202 s\n",
            "25/08/06 09:09:27 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string, salesRepEmployeeNumber: string>\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:09:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
            "25/08/06 09:09:27 INFO DAGScheduler: Job 11 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.231294 s\n",
            "25/08/06 09:09:27 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 8.0 MiB, free 401.2 MiB)\n",
            "25/08/06 09:09:27 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 401.2 MiB)\n",
            "25/08/06 09:09:27 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 95a0e3787e8d:33875 (size: 5.7 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:09:27 INFO SparkContext: Created broadcast 20 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 09:09:27 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#0)\n",
            "25/08/06 09:09:27 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string, salesRepEmployeeNumber: string>\n",
            "25/08/06 09:09:28 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 09:09:28 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#0)\n",
            "25/08/06 09:09:28 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string, salesRepEmployeeNumber: string>\n",
            "25/08/06 09:09:28 INFO CodeGenerator: Code generated in 230.266819 ms\n",
            "25/08/06 09:09:28 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 201.5 KiB, free 401.0 MiB)\n",
            "25/08/06 09:09:28 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 400.9 MiB)\n",
            "25/08/06 09:09:28 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 95a0e3787e8d:33875 (size: 34.9 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:09:28 INFO SparkContext: Created broadcast 21 from parquet at RegionalSalesInsights.java:49\n",
            "25/08/06 09:09:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:09:28 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 95a0e3787e8d:33875 in memory (size: 6.1 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:09:28 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 95a0e3787e8d:33875 in memory (size: 6.1 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:09:28 INFO DAGScheduler: Registering RDD 39 (parquet at RegionalSalesInsights.java:49) as input to shuffle 2\n",
            "25/08/06 09:09:28 INFO DAGScheduler: Got map stage job 12 (parquet at RegionalSalesInsights.java:49) with 1 output partitions\n",
            "25/08/06 09:09:28 INFO DAGScheduler: Final stage: ShuffleMapStage 16 (parquet at RegionalSalesInsights.java:49)\n",
            "25/08/06 09:09:28 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:09:28 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:28 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[39] at parquet at RegionalSalesInsights.java:49), which has no missing parents\n",
            "25/08/06 09:09:28 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 64.9 KiB, free 400.9 MiB)\n",
            "25/08/06 09:09:28 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 26.0 KiB, free 400.9 MiB)\n",
            "25/08/06 09:09:28 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 95a0e3787e8d:33875 (size: 26.0 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:09:28 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[39] at parquet at RegionalSalesInsights.java:49) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:28 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:28 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 12) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4966 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:28 INFO Executor: Running task 0.0 in stage 16.0 (TID 12)\n",
            "25/08/06 09:09:28 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 95a0e3787e8d:33875 in memory (size: 6.1 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:09:28 INFO CodeGenerator: Code generated in 68.121773 ms\n",
            "25/08/06 09:09:29 INFO CodeGenerator: Code generated in 169.096863 ms\n",
            "25/08/06 09:09:29 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-b524bbe9-c037-49cb-aeb1-7f1d63d317e5-c000.snappy.parquet, range: 0-15479, partition values: [empty row]\n",
            "25/08/06 09:09:29 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 09:09:29 INFO Executor: Finished task 0.0 in stage 16.0 (TID 12). 5568 bytes result sent to driver\n",
            "25/08/06 09:09:29 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 12) in 720 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:29 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:29 INFO DAGScheduler: ShuffleMapStage 16 (parquet at RegionalSalesInsights.java:49) finished in 0.778 s\n",
            "25/08/06 09:09:29 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 09:09:29 INFO DAGScheduler: running: Set()\n",
            "25/08/06 09:09:29 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 09:09:29 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 09:09:29 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 09:09:29 INFO CodeGenerator: Code generated in 179.279209 ms\n",
            "25/08/06 09:09:29 INFO DAGScheduler: Registering RDD 42 (parquet at RegionalSalesInsights.java:49) as input to shuffle 3\n",
            "25/08/06 09:09:29 INFO DAGScheduler: Got map stage job 13 (parquet at RegionalSalesInsights.java:49) with 1 output partitions\n",
            "25/08/06 09:09:29 INFO DAGScheduler: Final stage: ShuffleMapStage 18 (parquet at RegionalSalesInsights.java:49)\n",
            "25/08/06 09:09:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\n",
            "25/08/06 09:09:29 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:29 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[42] at parquet at RegionalSalesInsights.java:49), which has no missing parents\n",
            "25/08/06 09:09:29 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 77.6 KiB, free 400.8 MiB)\n",
            "25/08/06 09:09:29 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 400.8 MiB)\n",
            "25/08/06 09:09:29 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 95a0e3787e8d:33875 (size: 29.5 KiB, free: 434.1 MiB)\n",
            "25/08/06 09:09:29 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:29 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[42] at parquet at RegionalSalesInsights.java:49) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:29 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:29 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 13) (95a0e3787e8d, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:29 INFO Executor: Running task 0.0 in stage 18.0 (TID 13)\n",
            "25/08/06 09:09:29 INFO ShuffleBlockFetcherIterator: Getting 1 (11.5 KiB) non-empty blocks including 1 (11.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 09:09:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "25/08/06 09:09:30 INFO CodeGenerator: Code generated in 34.949165 ms\n",
            "25/08/06 09:09:30 INFO CodeGenerator: Code generated in 22.585442 ms\n",
            "25/08/06 09:09:30 INFO CodeGenerator: Code generated in 26.711543 ms\n",
            "25/08/06 09:09:30 INFO CodeGenerator: Code generated in 14.724909 ms\n",
            "25/08/06 09:09:30 INFO Executor: Finished task 0.0 in stage 18.0 (TID 13). 7355 bytes result sent to driver\n",
            "25/08/06 09:09:30 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 13) in 285 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:30 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:30 INFO DAGScheduler: ShuffleMapStage 18 (parquet at RegionalSalesInsights.java:49) finished in 0.326 s\n",
            "25/08/06 09:09:30 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 09:09:30 INFO DAGScheduler: running: Set()\n",
            "25/08/06 09:09:30 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 09:09:30 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 09:09:30 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 09:09:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:09:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:09:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:09:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:09:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:09:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:09:30 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 09:09:30 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 95a0e3787e8d:33875 in memory (size: 29.5 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:09:30 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 95a0e3787e8d:33875 in memory (size: 26.0 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:09:30 INFO CodeGenerator: Code generated in 84.226831 ms\n",
            "25/08/06 09:09:30 INFO SparkContext: Starting job: parquet at RegionalSalesInsights.java:49\n",
            "25/08/06 09:09:30 INFO DAGScheduler: Got job 14 (parquet at RegionalSalesInsights.java:49) with 1 output partitions\n",
            "25/08/06 09:09:30 INFO DAGScheduler: Final stage: ResultStage 21 (parquet at RegionalSalesInsights.java:49)\n",
            "25/08/06 09:09:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)\n",
            "25/08/06 09:09:30 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:30 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[44] at parquet at RegionalSalesInsights.java:49), which has no missing parents\n",
            "25/08/06 09:09:30 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 266.2 KiB, free 400.7 MiB)\n",
            "25/08/06 09:09:30 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 96.9 KiB, free 400.6 MiB)\n",
            "25/08/06 09:09:30 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 95a0e3787e8d:33875 (size: 96.9 KiB, free: 434.1 MiB)\n",
            "25/08/06 09:09:30 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[44] at parquet at RegionalSalesInsights.java:49) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:30 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:30 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 14) (95a0e3787e8d, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:30 INFO Executor: Running task 0.0 in stage 21.0 (TID 14)\n",
            "25/08/06 09:09:30 INFO ShuffleBlockFetcherIterator: Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 09:09:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 09:09:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:09:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:09:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:09:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:09:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:09:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:09:30 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 09:09:30 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 09:09:30 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
            "25/08/06 09:09:30 INFO ParquetOutputFormat: Validation is off\n",
            "25/08/06 09:09:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "25/08/06 09:09:30 INFO ParquetOutputFormat: Parquet properties are:\n",
            "Parquet page size to 1048576\n",
            "Parquet dictionary page size to 1048576\n",
            "Dictionary is true\n",
            "Writer version is: PARQUET_1_0\n",
            "Page size checking is: estimated\n",
            "Min row count for page size check is: 100\n",
            "Max row count for page size check is: 10000\n",
            "Truncate length for column indexes is: 64\n",
            "Truncate length for statistics min/max  is: 2147483647\n",
            "Bloom filter enabled: false\n",
            "Max Bloom filter size for a column is 1048576\n",
            "Bloom filter expected number of distinct values are: null\n",
            "Page row count limit to 20000\n",
            "Writing page checksums is: on\n",
            "25/08/06 09:09:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : false,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : false,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : false,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"customerCount\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : false,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalSales\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  required binary officeCode (STRING);\n",
            "  required binary city (STRING);\n",
            "  required binary country (STRING);\n",
            "  required int64 customerCount;\n",
            "  optional double totalSales;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 09:09:31 INFO FileOutputCommitter: Saved output of task 'attempt_202508060909306484792726633893416_0021_m_000000_14' to file:/output/processed/customer_sales_by_office.parquet/_temporary/0/task_202508060909306484792726633893416_0021_m_000000\n",
            "25/08/06 09:09:31 INFO SparkHadoopMapRedUtil: attempt_202508060909306484792726633893416_0021_m_000000_14: Committed. Elapsed time: 4 ms.\n",
            "25/08/06 09:09:31 INFO Executor: Finished task 0.0 in stage 21.0 (TID 14). 9052 bytes result sent to driver\n",
            "25/08/06 09:09:31 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 14) in 280 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:31 INFO DAGScheduler: ResultStage 21 (parquet at RegionalSalesInsights.java:49) finished in 0.369 s\n",
            "25/08/06 09:09:31 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:09:31 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished\n",
            "25/08/06 09:09:31 INFO DAGScheduler: Job 14 finished: parquet at RegionalSalesInsights.java:49, took 0.394985 s\n",
            "25/08/06 09:09:31 INFO FileFormatWriter: Start to commit write Job 0d852c3f-4057-4cd3-ba34-eeab56e79d41.\n",
            "25/08/06 09:09:31 INFO FileFormatWriter: Write Job 0d852c3f-4057-4cd3-ba34-eeab56e79d41 committed. Elapsed time: 51 ms.\n",
            "25/08/06 09:09:31 INFO FileFormatWriter: Finished processing stats for write job 0d852c3f-4057-4cd3-ba34-eeab56e79d41.\n",
            "25/08/06 09:09:31 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 09:09:31 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#0)\n",
            "25/08/06 09:09:31 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string, salesRepEmployeeNumber: string>\n",
            "25/08/06 09:09:31 INFO FileSourceStrategy: Pushed Filters: IsNotNull(employeeNumber)\n",
            "25/08/06 09:09:31 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(employeeNumber#34)\n",
            "25/08/06 09:09:31 INFO FileSourceStrategy: Output Data Schema: struct<employeeNumber: string, officeCode: string>\n",
            "25/08/06 09:09:31 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 09:09:31 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#50)\n",
            "25/08/06 09:09:31 INFO FileSourceStrategy: Output Data Schema: struct<officeCode: string, city: string, country: string ... 1 more fields>\n",
            "25/08/06 09:09:31 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 09:09:31 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#26)\n",
            "25/08/06 09:09:31 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string, amount: string>\n",
            "25/08/06 09:09:31 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:09:31 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 201.5 KiB, free 400.4 MiB)\n",
            "25/08/06 09:09:31 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 201.5 KiB, free 400.2 MiB)\n",
            "25/08/06 09:09:31 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 201.6 KiB, free 400.0 MiB)\n",
            "25/08/06 09:09:31 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 400.0 MiB)\n",
            "25/08/06 09:09:31 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 95a0e3787e8d:33875 (size: 34.8 KiB, free: 434.1 MiB)\n",
            "25/08/06 09:09:31 INFO SparkContext: Created broadcast 25 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:09:31 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 400.0 MiB)\n",
            "25/08/06 09:09:31 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 95a0e3787e8d:33875 (size: 34.9 KiB, free: 434.0 MiB)\n",
            "25/08/06 09:09:31 INFO SparkContext: Created broadcast 26 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:09:31 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 399.9 MiB)\n",
            "25/08/06 09:09:31 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 95a0e3787e8d:33875 (size: 34.9 KiB, free: 434.0 MiB)\n",
            "25/08/06 09:09:31 INFO SparkContext: Created broadcast 27 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:09:32 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Got job 15 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Final stage: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[48] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 09:09:32 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 13.6 KiB, free 399.9 MiB)\n",
            "25/08/06 09:09:32 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 399.9 MiB)\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 95a0e3787e8d:33875 (size: 6.1 KiB, free: 434.0 MiB)\n",
            "25/08/06 09:09:32 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:32 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[48] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:32 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 95a0e3787e8d:33875 in memory (size: 34.9 KiB, free: 434.0 MiB)\n",
            "25/08/06 09:09:32 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Got job 16 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Final stage: ResultStage 23 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:32 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 15) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4977 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:32 INFO Executor: Running task 0.0 in stage 22.0 (TID 15)\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[52] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 09:09:32 INFO FileScanRDD: Reading File path: file:///content/data/parquet/employees/part-00000-f344b70f-71cc-46aa-9cc7-d45bd5c2daa2-c000.snappy.parquet, range: 0-3574, partition values: [empty row]\n",
            "25/08/06 09:09:32 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 13.6 KiB, free 400.1 MiB)\n",
            "25/08/06 09:09:32 INFO FilterCompat: Filtering using predicate: noteq(employeeNumber, null)\n",
            "25/08/06 09:09:32 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 400.1 MiB)\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 95a0e3787e8d:33875 (size: 6.1 KiB, free: 434.0 MiB)\n",
            "25/08/06 09:09:32 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[52] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:32 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Got job 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 09:09:32 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 16) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4976 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:32 INFO Executor: Running task 0.0 in stage 23.0 (TID 16)\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Final stage: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[56] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 09:09:32 INFO FileScanRDD: Reading File path: file:///content/data/parquet/payments/part-00000-45dbd226-5b39-42cf-a89c-38bd90d91eec-c000.snappy.parquet, range: 0-8248, partition values: [empty row]\n",
            "25/08/06 09:09:32 INFO Executor: Finished task 0.0 in stage 22.0 (TID 15). 2062 bytes result sent to driver\n",
            "25/08/06 09:09:32 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 15) in 86 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:32 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:32 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 14.2 KiB, free 400.1 MiB)\n",
            "25/08/06 09:09:32 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 400.1 MiB)\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 95a0e3787e8d:33875 (size: 6.1 KiB, free: 434.0 MiB)\n",
            "25/08/06 09:09:32 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[56] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:32 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:32 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 17) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4975 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:32 INFO Executor: Running task 0.0 in stage 24.0 (TID 17)\n",
            "25/08/06 09:09:32 INFO DAGScheduler: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.158 s\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:09:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Job 15 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.163718 s\n",
            "25/08/06 09:09:32 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 09:09:32 INFO FileScanRDD: Reading File path: file:///content/data/parquet/offices/part-00000-6297b028-46cf-4dae-aa50-43a65d6f0ad4-c000.snappy.parquet, range: 0-3077, partition values: [empty row]\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 95a0e3787e8d:33875 in memory (size: 34.9 KiB, free: 434.0 MiB)\n",
            "25/08/06 09:09:32 INFO FilterCompat: Filtering using predicate: noteq(officeCode, null)\n",
            "25/08/06 09:09:32 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 8.0 MiB, free 392.3 MiB)\n",
            "25/08/06 09:09:32 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 582.0 B, free 392.3 MiB)\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 95a0e3787e8d:33875 (size: 582.0 B, free: 434.0 MiB)\n",
            "25/08/06 09:09:32 INFO SparkContext: Created broadcast 31 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 95a0e3787e8d:33875 in memory (size: 450.0 B, free: 434.0 MiB)\n",
            "25/08/06 09:09:32 INFO Executor: Finished task 0.0 in stage 24.0 (TID 17). 2006 bytes result sent to driver\n",
            "25/08/06 09:09:32 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 17) in 132 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:32 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:32 INFO DAGScheduler: ResultStage 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.159 s\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:09:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Job 17 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.236592 s\n",
            "25/08/06 09:09:32 INFO Executor: Finished task 0.0 in stage 23.0 (TID 16). 5770 bytes result sent to driver\n",
            "25/08/06 09:09:32 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 16) in 200 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:32 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:32 INFO DAGScheduler: ResultStage 23 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.228 s\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:09:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Job 16 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.270043 s\n",
            "25/08/06 09:09:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 09:09:32 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#0)\n",
            "25/08/06 09:09:32 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string, salesRepEmployeeNumber: string>\n",
            "25/08/06 09:09:32 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 8.0 MiB, free 392.3 MiB)\n",
            "25/08/06 09:09:32 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 450.0 B, free 392.3 MiB)\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 95a0e3787e8d:33875 (size: 450.0 B, free: 434.0 MiB)\n",
            "25/08/06 09:09:32 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 8.0 MiB, free 384.3 MiB)\n",
            "25/08/06 09:09:32 INFO SparkContext: Created broadcast 32 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:32 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 384.3 MiB)\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 95a0e3787e8d:33875 (size: 5.7 KiB, free: 434.0 MiB)\n",
            "25/08/06 09:09:32 INFO SparkContext: Created broadcast 33 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 95a0e3787e8d:33875 in memory (size: 34.9 KiB, free: 434.1 MiB)\n",
            "25/08/06 09:09:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 09:09:32 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#0)\n",
            "25/08/06 09:09:32 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string, salesRepEmployeeNumber: string>\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 95a0e3787e8d:33875 in memory (size: 6.1 KiB, free: 434.1 MiB)\n",
            "25/08/06 09:09:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 09:09:32 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#0)\n",
            "25/08/06 09:09:32 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string, salesRepEmployeeNumber: string>\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 95a0e3787e8d:33875 in memory (size: 6.1 KiB, free: 434.1 MiB)\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 95a0e3787e8d:33875 in memory (size: 6.1 KiB, free: 434.1 MiB)\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 95a0e3787e8d:33875 in memory (size: 5.7 KiB, free: 434.1 MiB)\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 95a0e3787e8d:33875 in memory (size: 582.0 B, free: 434.1 MiB)\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 95a0e3787e8d:33875 in memory (size: 2.8 KiB, free: 434.1 MiB)\n",
            "25/08/06 09:09:32 INFO CodeGenerator: Code generated in 160.979558 ms\n",
            "25/08/06 09:09:32 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 201.5 KiB, free 408.4 MiB)\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 95a0e3787e8d:33875 in memory (size: 34.9 KiB, free: 434.1 MiB)\n",
            "25/08/06 09:09:32 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 408.6 MiB)\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 95a0e3787e8d:33875 (size: 34.9 KiB, free: 434.1 MiB)\n",
            "25/08/06 09:09:32 INFO SparkContext: Created broadcast 34 from parquet at RegionalSalesInsights.java:57\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 95a0e3787e8d:33875 in memory (size: 34.8 KiB, free: 434.1 MiB)\n",
            "25/08/06 09:09:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Registering RDD 60 (parquet at RegionalSalesInsights.java:57) as input to shuffle 4\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Got map stage job 18 (parquet at RegionalSalesInsights.java:57) with 1 output partitions\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Final stage: ShuffleMapStage 25 (parquet at RegionalSalesInsights.java:57)\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[60] at parquet at RegionalSalesInsights.java:57), which has no missing parents\n",
            "25/08/06 09:09:32 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 63.6 KiB, free 408.8 MiB)\n",
            "25/08/06 09:09:32 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 25.6 KiB, free 408.8 MiB)\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 95a0e3787e8d:33875 (size: 25.6 KiB, free: 434.1 MiB)\n",
            "25/08/06 09:09:32 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:32 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[60] at parquet at RegionalSalesInsights.java:57) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:32 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:32 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 18) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4966 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:32 INFO Executor: Running task 0.0 in stage 25.0 (TID 18)\n",
            "25/08/06 09:09:32 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 95a0e3787e8d:33875 in memory (size: 34.9 KiB, free: 434.1 MiB)\n",
            "25/08/06 09:09:33 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 95a0e3787e8d:33875 in memory (size: 96.9 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:09:33 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-b524bbe9-c037-49cb-aeb1-7f1d63d317e5-c000.snappy.parquet, range: 0-15479, partition values: [empty row]\n",
            "25/08/06 09:09:33 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 09:09:33 INFO Executor: Finished task 0.0 in stage 25.0 (TID 18). 5611 bytes result sent to driver\n",
            "25/08/06 09:09:33 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 18) in 180 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:33 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:33 INFO DAGScheduler: ShuffleMapStage 25 (parquet at RegionalSalesInsights.java:57) finished in 0.197 s\n",
            "25/08/06 09:09:33 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 09:09:33 INFO DAGScheduler: running: Set()\n",
            "25/08/06 09:09:33 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 09:09:33 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 09:09:33 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 09:09:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:09:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:09:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:09:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:09:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:09:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:09:33 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 09:09:33 INFO CodeGenerator: Code generated in 57.1401 ms\n",
            "25/08/06 09:09:33 INFO SparkContext: Starting job: parquet at RegionalSalesInsights.java:57\n",
            "25/08/06 09:09:33 INFO DAGScheduler: Got job 19 (parquet at RegionalSalesInsights.java:57) with 1 output partitions\n",
            "25/08/06 09:09:33 INFO DAGScheduler: Final stage: ResultStage 27 (parquet at RegionalSalesInsights.java:57)\n",
            "25/08/06 09:09:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)\n",
            "25/08/06 09:09:33 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:09:33 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[62] at parquet at RegionalSalesInsights.java:57), which has no missing parents\n",
            "25/08/06 09:09:33 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 255.6 KiB, free 409.1 MiB)\n",
            "25/08/06 09:09:33 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 93.8 KiB, free 409.0 MiB)\n",
            "25/08/06 09:09:33 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 95a0e3787e8d:33875 (size: 93.8 KiB, free: 434.1 MiB)\n",
            "25/08/06 09:09:33 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:09:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[62] at parquet at RegionalSalesInsights.java:57) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:09:33 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:09:33 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 19) (95a0e3787e8d, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:09:33 INFO Executor: Running task 0.0 in stage 27.0 (TID 19)\n",
            "25/08/06 09:09:33 INFO ShuffleBlockFetcherIterator: Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 09:09:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 09:09:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:09:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:09:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:09:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:09:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:09:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:09:33 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 09:09:33 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 09:09:33 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
            "25/08/06 09:09:33 INFO ParquetOutputFormat: Validation is off\n",
            "25/08/06 09:09:33 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "25/08/06 09:09:33 INFO ParquetOutputFormat: Parquet properties are:\n",
            "Parquet page size to 1048576\n",
            "Parquet dictionary page size to 1048576\n",
            "Dictionary is true\n",
            "Writer version is: PARQUET_1_0\n",
            "Page size checking is: estimated\n",
            "Min row count for page size check is: 100\n",
            "Max row count for page size check is: 10000\n",
            "Truncate length for column indexes is: 64\n",
            "Truncate length for statistics min/max  is: 2147483647\n",
            "Bloom filter enabled: false\n",
            "Max Bloom filter size for a column is 1048576\n",
            "Bloom filter expected number of distinct values are: null\n",
            "Page row count limit to 20000\n",
            "Writing page checksums is: on\n",
            "25/08/06 09:09:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"officeCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : false,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"city\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : false,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"country\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : false,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalSales\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  required binary officeCode (STRING);\n",
            "  required binary city (STRING);\n",
            "  required binary country (STRING);\n",
            "  optional double totalSales;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 09:09:33 INFO FileOutputCommitter: Saved output of task 'attempt_202508060909331423630692748345425_0027_m_000000_19' to file:/output/processed/top_offices.parquet/_temporary/0/task_202508060909331423630692748345425_0027_m_000000\n",
            "25/08/06 09:09:33 INFO SparkHadoopMapRedUtil: attempt_202508060909331423630692748345425_0027_m_000000_19: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 09:09:33 INFO Executor: Finished task 0.0 in stage 27.0 (TID 19). 7652 bytes result sent to driver\n",
            "25/08/06 09:09:33 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 19) in 106 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:09:33 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:09:33 INFO DAGScheduler: ResultStage 27 (parquet at RegionalSalesInsights.java:57) finished in 0.148 s\n",
            "25/08/06 09:09:33 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:09:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished\n",
            "25/08/06 09:09:33 INFO DAGScheduler: Job 19 finished: parquet at RegionalSalesInsights.java:57, took 0.162219 s\n",
            "25/08/06 09:09:33 INFO FileFormatWriter: Start to commit write Job 042a8140-7a3d-4db8-85c2-f1ea249d88a2.\n",
            "25/08/06 09:09:33 INFO FileFormatWriter: Write Job 042a8140-7a3d-4db8-85c2-f1ea249d88a2 committed. Elapsed time: 26 ms.\n",
            "25/08/06 09:09:33 INFO FileFormatWriter: Finished processing stats for write job 042a8140-7a3d-4db8-85c2-f1ea249d88a2.\n",
            "25/08/06 09:09:33 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 95a0e3787e8d:33875 in memory (size: 25.6 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:09:33 INFO SparkUI: Stopped Spark web UI at http://95a0e3787e8d:4041\n",
            "25/08/06 09:09:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 09:09:33 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 09:09:33 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 09:09:33 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 09:09:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 09:09:33 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 09:09:33 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 09:09:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-9390acdc-81a6-4390-b614-9dea6a614c44\n",
            "25/08/06 09:09:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-cbb98b14-b7d9-4014-9bb8-b99f7a41e81e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "spark.read.parquet(\"/output/processed/customer_sales_by_office.parquet\").show()\n",
        "spark.read.parquet(\"/output/processed/revenue_by_country.parquet\").show()\n",
        "spark.read.parquet(\"/output/processed/top_offices.parquet\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rku998KmKU4",
        "outputId": "8ec34382-b0ec-4e60-a953-ebf2f9c7a8f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------+---------------+-------------+----------+\n",
            "|    officeCode|        city|        country|customerCount|totalSales|\n",
            "+--------------+------------+---------------+-------------+----------+\n",
            "|Unknown Office|Unknown City|Unknown Country|           98|8853839.23|\n",
            "+--------------+------------+---------------+-------------+----------+\n",
            "\n",
            "+-----------+------------+\n",
            "|    country|totalRevenue|\n",
            "+-----------+------------+\n",
            "|        USA|  3040029.52|\n",
            "|      Spain|   994438.53|\n",
            "|     France|   965750.58|\n",
            "|  Australia|   509385.82|\n",
            "|New Zealand|   392486.59|\n",
            "|         UK|    391503.9|\n",
            "|      Italy|   325254.55|\n",
            "|    Finland|   295149.35|\n",
            "|  Singapore|    261671.6|\n",
            "|     Canada|   205911.86|\n",
            "|    Denmark|    197356.3|\n",
            "|    Germany|   196470.99|\n",
            "|      Japan|   167909.95|\n",
            "|   Norway  |   166621.51|\n",
            "|    Austria|   136119.99|\n",
            "|     Sweden|   120457.09|\n",
            "|Switzerland|   108777.92|\n",
            "|     Norway|   104224.79|\n",
            "|    Belgium|    91471.03|\n",
            "|Philippines|     87468.3|\n",
            "+-----------+------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------+------------+---------------+----------+\n",
            "|    officeCode|        city|        country|totalSales|\n",
            "+--------------+------------+---------------+----------+\n",
            "|Unknown Office|Unknown City|Unknown Country|8853839.23|\n",
            "+--------------+------------+---------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4:"
      ],
      "metadata": {
        "id": "Shel6vP8pYwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/project/src/PerformanceOptimization.java\n",
        "import org.apache.spark.sql.*;\n",
        "import static org.apache.spark.sql.functions.*;\n",
        "import org.apache.spark.api.java.function.MapPartitionsFunction;\n",
        "\n",
        "public class PerformanceOptimization {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "            .appName(\"PerformanceOptimization\")\n",
        "            .master(\"local\")\n",
        "            .getOrCreate();\n",
        "\n",
        "        // 1. Read & cache datasets\n",
        "        Dataset<Row> orderDetails = spark.read().parquet(\"/content/data/parquet/orderdetails\").cache();\n",
        "        Dataset<Row> products     = spark.read().parquet(\"/content/data/parquet/products\").cache();\n",
        "        Dataset<Row> offices      = spark.read().parquet(\"/content/data/parquet/offices\");\n",
        "        Dataset<Row> orders       = spark.read().parquet(\"/content/data/parquet/orders\");\n",
        "        Dataset<Row> customers    = spark.read().parquet(\"/content/data/parquet/customers\");\n",
        "        Dataset<Row> employees    = spark.read().parquet(\"/content/data/parquet/employees\");\n",
        "\n",
        "        // 2. Trigger lazy evaluation\n",
        "        Dataset<Row> cached = orderDetails\n",
        "            .join(products, \"productCode\")\n",
        "            .withColumn(\"lineRevenue\", col(\"quantityOrdered\").multiply(col(\"priceEach\")))\n",
        "            .cache();\n",
        "        cached.count();\n",
        "\n",
        "        // 3. Broadcast join for office sales\n",
        "        Dataset<Row> officeSales = orders\n",
        "            .join(customers, \"customerNumber\")\n",
        "            .join(employees,\n",
        "                  customers.col(\"salesRepEmployeeNumber\")\n",
        "                           .equalTo(employees.col(\"employeeNumber\")))\n",
        "            .join(broadcast(offices),\n",
        "                  employees.col(\"officeCode\")\n",
        "                           .equalTo(offices.col(\"officeCode\")))\n",
        "            .groupBy(offices.col(\"officeCode\"),\n",
        "                     offices.col(\"city\"),\n",
        "                     offices.col(\"country\"))\n",
        "            .agg(count(\"*\").alias(\"orderCount\"));\n",
        "\n",
        "        officeSales.show();\n",
        "\n",
        "        // 4. mapPartitions example\n",
        "        Dataset<String> productNames = products.mapPartitions(\n",
        "            new MapPartitionsFunction<Row, String>() {\n",
        "                @Override\n",
        "                public java.util.Iterator<String> call(java.util.Iterator<Row> it) {\n",
        "                    java.util.List<String> out = new java.util.ArrayList<>();\n",
        "                    while (it.hasNext()) {\n",
        "                        Row r = it.next();\n",
        "                        out.add(r.getAs(\"productName\").toString().toUpperCase());\n",
        "                    }\n",
        "                    return out.iterator();\n",
        "                }\n",
        "            }, Encoders.STRING()\n",
        "        );\n",
        "        productNames.show();\n",
        "\n",
        "        // 5. Write outputs\n",
        "        officeSales.write().mode(\"overwrite\")\n",
        "            .parquet(\"/content/output/processed/employee_sales_summary.parquet\");\n",
        "        productNames.write().mode(\"overwrite\")\n",
        "            .parquet(\"/content/output/processed/product_names_uppercase.parquet\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAD4nbDEpcSD",
        "outputId": "7a372c84-0c02-4fd2-a083-1bb53218ca7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/project/src/PerformanceOptimization.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile & package as a JAR\n",
        "%cd /content/project/src\n",
        "!javac -cp \"/content/spark-3.3.2-bin-hadoop3/jars/*\" PerformanceOptimization.java\n",
        "!jar cf performance-opt.jar PerformanceOptimization.class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGlS6JEspfRK",
        "outputId": "b26a42db-0fa0-4138-ceec-5a798a08dd17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/project/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run with spark-submit\n",
        "!/content/spark-3.3.2-bin-hadoop3/bin/spark-submit \\\n",
        "  --class PerformanceOptimization \\\n",
        "  --master local \\\n",
        "  --jars /content/spark-3.3.2-bin-hadoop3/jars/*,/content/project/src \\\n",
        "  performance-opt.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Zq3PneqvpkNT",
        "outputId": "5161467d-d968-46e8-8cd5-df66d839ea05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/08/06 09:12:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 09:12:21 INFO SparkContext: Running Spark version 3.3.2\n",
            "25/08/06 09:12:21 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 09:12:21 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 09:12:21 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 09:12:21 INFO SparkContext: Submitted application: PerformanceOptimization\n",
            "25/08/06 09:12:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 09:12:21 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 09:12:21 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 09:12:21 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 09:12:21 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 09:12:21 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 09:12:21 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 09:12:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "25/08/06 09:12:22 INFO Utils: Successfully started service 'sparkDriver' on port 45387.\n",
            "25/08/06 09:12:22 INFO SparkEnv: Registering MapOutputTracker\n",
            "25/08/06 09:12:22 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 09:12:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 09:12:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 09:12:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 09:12:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e315d6d4-f0de-48ad-a684-549f3b49b452\n",
            "25/08/06 09:12:22 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "25/08/06 09:12:22 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 09:12:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/06 09:12:22 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/06 09:12:22 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/HikariCP-2.5.1.jar at spark://95a0e3787e8d:45387/jars/HikariCP-2.5.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/JLargeArrays-1.5.jar at spark://95a0e3787e8d:45387/jars/JLargeArrays-1.5.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/JTransforms-3.1.jar at spark://95a0e3787e8d:45387/jars/JTransforms-3.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/RoaringBitmap-0.9.25.jar at spark://95a0e3787e8d:45387/jars/RoaringBitmap-0.9.25.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/ST4-4.0.4.jar at spark://95a0e3787e8d:45387/jars/ST4-4.0.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/activation-1.1.1.jar at spark://95a0e3787e8d:45387/jars/activation-1.1.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/aircompressor-0.21.jar at spark://95a0e3787e8d:45387/jars/aircompressor-0.21.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/algebra_2.12-2.0.1.jar at spark://95a0e3787e8d:45387/jars/algebra_2.12-2.0.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/annotations-17.0.0.jar at spark://95a0e3787e8d:45387/jars/annotations-17.0.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/antlr-runtime-3.5.2.jar at spark://95a0e3787e8d:45387/jars/antlr-runtime-3.5.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/antlr4-runtime-4.8.jar at spark://95a0e3787e8d:45387/jars/antlr4-runtime-4.8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/aopalliance-repackaged-2.6.1.jar at spark://95a0e3787e8d:45387/jars/aopalliance-repackaged-2.6.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/arpack-2.2.1.jar at spark://95a0e3787e8d:45387/jars/arpack-2.2.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/arpack_combined_all-0.1.jar at spark://95a0e3787e8d:45387/jars/arpack_combined_all-0.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/arrow-format-7.0.0.jar at spark://95a0e3787e8d:45387/jars/arrow-format-7.0.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/arrow-memory-core-7.0.0.jar at spark://95a0e3787e8d:45387/jars/arrow-memory-core-7.0.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/arrow-memory-netty-7.0.0.jar at spark://95a0e3787e8d:45387/jars/arrow-memory-netty-7.0.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/arrow-vector-7.0.0.jar at spark://95a0e3787e8d:45387/jars/arrow-vector-7.0.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/audience-annotations-0.5.0.jar at spark://95a0e3787e8d:45387/jars/audience-annotations-0.5.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/automaton-1.11-8.jar at spark://95a0e3787e8d:45387/jars/automaton-1.11-8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/avro-1.11.0.jar at spark://95a0e3787e8d:45387/jars/avro-1.11.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/avro-ipc-1.11.0.jar at spark://95a0e3787e8d:45387/jars/avro-ipc-1.11.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/avro-mapred-1.11.0.jar at spark://95a0e3787e8d:45387/jars/avro-mapred-1.11.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/blas-2.2.1.jar at spark://95a0e3787e8d:45387/jars/blas-2.2.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/bonecp-0.8.0.RELEASE.jar at spark://95a0e3787e8d:45387/jars/bonecp-0.8.0.RELEASE.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/breeze-macros_2.12-1.2.jar at spark://95a0e3787e8d:45387/jars/breeze-macros_2.12-1.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/breeze_2.12-1.2.jar at spark://95a0e3787e8d:45387/jars/breeze_2.12-1.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/cats-kernel_2.12-2.1.1.jar at spark://95a0e3787e8d:45387/jars/cats-kernel_2.12-2.1.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/chill-java-0.10.0.jar at spark://95a0e3787e8d:45387/jars/chill-java-0.10.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/chill_2.12-0.10.0.jar at spark://95a0e3787e8d:45387/jars/chill_2.12-0.10.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/commons-cli-1.5.0.jar at spark://95a0e3787e8d:45387/jars/commons-cli-1.5.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/commons-codec-1.15.jar at spark://95a0e3787e8d:45387/jars/commons-codec-1.15.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/commons-collections-3.2.2.jar at spark://95a0e3787e8d:45387/jars/commons-collections-3.2.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/commons-collections4-4.4.jar at spark://95a0e3787e8d:45387/jars/commons-collections4-4.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/commons-compiler-3.0.16.jar at spark://95a0e3787e8d:45387/jars/commons-compiler-3.0.16.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/commons-compress-1.21.jar at spark://95a0e3787e8d:45387/jars/commons-compress-1.21.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/commons-crypto-1.1.0.jar at spark://95a0e3787e8d:45387/jars/commons-crypto-1.1.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/commons-dbcp-1.4.jar at spark://95a0e3787e8d:45387/jars/commons-dbcp-1.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/commons-io-2.11.0.jar at spark://95a0e3787e8d:45387/jars/commons-io-2.11.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/commons-lang-2.6.jar at spark://95a0e3787e8d:45387/jars/commons-lang-2.6.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/commons-lang3-3.12.0.jar at spark://95a0e3787e8d:45387/jars/commons-lang3-3.12.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/commons-logging-1.1.3.jar at spark://95a0e3787e8d:45387/jars/commons-logging-1.1.3.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/commons-math3-3.6.1.jar at spark://95a0e3787e8d:45387/jars/commons-math3-3.6.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/commons-pool-1.5.4.jar at spark://95a0e3787e8d:45387/jars/commons-pool-1.5.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/commons-text-1.10.0.jar at spark://95a0e3787e8d:45387/jars/commons-text-1.10.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/compress-lzf-1.1.jar at spark://95a0e3787e8d:45387/jars/compress-lzf-1.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/core-1.1.2.jar at spark://95a0e3787e8d:45387/jars/core-1.1.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/curator-client-2.13.0.jar at spark://95a0e3787e8d:45387/jars/curator-client-2.13.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/curator-framework-2.13.0.jar at spark://95a0e3787e8d:45387/jars/curator-framework-2.13.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/curator-recipes-2.13.0.jar at spark://95a0e3787e8d:45387/jars/curator-recipes-2.13.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/datanucleus-api-jdo-4.2.4.jar at spark://95a0e3787e8d:45387/jars/datanucleus-api-jdo-4.2.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/datanucleus-core-4.1.17.jar at spark://95a0e3787e8d:45387/jars/datanucleus-core-4.1.17.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/datanucleus-rdbms-4.1.19.jar at spark://95a0e3787e8d:45387/jars/datanucleus-rdbms-4.1.19.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/derby-10.14.2.0.jar at spark://95a0e3787e8d:45387/jars/derby-10.14.2.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar at spark://95a0e3787e8d:45387/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/flatbuffers-java-1.12.0.jar at spark://95a0e3787e8d:45387/jars/flatbuffers-java-1.12.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/generex-1.0.2.jar at spark://95a0e3787e8d:45387/jars/generex-1.0.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/gson-2.2.4.jar at spark://95a0e3787e8d:45387/jars/gson-2.2.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/guava-14.0.1.jar at spark://95a0e3787e8d:45387/jars/guava-14.0.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hadoop-client-api-3.3.2.jar at spark://95a0e3787e8d:45387/jars/hadoop-client-api-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hadoop-client-runtime-3.3.2.jar at spark://95a0e3787e8d:45387/jars/hadoop-client-runtime-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hadoop-shaded-guava-1.1.1.jar at spark://95a0e3787e8d:45387/jars/hadoop-shaded-guava-1.1.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hadoop-yarn-server-web-proxy-3.3.2.jar at spark://95a0e3787e8d:45387/jars/hadoop-yarn-server-web-proxy-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hive-beeline-2.3.9.jar at spark://95a0e3787e8d:45387/jars/hive-beeline-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hive-cli-2.3.9.jar at spark://95a0e3787e8d:45387/jars/hive-cli-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hive-common-2.3.9.jar at spark://95a0e3787e8d:45387/jars/hive-common-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hive-exec-2.3.9-core.jar at spark://95a0e3787e8d:45387/jars/hive-exec-2.3.9-core.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hive-jdbc-2.3.9.jar at spark://95a0e3787e8d:45387/jars/hive-jdbc-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hive-llap-common-2.3.9.jar at spark://95a0e3787e8d:45387/jars/hive-llap-common-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hive-metastore-2.3.9.jar at spark://95a0e3787e8d:45387/jars/hive-metastore-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hive-serde-2.3.9.jar at spark://95a0e3787e8d:45387/jars/hive-serde-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hive-service-rpc-3.1.2.jar at spark://95a0e3787e8d:45387/jars/hive-service-rpc-3.1.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hive-shims-0.23-2.3.9.jar at spark://95a0e3787e8d:45387/jars/hive-shims-0.23-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hive-shims-2.3.9.jar at spark://95a0e3787e8d:45387/jars/hive-shims-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hive-shims-common-2.3.9.jar at spark://95a0e3787e8d:45387/jars/hive-shims-common-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hive-shims-scheduler-2.3.9.jar at spark://95a0e3787e8d:45387/jars/hive-shims-scheduler-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hive-storage-api-2.7.2.jar at spark://95a0e3787e8d:45387/jars/hive-storage-api-2.7.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hive-vector-code-gen-2.3.9.jar at spark://95a0e3787e8d:45387/jars/hive-vector-code-gen-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hk2-api-2.6.1.jar at spark://95a0e3787e8d:45387/jars/hk2-api-2.6.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hk2-locator-2.6.1.jar at spark://95a0e3787e8d:45387/jars/hk2-locator-2.6.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/hk2-utils-2.6.1.jar at spark://95a0e3787e8d:45387/jars/hk2-utils-2.6.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/httpclient-4.5.13.jar at spark://95a0e3787e8d:45387/jars/httpclient-4.5.13.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/httpcore-4.4.14.jar at spark://95a0e3787e8d:45387/jars/httpcore-4.4.14.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/istack-commons-runtime-3.0.8.jar at spark://95a0e3787e8d:45387/jars/istack-commons-runtime-3.0.8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/ivy-2.5.1.jar at spark://95a0e3787e8d:45387/jars/ivy-2.5.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jackson-annotations-2.13.4.jar at spark://95a0e3787e8d:45387/jars/jackson-annotations-2.13.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jackson-core-2.13.4.jar at spark://95a0e3787e8d:45387/jars/jackson-core-2.13.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jackson-core-asl-1.9.13.jar at spark://95a0e3787e8d:45387/jars/jackson-core-asl-1.9.13.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jackson-databind-2.13.4.2.jar at spark://95a0e3787e8d:45387/jars/jackson-databind-2.13.4.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jackson-dataformat-yaml-2.13.4.jar at spark://95a0e3787e8d:45387/jars/jackson-dataformat-yaml-2.13.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jackson-datatype-jsr310-2.13.4.jar at spark://95a0e3787e8d:45387/jars/jackson-datatype-jsr310-2.13.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jackson-mapper-asl-1.9.13.jar at spark://95a0e3787e8d:45387/jars/jackson-mapper-asl-1.9.13.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jackson-module-scala_2.12-2.13.4.jar at spark://95a0e3787e8d:45387/jars/jackson-module-scala_2.12-2.13.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jakarta.annotation-api-1.3.5.jar at spark://95a0e3787e8d:45387/jars/jakarta.annotation-api-1.3.5.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jakarta.inject-2.6.1.jar at spark://95a0e3787e8d:45387/jars/jakarta.inject-2.6.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jakarta.servlet-api-4.0.3.jar at spark://95a0e3787e8d:45387/jars/jakarta.servlet-api-4.0.3.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jakarta.validation-api-2.0.2.jar at spark://95a0e3787e8d:45387/jars/jakarta.validation-api-2.0.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jakarta.ws.rs-api-2.1.6.jar at spark://95a0e3787e8d:45387/jars/jakarta.ws.rs-api-2.1.6.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jakarta.xml.bind-api-2.3.2.jar at spark://95a0e3787e8d:45387/jars/jakarta.xml.bind-api-2.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/janino-3.0.16.jar at spark://95a0e3787e8d:45387/jars/janino-3.0.16.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/javassist-3.25.0-GA.jar at spark://95a0e3787e8d:45387/jars/javassist-3.25.0-GA.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/javax.jdo-3.2.0-m3.jar at spark://95a0e3787e8d:45387/jars/javax.jdo-3.2.0-m3.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/javolution-5.5.1.jar at spark://95a0e3787e8d:45387/jars/javolution-5.5.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jaxb-runtime-2.3.2.jar at spark://95a0e3787e8d:45387/jars/jaxb-runtime-2.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jcl-over-slf4j-1.7.32.jar at spark://95a0e3787e8d:45387/jars/jcl-over-slf4j-1.7.32.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jdo-api-3.0.1.jar at spark://95a0e3787e8d:45387/jars/jdo-api-3.0.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jersey-client-2.36.jar at spark://95a0e3787e8d:45387/jars/jersey-client-2.36.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jersey-common-2.36.jar at spark://95a0e3787e8d:45387/jars/jersey-common-2.36.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jersey-container-servlet-2.36.jar at spark://95a0e3787e8d:45387/jars/jersey-container-servlet-2.36.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jersey-container-servlet-core-2.36.jar at spark://95a0e3787e8d:45387/jars/jersey-container-servlet-core-2.36.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jersey-hk2-2.36.jar at spark://95a0e3787e8d:45387/jars/jersey-hk2-2.36.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jersey-server-2.36.jar at spark://95a0e3787e8d:45387/jars/jersey-server-2.36.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jline-2.14.6.jar at spark://95a0e3787e8d:45387/jars/jline-2.14.6.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/joda-time-2.10.13.jar at spark://95a0e3787e8d:45387/jars/joda-time-2.10.13.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jodd-core-3.5.2.jar at spark://95a0e3787e8d:45387/jars/jodd-core-3.5.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jpam-1.1.jar at spark://95a0e3787e8d:45387/jars/jpam-1.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/json-1.8.jar at spark://95a0e3787e8d:45387/jars/json-1.8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/json4s-ast_2.12-3.7.0-M11.jar at spark://95a0e3787e8d:45387/jars/json4s-ast_2.12-3.7.0-M11.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/json4s-core_2.12-3.7.0-M11.jar at spark://95a0e3787e8d:45387/jars/json4s-core_2.12-3.7.0-M11.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/json4s-jackson_2.12-3.7.0-M11.jar at spark://95a0e3787e8d:45387/jars/json4s-jackson_2.12-3.7.0-M11.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/json4s-scalap_2.12-3.7.0-M11.jar at spark://95a0e3787e8d:45387/jars/json4s-scalap_2.12-3.7.0-M11.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jsr305-3.0.0.jar at spark://95a0e3787e8d:45387/jars/jsr305-3.0.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jta-1.1.jar at spark://95a0e3787e8d:45387/jars/jta-1.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/jul-to-slf4j-1.7.32.jar at spark://95a0e3787e8d:45387/jars/jul-to-slf4j-1.7.32.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kryo-shaded-4.0.2.jar at spark://95a0e3787e8d:45387/jars/kryo-shaded-4.0.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-client-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-client-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-admissionregistration-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-admissionregistration-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-apiextensions-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-apiextensions-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-apps-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-apps-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-autoscaling-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-autoscaling-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-batch-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-batch-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-certificates-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-certificates-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-common-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-common-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-coordination-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-coordination-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-core-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-core-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-discovery-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-discovery-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-events-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-events-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-extensions-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-extensions-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-flowcontrol-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-flowcontrol-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-metrics-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-metrics-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-networking-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-networking-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-node-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-node-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-policy-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-policy-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-rbac-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-rbac-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-scheduling-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-scheduling-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/kubernetes-model-storageclass-5.12.2.jar at spark://95a0e3787e8d:45387/jars/kubernetes-model-storageclass-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/lapack-2.2.1.jar at spark://95a0e3787e8d:45387/jars/lapack-2.2.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/leveldbjni-all-1.8.jar at spark://95a0e3787e8d:45387/jars/leveldbjni-all-1.8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/libfb303-0.9.3.jar at spark://95a0e3787e8d:45387/jars/libfb303-0.9.3.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/libthrift-0.12.0.jar at spark://95a0e3787e8d:45387/jars/libthrift-0.12.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/log4j-1.2-api-2.17.2.jar at spark://95a0e3787e8d:45387/jars/log4j-1.2-api-2.17.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/log4j-api-2.17.2.jar at spark://95a0e3787e8d:45387/jars/log4j-api-2.17.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/log4j-core-2.17.2.jar at spark://95a0e3787e8d:45387/jars/log4j-core-2.17.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/log4j-slf4j-impl-2.17.2.jar at spark://95a0e3787e8d:45387/jars/log4j-slf4j-impl-2.17.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/logging-interceptor-3.12.12.jar at spark://95a0e3787e8d:45387/jars/logging-interceptor-3.12.12.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/lz4-java-1.8.0.jar at spark://95a0e3787e8d:45387/jars/lz4-java-1.8.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/mesos-1.4.3-shaded-protobuf.jar at spark://95a0e3787e8d:45387/jars/mesos-1.4.3-shaded-protobuf.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/metrics-core-4.2.7.jar at spark://95a0e3787e8d:45387/jars/metrics-core-4.2.7.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/metrics-graphite-4.2.7.jar at spark://95a0e3787e8d:45387/jars/metrics-graphite-4.2.7.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/metrics-jmx-4.2.7.jar at spark://95a0e3787e8d:45387/jars/metrics-jmx-4.2.7.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/metrics-json-4.2.7.jar at spark://95a0e3787e8d:45387/jars/metrics-json-4.2.7.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/metrics-jvm-4.2.7.jar at spark://95a0e3787e8d:45387/jars/metrics-jvm-4.2.7.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/minlog-1.3.0.jar at spark://95a0e3787e8d:45387/jars/minlog-1.3.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/netty-all-4.1.74.Final.jar at spark://95a0e3787e8d:45387/jars/netty-all-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/netty-buffer-4.1.74.Final.jar at spark://95a0e3787e8d:45387/jars/netty-buffer-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/netty-codec-4.1.74.Final.jar at spark://95a0e3787e8d:45387/jars/netty-codec-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/netty-common-4.1.74.Final.jar at spark://95a0e3787e8d:45387/jars/netty-common-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/netty-handler-4.1.74.Final.jar at spark://95a0e3787e8d:45387/jars/netty-handler-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/netty-resolver-4.1.74.Final.jar at spark://95a0e3787e8d:45387/jars/netty-resolver-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/netty-tcnative-classes-2.0.48.Final.jar at spark://95a0e3787e8d:45387/jars/netty-tcnative-classes-2.0.48.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/netty-transport-4.1.74.Final.jar at spark://95a0e3787e8d:45387/jars/netty-transport-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/netty-transport-classes-epoll-4.1.74.Final.jar at spark://95a0e3787e8d:45387/jars/netty-transport-classes-epoll-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/netty-transport-classes-kqueue-4.1.74.Final.jar at spark://95a0e3787e8d:45387/jars/netty-transport-classes-kqueue-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/netty-transport-native-epoll-4.1.74.Final-linux-aarch_64.jar at spark://95a0e3787e8d:45387/jars/netty-transport-native-epoll-4.1.74.Final-linux-aarch_64.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/netty-transport-native-epoll-4.1.74.Final-linux-x86_64.jar at spark://95a0e3787e8d:45387/jars/netty-transport-native-epoll-4.1.74.Final-linux-x86_64.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/netty-transport-native-kqueue-4.1.74.Final-osx-aarch_64.jar at spark://95a0e3787e8d:45387/jars/netty-transport-native-kqueue-4.1.74.Final-osx-aarch_64.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/netty-transport-native-kqueue-4.1.74.Final-osx-x86_64.jar at spark://95a0e3787e8d:45387/jars/netty-transport-native-kqueue-4.1.74.Final-osx-x86_64.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/netty-transport-native-unix-common-4.1.74.Final.jar at spark://95a0e3787e8d:45387/jars/netty-transport-native-unix-common-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/objenesis-3.2.jar at spark://95a0e3787e8d:45387/jars/objenesis-3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/okhttp-3.12.12.jar at spark://95a0e3787e8d:45387/jars/okhttp-3.12.12.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/okio-1.14.0.jar at spark://95a0e3787e8d:45387/jars/okio-1.14.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/opencsv-2.3.jar at spark://95a0e3787e8d:45387/jars/opencsv-2.3.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/orc-core-1.7.8.jar at spark://95a0e3787e8d:45387/jars/orc-core-1.7.8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/orc-mapreduce-1.7.8.jar at spark://95a0e3787e8d:45387/jars/orc-mapreduce-1.7.8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/orc-shims-1.7.8.jar at spark://95a0e3787e8d:45387/jars/orc-shims-1.7.8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/oro-2.0.8.jar at spark://95a0e3787e8d:45387/jars/oro-2.0.8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/osgi-resource-locator-1.0.3.jar at spark://95a0e3787e8d:45387/jars/osgi-resource-locator-1.0.3.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/paranamer-2.8.jar at spark://95a0e3787e8d:45387/jars/paranamer-2.8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/parquet-column-1.12.2.jar at spark://95a0e3787e8d:45387/jars/parquet-column-1.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/parquet-common-1.12.2.jar at spark://95a0e3787e8d:45387/jars/parquet-common-1.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/parquet-encoding-1.12.2.jar at spark://95a0e3787e8d:45387/jars/parquet-encoding-1.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/parquet-format-structures-1.12.2.jar at spark://95a0e3787e8d:45387/jars/parquet-format-structures-1.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/parquet-hadoop-1.12.2.jar at spark://95a0e3787e8d:45387/jars/parquet-hadoop-1.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/parquet-jackson-1.12.2.jar at spark://95a0e3787e8d:45387/jars/parquet-jackson-1.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/pickle-1.2.jar at spark://95a0e3787e8d:45387/jars/pickle-1.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/protobuf-java-2.5.0.jar at spark://95a0e3787e8d:45387/jars/protobuf-java-2.5.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/py4j-0.10.9.5.jar at spark://95a0e3787e8d:45387/jars/py4j-0.10.9.5.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/rocksdbjni-6.20.3.jar at spark://95a0e3787e8d:45387/jars/rocksdbjni-6.20.3.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/scala-collection-compat_2.12-2.1.1.jar at spark://95a0e3787e8d:45387/jars/scala-collection-compat_2.12-2.1.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/scala-compiler-2.12.15.jar at spark://95a0e3787e8d:45387/jars/scala-compiler-2.12.15.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/scala-library-2.12.15.jar at spark://95a0e3787e8d:45387/jars/scala-library-2.12.15.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/scala-parser-combinators_2.12-1.1.2.jar at spark://95a0e3787e8d:45387/jars/scala-parser-combinators_2.12-1.1.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/scala-reflect-2.12.15.jar at spark://95a0e3787e8d:45387/jars/scala-reflect-2.12.15.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/scala-xml_2.12-1.2.0.jar at spark://95a0e3787e8d:45387/jars/scala-xml_2.12-1.2.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/shapeless_2.12-2.3.7.jar at spark://95a0e3787e8d:45387/jars/shapeless_2.12-2.3.7.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/shims-0.9.25.jar at spark://95a0e3787e8d:45387/jars/shims-0.9.25.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/slf4j-api-1.7.32.jar at spark://95a0e3787e8d:45387/jars/slf4j-api-1.7.32.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/snakeyaml-1.31.jar at spark://95a0e3787e8d:45387/jars/snakeyaml-1.31.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/snappy-java-1.1.8.4.jar at spark://95a0e3787e8d:45387/jars/snappy-java-1.1.8.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-catalyst_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-catalyst_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-core_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-core_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-graphx_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-graphx_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-hive-thriftserver_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-hive-thriftserver_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-hive_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-hive_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-kubernetes_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-kubernetes_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-kvstore_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-kvstore_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-launcher_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-launcher_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-mesos_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-mesos_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-mllib-local_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-mllib-local_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-mllib_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-mllib_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-network-common_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-network-common_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-network-shuffle_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-network-shuffle_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-repl_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-repl_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-sketch_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-sketch_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-sql_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-sql_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-streaming_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-streaming_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-tags_2.12-3.3.2-tests.jar at spark://95a0e3787e8d:45387/jars/spark-tags_2.12-3.3.2-tests.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-tags_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-tags_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-unsafe_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-unsafe_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spark-yarn_2.12-3.3.2.jar at spark://95a0e3787e8d:45387/jars/spark-yarn_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spire-macros_2.12-0.17.0.jar at spark://95a0e3787e8d:45387/jars/spire-macros_2.12-0.17.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spire-platform_2.12-0.17.0.jar at spark://95a0e3787e8d:45387/jars/spire-platform_2.12-0.17.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spire-util_2.12-0.17.0.jar at spark://95a0e3787e8d:45387/jars/spire-util_2.12-0.17.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/spire_2.12-0.17.0.jar at spark://95a0e3787e8d:45387/jars/spire_2.12-0.17.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/stax-api-1.0.1.jar at spark://95a0e3787e8d:45387/jars/stax-api-1.0.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/stream-2.9.6.jar at spark://95a0e3787e8d:45387/jars/stream-2.9.6.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/super-csv-2.2.0.jar at spark://95a0e3787e8d:45387/jars/super-csv-2.2.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/threeten-extra-1.5.0.jar at spark://95a0e3787e8d:45387/jars/threeten-extra-1.5.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/tink-1.6.1.jar at spark://95a0e3787e8d:45387/jars/tink-1.6.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/transaction-api-1.1.jar at spark://95a0e3787e8d:45387/jars/transaction-api-1.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/univocity-parsers-2.9.1.jar at spark://95a0e3787e8d:45387/jars/univocity-parsers-2.9.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/velocity-1.5.jar at spark://95a0e3787e8d:45387/jars/velocity-1.5.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/xbean-asm9-shaded-4.20.jar at spark://95a0e3787e8d:45387/jars/xbean-asm9-shaded-4.20.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/xz-1.9.jar at spark://95a0e3787e8d:45387/jars/xz-1.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/zjsonpatch-0.3.0.jar at spark://95a0e3787e8d:45387/jars/zjsonpatch-0.3.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/zookeeper-3.6.2.jar at spark://95a0e3787e8d:45387/jars/zookeeper-3.6.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/zookeeper-jute-3.6.2.jar at spark://95a0e3787e8d:45387/jars/zookeeper-jute-3.6.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/zstd-jni-1.5.2-1.jar at spark://95a0e3787e8d:45387/jars/zstd-jni-1.5.2-1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO SparkContext: Added JAR file:/content/project/src/performance-opt.jar at spark://95a0e3787e8d:45387/jars/performance-opt.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO Executor: Starting executor ID driver on host 95a0e3787e8d\n",
            "25/08/06 09:12:23 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 09:12:23 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-hive-thriftserver_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO TransportClientFactory: Successfully created connection to 95a0e3787e8d/172.28.0.12:45387 after 49 ms (0 ms spent in bootstraps)\n",
            "25/08/06 09:12:23 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-hive-thriftserver_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp8471718652981811400.tmp\n",
            "25/08/06 09:12:23 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-hive-thriftserver_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:23 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jersey-server-2.36.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jersey-server-2.36.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp13437935069479879221.tmp\n",
            "25/08/06 09:12:23 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jersey-server-2.36.jar to class loader\n",
            "25/08/06 09:12:23 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/netty-handler-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/netty-handler-4.1.74.Final.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp5065910945313418723.tmp\n",
            "25/08/06 09:12:23 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/netty-handler-4.1.74.Final.jar to class loader\n",
            "25/08/06 09:12:23 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/generex-1.0.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/generex-1.0.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp7795144337769492504.tmp\n",
            "25/08/06 09:12:23 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/generex-1.0.2.jar to class loader\n",
            "25/08/06 09:12:23 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/super-csv-2.2.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/super-csv-2.2.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp17428422247998128717.tmp\n",
            "25/08/06 09:12:23 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/super-csv-2.2.0.jar to class loader\n",
            "25/08/06 09:12:23 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jakarta.validation-api-2.0.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jakarta.validation-api-2.0.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp15114466004815939483.tmp\n",
            "25/08/06 09:12:23 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jakarta.validation-api-2.0.2.jar to class loader\n",
            "25/08/06 09:12:23 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hive-cli-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hive-cli-2.3.9.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp16746233052492111158.tmp\n",
            "25/08/06 09:12:23 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hive-cli-2.3.9.jar to class loader\n",
            "25/08/06 09:12:23 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/netty-transport-classes-kqueue-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/netty-transport-classes-kqueue-4.1.74.Final.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp15303113388448631375.tmp\n",
            "25/08/06 09:12:23 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/netty-transport-classes-kqueue-4.1.74.Final.jar to class loader\n",
            "25/08/06 09:12:23 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/osgi-resource-locator-1.0.3.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/osgi-resource-locator-1.0.3.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp16173206068592373120.tmp\n",
            "25/08/06 09:12:23 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/osgi-resource-locator-1.0.3.jar to class loader\n",
            "25/08/06 09:12:23 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-streaming_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-streaming_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6985934203286930455.tmp\n",
            "25/08/06 09:12:23 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-streaming_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:23 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/antlr-runtime-3.5.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/antlr-runtime-3.5.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp5871367816518665793.tmp\n",
            "25/08/06 09:12:23 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/antlr-runtime-3.5.2.jar to class loader\n",
            "25/08/06 09:12:23 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/log4j-core-2.17.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/log4j-core-2.17.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp9542163337691716227.tmp\n",
            "25/08/06 09:12:23 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/log4j-core-2.17.2.jar to class loader\n",
            "25/08/06 09:12:23 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/curator-recipes-2.13.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/curator-recipes-2.13.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp8540850275865935471.tmp\n",
            "25/08/06 09:12:23 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/curator-recipes-2.13.0.jar to class loader\n",
            "25/08/06 09:12:23 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-node-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-node-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp7097498339535526578.tmp\n",
            "25/08/06 09:12:23 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-node-5.12.2.jar to class loader\n",
            "25/08/06 09:12:23 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/snappy-java-1.1.8.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/snappy-java-1.1.8.4.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp11886321882461582999.tmp\n",
            "25/08/06 09:12:23 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/snappy-java-1.1.8.4.jar to class loader\n",
            "25/08/06 09:12:23 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/aopalliance-repackaged-2.6.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/aopalliance-repackaged-2.6.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp17602700743570760153.tmp\n",
            "25/08/06 09:12:23 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/aopalliance-repackaged-2.6.1.jar to class loader\n",
            "25/08/06 09:12:23 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/avro-1.11.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/avro-1.11.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp241652964533968987.tmp\n",
            "25/08/06 09:12:23 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/avro-1.11.0.jar to class loader\n",
            "25/08/06 09:12:23 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jersey-container-servlet-core-2.36.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jersey-container-servlet-core-2.36.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp14625907598233734575.tmp\n",
            "25/08/06 09:12:23 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jersey-container-servlet-core-2.36.jar to class loader\n",
            "25/08/06 09:12:23 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/arrow-memory-core-7.0.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:23 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/arrow-memory-core-7.0.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6193014452328520270.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/arrow-memory-core-7.0.0.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/scala-compiler-2.12.15.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/scala-compiler-2.12.15.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp10994953193673331565.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/scala-compiler-2.12.15.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-metrics-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-metrics-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6252297715550773429.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-metrics-5.12.2.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/objenesis-3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/objenesis-3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp8104785307158630158.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/objenesis-3.2.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/zookeeper-3.6.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/zookeeper-3.6.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp1211748417772220594.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/zookeeper-3.6.2.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/arrow-memory-netty-7.0.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/arrow-memory-netty-7.0.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6233211618172065842.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/arrow-memory-netty-7.0.0.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hive-jdbc-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hive-jdbc-2.3.9.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp9606489596780976070.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hive-jdbc-2.3.9.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jersey-client-2.36.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jersey-client-2.36.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp2107104752628113042.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jersey-client-2.36.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-sql_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-sql_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp2702273680910796899.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-sql_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/avro-mapred-1.11.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/avro-mapred-1.11.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp17039605959073940145.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/avro-mapred-1.11.0.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hk2-locator-2.6.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hk2-locator-2.6.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp5277902467128684278.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hk2-locator-2.6.1.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-apps-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-apps-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp14380996835855673884.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-apps-5.12.2.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/scala-reflect-2.12.15.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/scala-reflect-2.12.15.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp9461995742661337787.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/scala-reflect-2.12.15.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-core_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-core_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp7980437885444058017.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-core_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jersey-hk2-2.36.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jersey-hk2-2.36.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp9652013651976925911.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jersey-hk2-2.36.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/scala-xml_2.12-1.2.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/scala-xml_2.12-1.2.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp14327287446368357769.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/scala-xml_2.12-1.2.0.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/antlr4-runtime-4.8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/antlr4-runtime-4.8.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp11046142051455061921.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/antlr4-runtime-4.8.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hive-storage-api-2.7.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hive-storage-api-2.7.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp8243269399152211073.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hive-storage-api-2.7.2.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/blas-2.2.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/blas-2.2.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp14631861439539346076.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/blas-2.2.1.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jaxb-runtime-2.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jaxb-runtime-2.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp4081539418905065121.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jaxb-runtime-2.3.2.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/algebra_2.12-2.0.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/algebra_2.12-2.0.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp7453395255249814035.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/algebra_2.12-2.0.1.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jodd-core-3.5.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jodd-core-3.5.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp4811411753043948435.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jodd-core-3.5.2.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hadoop-yarn-server-web-proxy-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hadoop-yarn-server-web-proxy-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp17056395487407396821.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hadoop-yarn-server-web-proxy-3.3.2.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-unsafe_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-unsafe_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6269752913541386847.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-unsafe_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/netty-resolver-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/netty-resolver-4.1.74.Final.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp4626038556139451241.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/netty-resolver-4.1.74.Final.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/httpcore-4.4.14.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/httpcore-4.4.14.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp17093340928992723946.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/httpcore-4.4.14.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/audience-annotations-0.5.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/audience-annotations-0.5.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12813535593673976185.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/audience-annotations-0.5.0.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/flatbuffers-java-1.12.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/flatbuffers-java-1.12.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp10983170683627380635.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/flatbuffers-java-1.12.0.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/commons-pool-1.5.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/commons-pool-1.5.4.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp17812950643871407671.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/commons-pool-1.5.4.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/libfb303-0.9.3.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/libfb303-0.9.3.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp17379288815659524864.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/libfb303-0.9.3.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/breeze_2.12-1.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/breeze_2.12-1.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp2076063908348981531.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/breeze_2.12-1.2.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/datanucleus-api-jdo-4.2.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/datanucleus-api-jdo-4.2.4.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12272958851868129305.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/datanucleus-api-jdo-4.2.4.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-tags_2.12-3.3.2-tests.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-tags_2.12-3.3.2-tests.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12206207160648615261.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-tags_2.12-3.3.2-tests.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/guava-14.0.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/guava-14.0.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp8003083047897415775.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/guava-14.0.1.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/commons-compiler-3.0.16.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/commons-compiler-3.0.16.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp11141252768865309461.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/commons-compiler-3.0.16.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-catalyst_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-catalyst_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp1655127776930888512.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-catalyst_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jul-to-slf4j-1.7.32.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jul-to-slf4j-1.7.32.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp11832622702773026425.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jul-to-slf4j-1.7.32.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/log4j-slf4j-impl-2.17.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/log4j-slf4j-impl-2.17.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp3722419425013489298.tmp\n",
            "25/08/06 09:12:24 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/log4j-slf4j-impl-2.17.2.jar to class loader\n",
            "25/08/06 09:12:24 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/arrow-vector-7.0.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:24 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/arrow-vector-7.0.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp16552990345748445925.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/arrow-vector-7.0.0.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spire_2.12-0.17.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spire_2.12-0.17.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp7486541904826579679.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spire_2.12-0.17.0.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/pickle-1.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/pickle-1.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6775041952005927593.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/pickle-1.2.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jackson-databind-2.13.4.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jackson-databind-2.13.4.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp11629688456799919470.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jackson-databind-2.13.4.2.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/orc-shims-1.7.8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/orc-shims-1.7.8.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp10112343763871993602.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/orc-shims-1.7.8.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hive-service-rpc-3.1.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hive-service-rpc-3.1.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp8767441365266602981.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hive-service-rpc-3.1.2.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/log4j-1.2-api-2.17.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/log4j-1.2-api-2.17.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp5883034150243268960.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/log4j-1.2-api-2.17.2.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/stax-api-1.0.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/stax-api-1.0.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp9924868890308705192.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/stax-api-1.0.1.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/commons-collections4-4.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/commons-collections4-4.4.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp10790339656702963495.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/commons-collections4-4.4.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/activation-1.1.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/activation-1.1.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12071479851305291880.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/activation-1.1.1.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spire-util_2.12-0.17.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spire-util_2.12-0.17.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12635287720638475341.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spire-util_2.12-0.17.0.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/arrow-format-7.0.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/arrow-format-7.0.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp1450404295999108549.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/arrow-format-7.0.0.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/datanucleus-rdbms-4.1.19.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/datanucleus-rdbms-4.1.19.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp8557136519694179508.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/datanucleus-rdbms-4.1.19.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/commons-compress-1.21.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/commons-compress-1.21.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp3341636605694560562.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/commons-compress-1.21.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/lapack-2.2.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/lapack-2.2.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6294278061188733157.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/lapack-2.2.1.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hive-serde-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hive-serde-2.3.9.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp16035424858032841459.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hive-serde-2.3.9.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/janino-3.0.16.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/janino-3.0.16.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp17735861909115165505.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/janino-3.0.16.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-admissionregistration-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-admissionregistration-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp17835483274717474550.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-admissionregistration-5.12.2.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-networking-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-networking-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp14292656266553475269.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-networking-5.12.2.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jackson-core-asl-1.9.13.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jackson-core-asl-1.9.13.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6739142943710486811.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jackson-core-asl-1.9.13.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/netty-common-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/netty-common-4.1.74.Final.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp16989330700030142088.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/netty-common-4.1.74.Final.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hive-shims-common-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hive-shims-common-2.3.9.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp9251288824976747520.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hive-shims-common-2.3.9.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jakarta.inject-2.6.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jakarta.inject-2.6.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp998810829619486576.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jakarta.inject-2.6.1.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/metrics-jvm-4.2.7.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/metrics-jvm-4.2.7.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp14166848864204693236.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/metrics-jvm-4.2.7.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/parquet-format-structures-1.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/parquet-format-structures-1.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp1235014824729563712.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/parquet-format-structures-1.12.2.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hive-metastore-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hive-metastore-2.3.9.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp8634644453929198014.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hive-metastore-2.3.9.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/netty-all-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/netty-all-4.1.74.Final.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6045261014437355966.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/netty-all-4.1.74.Final.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-autoscaling-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-autoscaling-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12632143286611377430.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-autoscaling-5.12.2.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jackson-dataformat-yaml-2.13.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jackson-dataformat-yaml-2.13.4.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp16699448812385364764.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jackson-dataformat-yaml-2.13.4.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-policy-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-policy-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp11447427177482878698.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-policy-5.12.2.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/stream-2.9.6.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/stream-2.9.6.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp13183660482397761913.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/stream-2.9.6.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/commons-crypto-1.1.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/commons-crypto-1.1.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp2897159723885111944.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/commons-crypto-1.1.0.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jakarta.xml.bind-api-2.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jakarta.xml.bind-api-2.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp9935661403490147731.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jakarta.xml.bind-api-2.3.2.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/xz-1.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/xz-1.9.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp5581169598829914530.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/xz-1.9.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/commons-dbcp-1.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/commons-dbcp-1.4.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp10542672049746476007.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/commons-dbcp-1.4.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/annotations-17.0.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/annotations-17.0.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp5138685695395572810.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/annotations-17.0.0.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-batch-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-batch-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp5835928289284139446.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-batch-5.12.2.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp4965956554147635431.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/JTransforms-3.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/JTransforms-3.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp14372803080614667728.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/JTransforms-3.1.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-tags_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-tags_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6244765161375160864.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-tags_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/netty-transport-native-epoll-4.1.74.Final-linux-x86_64.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/netty-transport-native-epoll-4.1.74.Final-linux-x86_64.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp7216703399029482107.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/netty-transport-native-epoll-4.1.74.Final-linux-x86_64.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/core-1.1.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/core-1.1.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6266112049972511136.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/core-1.1.2.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/commons-lang3-3.12.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/commons-lang3-3.12.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12743090499653524484.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/commons-lang3-3.12.0.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jta-1.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jta-1.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp18355275218081805012.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jta-1.1.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/json-1.8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/json-1.8.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp14071709370239732872.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/json-1.8.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-network-common_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-network-common_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp8454413679621263672.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-network-common_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/scala-library-2.12.15.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/scala-library-2.12.15.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp2132095421354946935.tmp\n",
            "25/08/06 09:12:25 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/scala-library-2.12.15.jar to class loader\n",
            "25/08/06 09:12:25 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/curator-client-2.13.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:25 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/curator-client-2.13.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12058184952998906534.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/curator-client-2.13.0.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jline-2.14.6.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jline-2.14.6.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp16483551372095299410.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jline-2.14.6.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/commons-text-1.10.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/commons-text-1.10.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp15345496524448638462.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/commons-text-1.10.0.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-mesos_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-mesos_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12209446021056534928.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-mesos_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jackson-module-scala_2.12-2.13.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jackson-module-scala_2.12-2.13.4.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp17708891351193764666.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jackson-module-scala_2.12-2.13.4.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/compress-lzf-1.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/compress-lzf-1.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp14273195609387985849.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/compress-lzf-1.1.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/json4s-jackson_2.12-3.7.0-M11.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/json4s-jackson_2.12-3.7.0-M11.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp11026075997554571469.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/json4s-jackson_2.12-3.7.0-M11.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/httpclient-4.5.13.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/httpclient-4.5.13.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp17029728484068620722.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/httpclient-4.5.13.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/netty-transport-native-kqueue-4.1.74.Final-osx-x86_64.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/netty-transport-native-kqueue-4.1.74.Final-osx-x86_64.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp10679959358391051138.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/netty-transport-native-kqueue-4.1.74.Final-osx-x86_64.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-scheduling-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-scheduling-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp4975692977429629247.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-scheduling-5.12.2.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/automaton-1.11-8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/automaton-1.11-8.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp10483673552492586825.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/automaton-1.11-8.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-kubernetes_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-kubernetes_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp1069940128650057329.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-kubernetes_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/istack-commons-runtime-3.0.8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/istack-commons-runtime-3.0.8.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp7369035591193334828.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/istack-commons-runtime-3.0.8.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jackson-annotations-2.13.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jackson-annotations-2.13.4.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp8009026830943169104.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jackson-annotations-2.13.4.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jackson-core-2.13.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jackson-core-2.13.4.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp14594666332883856335.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jackson-core-2.13.4.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spire-platform_2.12-0.17.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spire-platform_2.12-0.17.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp7667713551707826993.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spire-platform_2.12-0.17.0.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/netty-codec-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/netty-codec-4.1.74.Final.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp14261743524613634984.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/netty-codec-4.1.74.Final.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/xbean-asm9-shaded-4.20.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/xbean-asm9-shaded-4.20.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp1604414209071682464.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/xbean-asm9-shaded-4.20.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/velocity-1.5.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/velocity-1.5.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp7070752018892427811.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/velocity-1.5.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-apiextensions-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-apiextensions-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12588183914081996389.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-apiextensions-5.12.2.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/logging-interceptor-3.12.12.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/logging-interceptor-3.12.12.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp4587093907319240144.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/logging-interceptor-3.12.12.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/netty-buffer-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/netty-buffer-4.1.74.Final.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp3305585880066625954.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/netty-buffer-4.1.74.Final.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hk2-api-2.6.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hk2-api-2.6.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp1014738507391203384.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hk2-api-2.6.1.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/RoaringBitmap-0.9.25.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/RoaringBitmap-0.9.25.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12655221615389273783.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/RoaringBitmap-0.9.25.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/commons-io-2.11.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/commons-io-2.11.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12152991737433686176.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/commons-io-2.11.0.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/log4j-api-2.17.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/log4j-api-2.17.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp15684992751402095390.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/log4j-api-2.17.2.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/joda-time-2.10.13.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/joda-time-2.10.13.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp17768424363821504695.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/joda-time-2.10.13.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-discovery-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-discovery-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp10668722350236993020.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-discovery-5.12.2.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hive-common-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hive-common-2.3.9.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp1300891874403807570.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hive-common-2.3.9.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/commons-cli-1.5.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/commons-cli-1.5.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6266760926523918205.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/commons-cli-1.5.0.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/json4s-ast_2.12-3.7.0-M11.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/json4s-ast_2.12-3.7.0-M11.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp10927490010752124140.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/json4s-ast_2.12-3.7.0-M11.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-yarn_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-yarn_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp16938122092850327895.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-yarn_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-rbac-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-rbac-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp1515425890166101228.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-rbac-5.12.2.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/netty-transport-native-epoll-4.1.74.Final-linux-aarch_64.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/netty-transport-native-epoll-4.1.74.Final-linux-aarch_64.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp13871741303687687207.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/netty-transport-native-epoll-4.1.74.Final-linux-aarch_64.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hive-llap-common-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hive-llap-common-2.3.9.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp17374823052182951939.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hive-llap-common-2.3.9.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/netty-tcnative-classes-2.0.48.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/netty-tcnative-classes-2.0.48.Final.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6662963211142532459.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/netty-tcnative-classes-2.0.48.Final.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-sketch_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-sketch_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp4395369088217046379.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-sketch_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/netty-transport-classes-epoll-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/netty-transport-classes-epoll-4.1.74.Final.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp9823985223689715334.tmp\n",
            "25/08/06 09:12:26 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/netty-transport-classes-epoll-4.1.74.Final.jar to class loader\n",
            "25/08/06 09:12:26 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hadoop-client-runtime-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:26 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hadoop-client-runtime-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6773369897189381547.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hadoop-client-runtime-3.3.2.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jpam-1.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jpam-1.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp15638182580027184425.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jpam-1.1.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-hive_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-hive_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp9159700444619605886.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-hive_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/avro-ipc-1.11.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/avro-ipc-1.11.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp16002879254706769473.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/avro-ipc-1.11.0.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hk2-utils-2.6.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hk2-utils-2.6.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12309503819958717808.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hk2-utils-2.6.1.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/ivy-2.5.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/ivy-2.5.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp2036445096163328175.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/ivy-2.5.1.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/gson-2.2.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/gson-2.2.4.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp7478659749344418597.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/gson-2.2.4.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hive-shims-scheduler-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hive-shims-scheduler-2.3.9.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp13034866401739376956.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hive-shims-scheduler-2.3.9.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-events-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-events-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp947192635430162851.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-events-5.12.2.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/scala-collection-compat_2.12-2.1.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/scala-collection-compat_2.12-2.1.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp153619277054394236.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/scala-collection-compat_2.12-2.1.1.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/metrics-graphite-4.2.7.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/metrics-graphite-4.2.7.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp18113068162401791580.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/metrics-graphite-4.2.7.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/protobuf-java-2.5.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/protobuf-java-2.5.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp3290534801483315941.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/protobuf-java-2.5.0.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jakarta.annotation-api-1.3.5.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jakarta.annotation-api-1.3.5.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp305928610167035786.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jakarta.annotation-api-1.3.5.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-extensions-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-extensions-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12468346969834957887.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-extensions-5.12.2.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/arpack_combined_all-0.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/arpack_combined_all-0.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp9481347031284742998.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/arpack_combined_all-0.1.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/javassist-3.25.0-GA.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/javassist-3.25.0-GA.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp646503323429711107.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/javassist-3.25.0-GA.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/shapeless_2.12-2.3.7.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/shapeless_2.12-2.3.7.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp13455466134882347293.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/shapeless_2.12-2.3.7.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/datanucleus-core-4.1.17.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/datanucleus-core-4.1.17.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp15589420760592853454.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/datanucleus-core-4.1.17.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/performance-opt.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/performance-opt.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp7294383678437492510.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/performance-opt.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-certificates-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-certificates-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp11287212663599923724.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-certificates-5.12.2.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jcl-over-slf4j-1.7.32.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jcl-over-slf4j-1.7.32.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp8092358710903218211.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jcl-over-slf4j-1.7.32.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/tink-1.6.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/tink-1.6.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp13962464077887478840.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/tink-1.6.1.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/shims-0.9.25.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/shims-0.9.25.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6712510702694869787.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/shims-0.9.25.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/curator-framework-2.13.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/curator-framework-2.13.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp5843654134304626443.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/curator-framework-2.13.0.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jersey-container-servlet-2.36.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jersey-container-servlet-2.36.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp14359704248421178043.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jersey-container-servlet-2.36.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/HikariCP-2.5.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/HikariCP-2.5.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp2772072094768967167.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/HikariCP-2.5.1.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/chill-java-0.10.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/chill-java-0.10.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6326635115010491605.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/chill-java-0.10.0.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-launcher_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-launcher_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp14482186284427060253.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-launcher_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/commons-collections-3.2.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/commons-collections-3.2.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp15466237693612216557.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/commons-collections-3.2.2.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hive-exec-2.3.9-core.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hive-exec-2.3.9-core.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp15736165342506865753.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hive-exec-2.3.9-core.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/okhttp-3.12.12.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/okhttp-3.12.12.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp5903849990920659412.tmp\n",
            "25/08/06 09:12:27 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/okhttp-3.12.12.jar to class loader\n",
            "25/08/06 09:12:27 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/rocksdbjni-6.20.3.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:27 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/rocksdbjni-6.20.3.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12001002806735547124.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/rocksdbjni-6.20.3.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/commons-lang-2.6.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/commons-lang-2.6.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp7722450728444286019.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/commons-lang-2.6.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/parquet-column-1.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/parquet-column-1.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp16479444526775728867.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/parquet-column-1.12.2.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/JLargeArrays-1.5.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/JLargeArrays-1.5.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp7379817241313786807.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/JLargeArrays-1.5.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/slf4j-api-1.7.32.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/slf4j-api-1.7.32.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp11358939814893235368.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/slf4j-api-1.7.32.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-coordination-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-coordination-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12249142266737442709.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-coordination-5.12.2.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/scala-parser-combinators_2.12-1.1.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/scala-parser-combinators_2.12-1.1.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp14758366523297515123.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/scala-parser-combinators_2.12-1.1.2.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/javolution-5.5.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/javolution-5.5.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp9472049443860248094.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/javolution-5.5.1.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/leveldbjni-all-1.8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/leveldbjni-all-1.8.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp16493185983792888977.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/leveldbjni-all-1.8.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/minlog-1.3.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/minlog-1.3.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp8346515049267686028.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/minlog-1.3.0.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-kvstore_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-kvstore_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp10744077911757126776.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-kvstore_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/parquet-encoding-1.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/parquet-encoding-1.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp11328979687447700083.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/parquet-encoding-1.12.2.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/netty-transport-native-kqueue-4.1.74.Final-osx-aarch_64.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/netty-transport-native-kqueue-4.1.74.Final-osx-aarch_64.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6808539960208992277.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/netty-transport-native-kqueue-4.1.74.Final-osx-aarch_64.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hadoop-shaded-guava-1.1.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hadoop-shaded-guava-1.1.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp2376481569068360728.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hadoop-shaded-guava-1.1.1.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/lz4-java-1.8.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/lz4-java-1.8.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp8833185926792475496.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/lz4-java-1.8.0.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-client-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-client-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp2616199435231818979.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-client-5.12.2.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-core-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-core-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp7721402646113695229.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-core-5.12.2.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/parquet-hadoop-1.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/parquet-hadoop-1.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp8291802577690877992.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/parquet-hadoop-1.12.2.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jakarta.servlet-api-4.0.3.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jakarta.servlet-api-4.0.3.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6597190065117387534.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jakarta.servlet-api-4.0.3.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-mllib-local_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-mllib-local_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp18349253978377716868.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-mllib-local_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/netty-transport-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/netty-transport-4.1.74.Final.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp2691646312138118734.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/netty-transport-4.1.74.Final.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jdo-api-3.0.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jdo-api-3.0.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp8909932353252369608.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jdo-api-3.0.1.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/cats-kernel_2.12-2.1.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/cats-kernel_2.12-2.1.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12766196530594356681.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/cats-kernel_2.12-2.1.1.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-graphx_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-graphx_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp11699483728072541742.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-graphx_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/metrics-json-4.2.7.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/metrics-json-4.2.7.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp1197718287661006216.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/metrics-json-4.2.7.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/breeze-macros_2.12-1.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/breeze-macros_2.12-1.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp8907342416125076115.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/breeze-macros_2.12-1.2.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/metrics-jmx-4.2.7.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/metrics-jmx-4.2.7.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12537854592552709310.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/metrics-jmx-4.2.7.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/zjsonpatch-0.3.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/zjsonpatch-0.3.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp2172762496575933663.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/zjsonpatch-0.3.0.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/json4s-scalap_2.12-3.7.0-M11.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/json4s-scalap_2.12-3.7.0-M11.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp5782840602333694857.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/json4s-scalap_2.12-3.7.0-M11.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/bonecp-0.8.0.RELEASE.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/bonecp-0.8.0.RELEASE.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp4234155861382885316.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/bonecp-0.8.0.RELEASE.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hive-beeline-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hive-beeline-2.3.9.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp10436645370043070675.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hive-beeline-2.3.9.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/snakeyaml-1.31.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/snakeyaml-1.31.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp3183246089439781063.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/snakeyaml-1.31.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/threeten-extra-1.5.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/threeten-extra-1.5.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp4056370649141386453.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/threeten-extra-1.5.0.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/commons-logging-1.1.3.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/commons-logging-1.1.3.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp677894462789494288.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/commons-logging-1.1.3.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/derby-10.14.2.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/derby-10.14.2.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp882256481509343245.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/derby-10.14.2.0.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/netty-transport-native-unix-common-4.1.74.Final.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/netty-transport-native-unix-common-4.1.74.Final.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp9534083376167092005.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/netty-transport-native-unix-common-4.1.74.Final.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-flowcontrol-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-flowcontrol-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp17377341284920795131.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-flowcontrol-5.12.2.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/libthrift-0.12.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/libthrift-0.12.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp4813712136708044096.tmp\n",
            "25/08/06 09:12:28 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/libthrift-0.12.0.jar to class loader\n",
            "25/08/06 09:12:28 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hadoop-client-api-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:28 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hadoop-client-api-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp2399349304050449807.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hadoop-client-api-3.3.2.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jersey-common-2.36.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jersey-common-2.36.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp11914973147303688375.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jersey-common-2.36.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jsr305-3.0.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jsr305-3.0.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12086558011920007672.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jsr305-3.0.0.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/orc-mapreduce-1.7.8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/orc-mapreduce-1.7.8.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp10327059428370333495.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/orc-mapreduce-1.7.8.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/arpack-2.2.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/arpack-2.2.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp7333315836727692958.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/arpack-2.2.1.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/orc-core-1.7.8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/orc-core-1.7.8.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp13397474304864811673.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/orc-core-1.7.8.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/opencsv-2.3.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/opencsv-2.3.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp5196896220867450329.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/opencsv-2.3.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/parquet-common-1.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/parquet-common-1.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp14416591522420371134.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/parquet-common-1.12.2.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/transaction-api-1.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/transaction-api-1.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp1834719904975948779.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/transaction-api-1.1.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/parquet-jackson-1.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/parquet-jackson-1.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6900979601218985622.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/parquet-jackson-1.12.2.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/zookeeper-jute-3.6.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/zookeeper-jute-3.6.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp13673223786688061304.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/zookeeper-jute-3.6.2.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/javax.jdo-3.2.0-m3.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/javax.jdo-3.2.0-m3.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6032953169498203493.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/javax.jdo-3.2.0-m3.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-network-shuffle_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-network-shuffle_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp1349828234598595927.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-network-shuffle_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kryo-shaded-4.0.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kryo-shaded-4.0.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp4942526767482163851.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kryo-shaded-4.0.2.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/metrics-core-4.2.7.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/metrics-core-4.2.7.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp12749129969157990399.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/metrics-core-4.2.7.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/oro-2.0.8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/oro-2.0.8.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp773339153726078559.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/oro-2.0.8.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/commons-math3-3.6.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/commons-math3-3.6.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp16346192657767212635.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/commons-math3-3.6.1.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jackson-mapper-asl-1.9.13.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jackson-mapper-asl-1.9.13.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp10388665374145184940.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jackson-mapper-asl-1.9.13.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/chill_2.12-0.10.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/chill_2.12-0.10.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp1698700534213720728.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/chill_2.12-0.10.0.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-common-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-common-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp4510669133637488912.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-common-5.12.2.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/aircompressor-0.21.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/aircompressor-0.21.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp2490026231632250734.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/aircompressor-0.21.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/ST4-4.0.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/ST4-4.0.4.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp13108773937170768156.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/ST4-4.0.4.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/zstd-jni-1.5.2-1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/zstd-jni-1.5.2-1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp536769954817967464.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/zstd-jni-1.5.2-1.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/mesos-1.4.3-shaded-protobuf.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/mesos-1.4.3-shaded-protobuf.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6542669595588661992.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/mesos-1.4.3-shaded-protobuf.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/univocity-parsers-2.9.1.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/univocity-parsers-2.9.1.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp15610095963364317137.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/univocity-parsers-2.9.1.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-mllib_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-mllib_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp9192543673793777762.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-mllib_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hive-shims-0.23-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hive-shims-0.23-2.3.9.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp7905152803431525386.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hive-shims-0.23-2.3.9.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-storageclass-5.12.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/kubernetes-model-storageclass-5.12.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp9622717320580812132.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/kubernetes-model-storageclass-5.12.2.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jakarta.ws.rs-api-2.1.6.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jakarta.ws.rs-api-2.1.6.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp11972568678742005210.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jakarta.ws.rs-api-2.1.6.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hive-vector-code-gen-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hive-vector-code-gen-2.3.9.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp302582765041170419.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hive-vector-code-gen-2.3.9.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/jackson-datatype-jsr310-2.13.4.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/jackson-datatype-jsr310-2.13.4.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp18441654523838103533.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/jackson-datatype-jsr310-2.13.4.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/commons-codec-1.15.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/commons-codec-1.15.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp16940548657641074513.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/commons-codec-1.15.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/paranamer-2.8.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/paranamer-2.8.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp17154667772046837921.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/paranamer-2.8.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/okio-1.14.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/okio-1.14.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp2757992424530241143.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/okio-1.14.0.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/json4s-core_2.12-3.7.0-M11.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/json4s-core_2.12-3.7.0-M11.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp5159422441788684470.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/json4s-core_2.12-3.7.0-M11.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spire-macros_2.12-0.17.0.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spire-macros_2.12-0.17.0.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp5689020761420172891.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spire-macros_2.12-0.17.0.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/hive-shims-2.3.9.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/hive-shims-2.3.9.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp6065988684943024556.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/hive-shims-2.3.9.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/py4j-0.10.9.5.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/py4j-0.10.9.5.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp9178302967446318193.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/py4j-0.10.9.5.jar to class loader\n",
            "25/08/06 09:12:29 INFO Executor: Fetching spark://95a0e3787e8d:45387/jars/spark-repl_2.12-3.3.2.jar with timestamp 1754471541157\n",
            "25/08/06 09:12:29 INFO Utils: Fetching spark://95a0e3787e8d:45387/jars/spark-repl_2.12-3.3.2.jar to /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/fetchFileTemp5179587805341013703.tmp\n",
            "25/08/06 09:12:29 INFO Executor: Adding file:/tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6/userFiles-21f6383f-eeed-4115-88ed-2eda8b3dbc17/spark-repl_2.12-3.3.2.jar to class loader\n",
            "25/08/06 09:12:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34363.\n",
            "25/08/06 09:12:30 INFO NettyBlockTransferService: Server created on 95a0e3787e8d:34363\n",
            "25/08/06 09:12:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 09:12:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 95a0e3787e8d, 34363, None)\n",
            "25/08/06 09:12:30 INFO BlockManagerMasterEndpoint: Registering block manager 95a0e3787e8d:34363 with 434.4 MiB RAM, BlockManagerId(driver, 95a0e3787e8d, 34363, None)\n",
            "25/08/06 09:12:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 95a0e3787e8d, 34363, None)\n",
            "25/08/06 09:12:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 95a0e3787e8d, 34363, None)\n",
            "25/08/06 09:12:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 09:12:31 INFO SharedState: Warehouse path is 'file:/content/project/src/spark-warehouse'.\n",
            "25/08/06 09:12:34 INFO InMemoryFileIndex: It took 102 ms to list leaf files for 1 paths.\n",
            "25/08/06 09:12:35 INFO SparkContext: Starting job: parquet at PerformanceOptimization.java:13\n",
            "25/08/06 09:12:35 INFO DAGScheduler: Got job 0 (parquet at PerformanceOptimization.java:13) with 1 output partitions\n",
            "25/08/06 09:12:35 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at PerformanceOptimization.java:13)\n",
            "25/08/06 09:12:35 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:12:35 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:12:35 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at PerformanceOptimization.java:13), which has no missing parents\n",
            "25/08/06 09:12:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 151.1 KiB, free 434.3 MiB)\n",
            "25/08/06 09:12:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 48.8 KiB, free 434.2 MiB)\n",
            "25/08/06 09:12:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 95a0e3787e8d:34363 (size: 48.8 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:12:35 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:12:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at PerformanceOptimization.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:12:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:12:35 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:12:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 09:12:36 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1882 bytes result sent to driver\n",
            "25/08/06 09:12:36 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 999 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:12:36 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:12:36 INFO DAGScheduler: ResultStage 0 (parquet at PerformanceOptimization.java:13) finished in 1.484 s\n",
            "25/08/06 09:12:36 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:12:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 09:12:36 INFO DAGScheduler: Job 0 finished: parquet at PerformanceOptimization.java:13, took 1.628381 s\n",
            "25/08/06 09:12:38 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 95a0e3787e8d:34363 in memory (size: 48.8 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:12:40 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 09:12:40 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 09:12:40 INFO FileSourceStrategy: Output Data Schema: struct<orderNumber: string, productCode: string, quantityOrdered: string, priceEach: string, orderLineNumber: string ... 3 more fields>\n",
            "25/08/06 09:12:40 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "25/08/06 09:12:40 INFO SparkContext: Starting job: parquet at PerformanceOptimization.java:14\n",
            "25/08/06 09:12:40 INFO DAGScheduler: Got job 1 (parquet at PerformanceOptimization.java:14) with 1 output partitions\n",
            "25/08/06 09:12:40 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at PerformanceOptimization.java:14)\n",
            "25/08/06 09:12:40 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:12:40 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:12:40 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at PerformanceOptimization.java:14), which has no missing parents\n",
            "25/08/06 09:12:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 151.1 KiB, free 434.3 MiB)\n",
            "25/08/06 09:12:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 48.8 KiB, free 434.2 MiB)\n",
            "25/08/06 09:12:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 95a0e3787e8d:34363 (size: 48.8 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:12:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:12:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at PerformanceOptimization.java:14) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:12:40 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:12:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4651 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:12:40 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 09:12:40 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2007 bytes result sent to driver\n",
            "25/08/06 09:12:40 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 172 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:12:40 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:12:40 INFO DAGScheduler: ResultStage 1 (parquet at PerformanceOptimization.java:14) finished in 0.239 s\n",
            "25/08/06 09:12:40 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:12:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 09:12:40 INFO DAGScheduler: Job 1 finished: parquet at PerformanceOptimization.java:14, took 0.247736 s\n",
            "25/08/06 09:12:40 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/06 09:12:40 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/06 09:12:40 INFO FileSourceStrategy: Output Data Schema: struct<productCode: string, productName: string, productLine: string, productScale: string, productVendor: string ... 7 more fields>\n",
            "25/08/06 09:12:40 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.\n",
            "25/08/06 09:12:41 INFO SparkContext: Starting job: parquet at PerformanceOptimization.java:15\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Got job 2 (parquet at PerformanceOptimization.java:15) with 1 output partitions\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at PerformanceOptimization.java:15)\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at PerformanceOptimization.java:15), which has no missing parents\n",
            "25/08/06 09:12:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 151.1 KiB, free 434.1 MiB)\n",
            "25/08/06 09:12:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 48.8 KiB, free 434.0 MiB)\n",
            "25/08/06 09:12:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 95a0e3787e8d:34363 (size: 48.8 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:12:41 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at PerformanceOptimization.java:15) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:12:41 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:12:41 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4650 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:12:41 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 09:12:41 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1935 bytes result sent to driver\n",
            "25/08/06 09:12:41 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 66 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:12:41 INFO DAGScheduler: ResultStage 2 (parquet at PerformanceOptimization.java:15) finished in 0.119 s\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:12:41 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:12:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Job 2 finished: parquet at PerformanceOptimization.java:15, took 0.131920 s\n",
            "25/08/06 09:12:41 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.\n",
            "25/08/06 09:12:41 INFO SparkContext: Starting job: parquet at PerformanceOptimization.java:16\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Got job 3 (parquet at PerformanceOptimization.java:16) with 1 output partitions\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at PerformanceOptimization.java:16)\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[7] at parquet at PerformanceOptimization.java:16), which has no missing parents\n",
            "25/08/06 09:12:41 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 151.1 KiB, free 433.9 MiB)\n",
            "25/08/06 09:12:41 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 48.8 KiB, free 433.8 MiB)\n",
            "25/08/06 09:12:41 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 95a0e3787e8d:34363 (size: 48.8 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:12:41 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at parquet at PerformanceOptimization.java:16) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:12:41 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:12:41 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:12:41 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 09:12:41 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1870 bytes result sent to driver\n",
            "25/08/06 09:12:41 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 64 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:12:41 INFO DAGScheduler: ResultStage 3 (parquet at PerformanceOptimization.java:16) finished in 0.105 s\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:12:41 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:12:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Job 3 finished: parquet at PerformanceOptimization.java:16, took 0.115028 s\n",
            "25/08/06 09:12:41 INFO InMemoryFileIndex: It took 11 ms to list leaf files for 1 paths.\n",
            "25/08/06 09:12:41 INFO SparkContext: Starting job: parquet at PerformanceOptimization.java:17\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Got job 4 (parquet at PerformanceOptimization.java:17) with 1 output partitions\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Final stage: ResultStage 4 (parquet at PerformanceOptimization.java:17)\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[9] at parquet at PerformanceOptimization.java:17), which has no missing parents\n",
            "25/08/06 09:12:41 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 151.1 KiB, free 433.7 MiB)\n",
            "25/08/06 09:12:41 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 48.8 KiB, free 433.6 MiB)\n",
            "25/08/06 09:12:41 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 95a0e3787e8d:34363 (size: 48.8 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:12:41 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at parquet at PerformanceOptimization.java:17) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:12:41 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:12:41 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4652 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:12:41 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 09:12:41 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2133 bytes result sent to driver\n",
            "25/08/06 09:12:41 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 49 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:12:41 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:12:41 INFO DAGScheduler: ResultStage 4 (parquet at PerformanceOptimization.java:17) finished in 0.084 s\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:12:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Job 4 finished: parquet at PerformanceOptimization.java:17, took 0.097086 s\n",
            "25/08/06 09:12:41 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.\n",
            "25/08/06 09:12:41 INFO SparkContext: Starting job: parquet at PerformanceOptimization.java:18\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Got job 5 (parquet at PerformanceOptimization.java:18) with 1 output partitions\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Final stage: ResultStage 5 (parquet at PerformanceOptimization.java:18)\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[11] at parquet at PerformanceOptimization.java:18), which has no missing parents\n",
            "25/08/06 09:12:41 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 151.1 KiB, free 433.5 MiB)\n",
            "25/08/06 09:12:41 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 48.8 KiB, free 433.4 MiB)\n",
            "25/08/06 09:12:41 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 95a0e3787e8d:34363 (size: 48.8 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:12:41 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at parquet at PerformanceOptimization.java:18) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:12:41 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:12:41 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4652 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:12:41 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "25/08/06 09:12:41 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1902 bytes result sent to driver\n",
            "25/08/06 09:12:41 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 38 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:12:41 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:12:41 INFO DAGScheduler: ResultStage 5 (parquet at PerformanceOptimization.java:18) finished in 0.087 s\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:12:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "25/08/06 09:12:41 INFO DAGScheduler: Job 5 finished: parquet at PerformanceOptimization.java:18, took 0.120682 s\n",
            "25/08/06 09:12:43 INFO CodeGenerator: Code generated in 258.341721 ms\n",
            "25/08/06 09:12:43 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 95a0e3787e8d:34363 in memory (size: 48.8 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:12:43 INFO CodeGenerator: Code generated in 142.252506 ms\n",
            "25/08/06 09:12:43 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 95a0e3787e8d:34363 in memory (size: 48.8 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:12:43 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 95a0e3787e8d:34363 in memory (size: 48.8 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:12:43 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 95a0e3787e8d:34363 in memory (size: 48.8 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:12:43 INFO CodeGenerator: Code generated in 82.196424 ms\n",
            "25/08/06 09:12:43 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 95a0e3787e8d:34363 in memory (size: 48.8 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:12:43 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 249.9 KiB, free 434.2 MiB)\n",
            "25/08/06 09:12:43 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 47.2 KiB, free 434.1 MiB)\n",
            "25/08/06 09:12:43 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 95a0e3787e8d:34363 (size: 47.2 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:12:43 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:12:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4210971 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:12:43 INFO DefaultCachedBatchSerializer: Predicate isnotnull(productCode#35) generates partition filter: ((productCode.count#829 - productCode.nullCount#828) > 0)\n",
            "25/08/06 09:12:44 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:12:44 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 09:12:44 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 09:12:44 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:12:44 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:12:44 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 09:12:44 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 34.9 KiB, free 434.1 MiB)\n",
            "25/08/06 09:12:44 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 434.1 MiB)\n",
            "25/08/06 09:12:44 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 95a0e3787e8d:34363 (size: 13.4 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:12:44 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:12:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:12:44 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:12:44 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4976 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:12:44 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "25/08/06 09:12:44 INFO FileScanRDD: Reading File path: file:///content/data/parquet/products/part-00000-6f591ce5-8ce6-4705-a661-9754a49f5377-c000.snappy.parquet, range: 0-16667, partition values: [empty row]\n",
            "25/08/06 09:12:45 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 09:12:46 INFO MemoryStore: Block rdd_16_0 stored as values in memory (estimated size 29.4 KiB, free 434.0 MiB)\n",
            "25/08/06 09:12:46 INFO BlockManagerInfo: Added rdd_16_0 in memory on 95a0e3787e8d:34363 (size: 29.4 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:12:46 INFO CodeGenerator: Code generated in 62.091897 ms\n",
            "25/08/06 09:12:46 INFO CodeGenerator: Code generated in 194.361251 ms\n",
            "25/08/06 09:12:46 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 20845 bytes result sent to driver\n",
            "25/08/06 09:12:46 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 2706 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:12:46 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:12:46 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 2.754 s\n",
            "25/08/06 09:12:46 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:12:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "25/08/06 09:12:46 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 2.824276 s\n",
            "25/08/06 09:12:46 INFO CodeGenerator: Code generated in 50.106897 ms\n",
            "25/08/06 09:12:47 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 16.0 MiB, free 418.0 MiB)\n",
            "25/08/06 09:12:47 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 20.6 KiB, free 418.0 MiB)\n",
            "25/08/06 09:12:47 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 95a0e3787e8d:34363 (size: 20.6 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:12:47 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:12:47 INFO CodeGenerator: Code generated in 177.58676 ms\n",
            "25/08/06 09:12:47 INFO CodeGenerator: Code generated in 51.575784 ms\n",
            "25/08/06 09:12:47 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 249.4 KiB, free 417.8 MiB)\n",
            "25/08/06 09:12:47 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 47.2 KiB, free 417.7 MiB)\n",
            "25/08/06 09:12:47 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 95a0e3787e8d:34363 (size: 47.2 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:12:47 INFO SparkContext: Created broadcast 9 from count at PerformanceOptimization.java:25\n",
            "25/08/06 09:12:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4219095 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:12:47 INFO DefaultCachedBatchSerializer: Predicate isnotnull(productCode#1) generates partition filter: ((productCode.count#879 - productCode.nullCount#878) > 0)\n",
            "25/08/06 09:12:47 INFO DAGScheduler: Registering RDD 37 (count at PerformanceOptimization.java:25) as input to shuffle 0\n",
            "25/08/06 09:12:47 INFO DAGScheduler: Got map stage job 7 (count at PerformanceOptimization.java:25) with 1 output partitions\n",
            "25/08/06 09:12:47 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (count at PerformanceOptimization.java:25)\n",
            "25/08/06 09:12:47 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:12:47 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:12:47 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[37] at count at PerformanceOptimization.java:25), which has no missing parents\n",
            "25/08/06 09:12:47 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 43.7 KiB, free 417.7 MiB)\n",
            "25/08/06 09:12:47 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 417.7 MiB)\n",
            "25/08/06 09:12:47 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 95a0e3787e8d:34363 (size: 17.9 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:12:47 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:12:47 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[37] at count at PerformanceOptimization.java:25) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:12:47 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:12:47 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:12:47 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
            "25/08/06 09:12:47 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails/part-00000-d0518666-5369-4332-9ddc-d7a9dd010872-c000.snappy.parquet, range: 0-24791, partition values: [empty row]\n",
            "25/08/06 09:12:48 INFO MemoryStore: Block rdd_26_0 stored as values in memory (estimated size 44.6 KiB, free 417.6 MiB)\n",
            "25/08/06 09:12:48 INFO BlockManagerInfo: Added rdd_26_0 in memory on 95a0e3787e8d:34363 (size: 44.6 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:12:48 INFO CodeGenerator: Code generated in 23.422597 ms\n",
            "25/08/06 09:12:48 INFO CodeGenerator: Code generated in 118.964086 ms\n",
            "25/08/06 09:12:48 INFO MemoryStore: Block rdd_32_0 stored as values in memory (estimated size 140.1 KiB, free 417.5 MiB)\n",
            "25/08/06 09:12:48 INFO BlockManagerInfo: Added rdd_32_0 in memory on 95a0e3787e8d:34363 (size: 140.1 KiB, free: 434.0 MiB)\n",
            "25/08/06 09:12:48 INFO CodeGenerator: Code generated in 14.109497 ms\n",
            "25/08/06 09:12:48 INFO CodeGenerator: Code generated in 42.138592 ms\n",
            "25/08/06 09:12:48 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2769 bytes result sent to driver\n",
            "25/08/06 09:12:48 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1182 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:12:48 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:12:48 INFO DAGScheduler: ShuffleMapStage 7 (count at PerformanceOptimization.java:25) finished in 1.282 s\n",
            "25/08/06 09:12:48 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 09:12:48 INFO DAGScheduler: running: Set()\n",
            "25/08/06 09:12:48 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 09:12:48 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 09:12:49 INFO CodeGenerator: Code generated in 20.369955 ms\n",
            "25/08/06 09:12:49 INFO SparkContext: Starting job: count at PerformanceOptimization.java:25\n",
            "25/08/06 09:12:49 INFO DAGScheduler: Got job 8 (count at PerformanceOptimization.java:25) with 1 output partitions\n",
            "25/08/06 09:12:49 INFO DAGScheduler: Final stage: ResultStage 9 (count at PerformanceOptimization.java:25)\n",
            "25/08/06 09:12:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
            "25/08/06 09:12:49 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:12:49 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[40] at count at PerformanceOptimization.java:25), which has no missing parents\n",
            "25/08/06 09:12:49 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 11.1 KiB, free 417.5 MiB)\n",
            "25/08/06 09:12:49 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 417.5 MiB)\n",
            "25/08/06 09:12:49 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 95a0e3787e8d:34363 (size: 5.5 KiB, free: 434.0 MiB)\n",
            "25/08/06 09:12:49 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:12:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[40] at count at PerformanceOptimization.java:25) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:12:49 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:12:49 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 8) (95a0e3787e8d, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:12:49 INFO Executor: Running task 0.0 in stage 9.0 (TID 8)\n",
            "25/08/06 09:12:49 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 09:12:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 17 ms\n",
            "25/08/06 09:12:49 INFO Executor: Finished task 0.0 in stage 9.0 (TID 8). 2656 bytes result sent to driver\n",
            "25/08/06 09:12:49 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 8) in 110 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:12:49 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:12:49 INFO DAGScheduler: ResultStage 9 (count at PerformanceOptimization.java:25) finished in 0.127 s\n",
            "25/08/06 09:12:49 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:12:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "25/08/06 09:12:49 INFO DAGScheduler: Job 8 finished: count at PerformanceOptimization.java:25, took 0.146929 s\n",
            "25/08/06 09:12:49 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 95a0e3787e8d:34363 in memory (size: 5.5 KiB, free: 434.0 MiB)\n",
            "25/08/06 09:12:49 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 95a0e3787e8d:34363 in memory (size: 17.9 KiB, free: 434.1 MiB)\n",
            "25/08/06 09:12:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber)\n",
            "25/08/06 09:12:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#122)\n",
            "25/08/06 09:12:50 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string>\n",
            "25/08/06 09:12:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber),IsNotNull(salesRepEmployeeNumber)\n",
            "25/08/06 09:12:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#130),isnotnull(salesRepEmployeeNumber#141)\n",
            "25/08/06 09:12:50 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string, salesRepEmployeeNumber: string>\n",
            "25/08/06 09:12:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(employeeNumber),IsNotNull(officeCode)\n",
            "25/08/06 09:12:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(employeeNumber#156),isnotnull(officeCode#161)\n",
            "25/08/06 09:12:50 INFO FileSourceStrategy: Output Data Schema: struct<employeeNumber: string, officeCode: string>\n",
            "25/08/06 09:12:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(officeCode)\n",
            "25/08/06 09:12:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(officeCode#98)\n",
            "25/08/06 09:12:50 INFO FileSourceStrategy: Output Data Schema: struct<officeCode: string, city: string, country: string ... 1 more fields>\n",
            "25/08/06 09:12:50 INFO CodeGenerator: Code generated in 55.626552 ms\n",
            "25/08/06 09:12:50 INFO CodeGenerator: Code generated in 74.394612 ms\n",
            "25/08/06 09:12:50 INFO CodeGenerator: Code generated in 70.044291 ms\n",
            "25/08/06 09:12:50 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 248.9 KiB, free 417.1 MiB)\n",
            "25/08/06 09:12:50 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 248.8 KiB, free 417.1 MiB)\n",
            "25/08/06 09:12:50 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 249.1 KiB, free 416.8 MiB)\n",
            "25/08/06 09:12:50 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 47.1 KiB, free 416.7 MiB)\n",
            "25/08/06 09:12:50 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 47.1 KiB, free 416.7 MiB)\n",
            "25/08/06 09:12:50 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 95a0e3787e8d:34363 (size: 47.1 KiB, free: 434.0 MiB)\n",
            "25/08/06 09:12:50 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 95a0e3787e8d:34363 (size: 47.1 KiB, free: 434.0 MiB)\n",
            "25/08/06 09:12:50 INFO SparkContext: Created broadcast 12 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:12:50 INFO SparkContext: Created broadcast 13 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:12:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197878 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:12:50 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 47.0 KiB, free 416.7 MiB)\n",
            "25/08/06 09:12:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4205959 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:12:50 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 95a0e3787e8d:34363 (size: 47.0 KiB, free: 433.9 MiB)\n",
            "25/08/06 09:12:50 INFO SparkContext: Created broadcast 14 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:12:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197381 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:12:50 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:12:50 INFO DAGScheduler: Got job 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 09:12:50 INFO DAGScheduler: Final stage: ResultStage 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 09:12:50 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:12:50 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:12:50 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[51] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 09:12:50 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 13.6 KiB, free 416.7 MiB)\n",
            "25/08/06 09:12:50 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:12:50 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 416.7 MiB)\n",
            "25/08/06 09:12:50 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 95a0e3787e8d:34363 (size: 6.0 KiB, free: 433.9 MiB)\n",
            "25/08/06 09:12:50 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:12:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[51] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:12:50 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:12:50 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:12:50 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 9) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4977 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:12:50 INFO Executor: Running task 0.0 in stage 10.0 (TID 9)\n",
            "25/08/06 09:12:50 INFO DAGScheduler: Got job 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 09:12:50 INFO DAGScheduler: Final stage: ResultStage 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 09:12:50 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:12:50 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:12:50 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[49] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 09:12:51 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 13.0 KiB, free 416.6 MiB)\n",
            "25/08/06 09:12:51 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 416.6 MiB)\n",
            "25/08/06 09:12:51 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 95a0e3787e8d:34363 (size: 5.9 KiB, free: 433.9 MiB)\n",
            "25/08/06 09:12:51 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:12:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[49] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:12:51 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:12:51 INFO FileScanRDD: Reading File path: file:///content/data/parquet/employees/part-00000-f344b70f-71cc-46aa-9cc7-d45bd5c2daa2-c000.snappy.parquet, range: 0-3574, partition values: [empty row]\n",
            "25/08/06 09:12:51 INFO DAGScheduler: Got job 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 09:12:51 INFO DAGScheduler: Final stage: ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 09:12:51 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:12:51 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:12:51 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[52] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 09:12:51 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 14.2 KiB, free 416.6 MiB)\n",
            "25/08/06 09:12:51 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 416.6 MiB)\n",
            "25/08/06 09:12:51 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 95a0e3787e8d:34363 (size: 6.1 KiB, free: 433.9 MiB)\n",
            "25/08/06 09:12:51 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:12:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[52] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:12:51 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:12:51 INFO FilterCompat: Filtering using predicate: and(noteq(employeeNumber, null), noteq(officeCode, null))\n",
            "25/08/06 09:12:51 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 95a0e3787e8d:34363 in memory (size: 13.4 KiB, free: 433.9 MiB)\n",
            "25/08/06 09:12:51 INFO Executor: Finished task 0.0 in stage 10.0 (TID 9). 2062 bytes result sent to driver\n",
            "25/08/06 09:12:51 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 10) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4974 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:12:51 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 9) in 604 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:12:51 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:12:51 INFO DAGScheduler: ResultStage 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.627 s\n",
            "25/08/06 09:12:51 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:12:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
            "25/08/06 09:12:51 INFO DAGScheduler: Job 9 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.641118 s\n",
            "25/08/06 09:12:51 INFO Executor: Running task 0.0 in stage 11.0 (TID 10)\n",
            "25/08/06 09:12:51 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 16.0 MiB, free 400.7 MiB)\n",
            "25/08/06 09:12:51 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 582.0 B, free 400.7 MiB)\n",
            "25/08/06 09:12:51 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 95a0e3787e8d:34363 (size: 582.0 B, free: 433.9 MiB)\n",
            "25/08/06 09:12:51 INFO SparkContext: Created broadcast 18 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:12:51 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orders/part-00000-14514ce3-7c29-43db-bcb9-4c7b047139dd-c000.snappy.parquet, range: 0-11655, partition values: [empty row]\n",
            "25/08/06 09:12:51 INFO FilterCompat: Filtering using predicate: noteq(customerNumber, null)\n",
            "25/08/06 09:12:51 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber),IsNotNull(salesRepEmployeeNumber)\n",
            "25/08/06 09:12:51 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#130),isnotnull(salesRepEmployeeNumber#141)\n",
            "25/08/06 09:12:51 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string, salesRepEmployeeNumber: string>\n",
            "25/08/06 09:12:51 INFO Executor: Finished task 0.0 in stage 11.0 (TID 10). 3019 bytes result sent to driver\n",
            "25/08/06 09:12:51 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 11) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4975 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:12:51 INFO Executor: Running task 0.0 in stage 12.0 (TID 11)\n",
            "25/08/06 09:12:51 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 10) in 218 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:12:51 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:12:51 INFO DAGScheduler: ResultStage 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.800 s\n",
            "25/08/06 09:12:51 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:12:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
            "25/08/06 09:12:51 INFO DAGScheduler: Job 10 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.832356 s\n",
            "25/08/06 09:12:51 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 16.0 MiB, free 384.7 MiB)\n",
            "25/08/06 09:12:51 INFO FileScanRDD: Reading File path: file:///content/data/parquet/offices/part-00000-6297b028-46cf-4dae-aa50-43a65d6f0ad4-c000.snappy.parquet, range: 0-3077, partition values: [empty row]\n",
            "25/08/06 09:12:51 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 384.6 MiB)\n",
            "25/08/06 09:12:51 INFO FilterCompat: Filtering using predicate: noteq(officeCode, null)\n",
            "25/08/06 09:12:51 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 95a0e3787e8d:34363 (size: 4.2 KiB, free: 433.9 MiB)\n",
            "25/08/06 09:12:51 INFO SparkContext: Created broadcast 19 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:12:51 INFO Executor: Finished task 0.0 in stage 12.0 (TID 11). 2049 bytes result sent to driver\n",
            "25/08/06 09:12:51 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 11) in 132 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:12:51 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:12:51 INFO DAGScheduler: ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.814 s\n",
            "25/08/06 09:12:51 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:12:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
            "25/08/06 09:12:51 INFO DAGScheduler: Job 11 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.960886 s\n",
            "25/08/06 09:12:51 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 95a0e3787e8d:34363 in memory (size: 5.9 KiB, free: 433.9 MiB)\n",
            "25/08/06 09:12:52 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 16.0 MiB, free 368.7 MiB)\n",
            "25/08/06 09:12:52 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 450.0 B, free 368.7 MiB)\n",
            "25/08/06 09:12:52 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber),IsNotNull(salesRepEmployeeNumber)\n",
            "25/08/06 09:12:52 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#130),isnotnull(salesRepEmployeeNumber#141)\n",
            "25/08/06 09:12:52 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string, salesRepEmployeeNumber: string>\n",
            "25/08/06 09:12:52 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 95a0e3787e8d:34363 (size: 450.0 B, free: 433.9 MiB)\n",
            "25/08/06 09:12:52 INFO SparkContext: Created broadcast 20 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:12:52 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 95a0e3787e8d:34363 in memory (size: 6.0 KiB, free: 433.9 MiB)\n",
            "25/08/06 09:12:52 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerNumber),IsNotNull(salesRepEmployeeNumber)\n",
            "25/08/06 09:12:52 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerNumber#130),isnotnull(salesRepEmployeeNumber#141)\n",
            "25/08/06 09:12:52 INFO FileSourceStrategy: Output Data Schema: struct<customerNumber: string, salesRepEmployeeNumber: string>\n",
            "25/08/06 09:12:52 INFO CodeGenerator: Code generated in 257.971988 ms\n",
            "25/08/06 09:12:52 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 249.0 KiB, free 368.4 MiB)\n",
            "25/08/06 09:12:52 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 47.1 KiB, free 368.4 MiB)\n",
            "25/08/06 09:12:52 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 95a0e3787e8d:34363 (size: 47.1 KiB, free: 433.9 MiB)\n",
            "25/08/06 09:12:52 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 95a0e3787e8d:34363 in memory (size: 6.1 KiB, free: 433.9 MiB)\n",
            "25/08/06 09:12:52 INFO SparkContext: Created broadcast 21 from show at PerformanceOptimization.java:41\n",
            "25/08/06 09:12:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4209783 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:12:53 INFO DAGScheduler: Registering RDD 56 (show at PerformanceOptimization.java:41) as input to shuffle 1\n",
            "25/08/06 09:12:53 INFO DAGScheduler: Got map stage job 12 (show at PerformanceOptimization.java:41) with 1 output partitions\n",
            "25/08/06 09:12:53 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (show at PerformanceOptimization.java:41)\n",
            "25/08/06 09:12:53 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:12:53 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:12:53 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[56] at show at PerformanceOptimization.java:41), which has no missing parents\n",
            "25/08/06 09:12:53 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 58.3 KiB, free 368.4 MiB)\n",
            "25/08/06 09:12:53 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 23.6 KiB, free 368.3 MiB)\n",
            "25/08/06 09:12:53 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 95a0e3787e8d:34363 (size: 23.6 KiB, free: 433.9 MiB)\n",
            "25/08/06 09:12:53 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:12:53 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[56] at show at PerformanceOptimization.java:41) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:12:53 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:12:53 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 12) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4966 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:12:53 INFO Executor: Running task 0.0 in stage 13.0 (TID 12)\n",
            "25/08/06 09:12:53 INFO CodeGenerator: Code generated in 89.769623 ms\n",
            "25/08/06 09:12:53 INFO CodeGenerator: Code generated in 42.109465 ms\n",
            "25/08/06 09:12:53 INFO CodeGenerator: Code generated in 35.102759 ms\n",
            "25/08/06 09:12:53 INFO CodeGenerator: Code generated in 38.614153 ms\n",
            "25/08/06 09:12:53 INFO FileScanRDD: Reading File path: file:///content/data/parquet/customers/part-00000-b524bbe9-c037-49cb-aeb1-7f1d63d317e5-c000.snappy.parquet, range: 0-15479, partition values: [empty row]\n",
            "25/08/06 09:12:53 INFO FilterCompat: Filtering using predicate: and(noteq(customerNumber, null), noteq(salesRepEmployeeNumber, null))\n",
            "25/08/06 09:12:53 INFO Executor: Finished task 0.0 in stage 13.0 (TID 12). 5525 bytes result sent to driver\n",
            "25/08/06 09:12:53 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 12) in 650 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:12:53 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:12:53 INFO DAGScheduler: ShuffleMapStage 13 (show at PerformanceOptimization.java:41) finished in 0.696 s\n",
            "25/08/06 09:12:53 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 09:12:53 INFO DAGScheduler: running: Set()\n",
            "25/08/06 09:12:53 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 09:12:53 INFO DAGScheduler: failed: Set()\n",
            "+----------+----+-------+----------+\n",
            "|officeCode|city|country|orderCount|\n",
            "+----------+----+-------+----------+\n",
            "+----------+----+-------+----------+\n",
            "\n",
            "Exception in thread \"main\" java.lang.NoClassDefFoundError: PerformanceOptimization$1\n",
            "\tat PerformanceOptimization.main(PerformanceOptimization.java:44)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n",
            "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\n",
            "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
            "Caused by: java.lang.ClassNotFoundException: PerformanceOptimization$1\n",
            "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n",
            "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n",
            "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n",
            "\t... 13 more\n",
            "25/08/06 09:12:53 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "25/08/06 09:12:53 INFO SparkUI: Stopped Spark web UI at http://95a0e3787e8d:4041\n",
            "25/08/06 09:12:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 09:12:54 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 09:12:54 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 09:12:54 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 09:12:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 09:12:54 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 09:12:54 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 09:12:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-8289dc2e-553d-4079-bac9-5a551da114c6\n",
            "25/08/06 09:12:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-c1f5df08-39d9-4b95-b1a3-1e26cbb5db6a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the Parquet outputs\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"VerifyTask4\").getOrCreate()\n",
        "\n",
        "print(\"=== Office Sales Summary ===\")\n",
        "spark.read.parquet(\"/content/output/processed/employee_sales_summary.parquet\").show()\n",
        "\n",
        "print(\"=== Uppercased Product Names ===\")\n",
        "spark.read.parquet(\"/content/output/processed/product_names_uppercase.parquet\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZYDfYLjpnpo",
        "outputId": "cc90d68a-828d-426d-a700-0d557ec76977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Office Sales Summary ===\n",
            "+----------+-------------+---------+----------+\n",
            "|officeCode|         city|  country|orderCount|\n",
            "+----------+-------------+---------+----------+\n",
            "|         2|       Boston|      USA|        32|\n",
            "|         6|       Sydney|Australia|        38|\n",
            "|         1|San Francisco|      USA|        48|\n",
            "|         3|          NYC|      USA|        39|\n",
            "|         5|        Tokyo|    Japan|        16|\n",
            "|         7|       London|       UK|        47|\n",
            "|         4|        Paris|   France|       106|\n",
            "+----------+-------------+---------+----------+\n",
            "\n",
            "=== Uppercased Product Names ===\n",
            "+--------------------+\n",
            "|               value|\n",
            "+--------------------+\n",
            "|1969 HARLEY DAVID...|\n",
            "|1952 ALPINE RENAU...|\n",
            "|1996 MOTO GUZZI 1...|\n",
            "|2003 HARLEY-DAVID...|\n",
            "| 1972 ALFA ROMEO GTA|\n",
            "|1962 LANCIAA DELT...|\n",
            "|   1968 FORD MUSTANG|\n",
            "|   2001 FERRARI ENZO|\n",
            "|      1958 SETRA BUS|\n",
            "|    2002 SUZUKI XREO|\n",
            "|  1969 CORVAIR MONZA|\n",
            "|  1968 DODGE CHARGER|\n",
            "|    1969 FORD FALCON|\n",
            "|1970 PLYMOUTH HEM...|\n",
            "|   1957 CHEVY PICKUP|\n",
            "|  1969 DODGE CHARGER|\n",
            "|1940 FORD PICKUP ...|\n",
            "|     1993 MAZDA RX-7|\n",
            "|1937 LINCOLN BERLINE|\n",
            "|1936 MERCEDES-BEN...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5:"
      ],
      "metadata": {
        "id": "E5t-mjefqBmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/project/src/ProductDemandAnalysis.java\n",
        "import org.apache.spark.sql.*;\n",
        "import static org.apache.spark.sql.functions.*;\n",
        "\n",
        "public class ProductDemandAnalysis {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "            .appName(\"ProductDemandAnalysis\")\n",
        "            .master(\"local\")\n",
        "            .getOrCreate();\n",
        "\n",
        "        // Read the already‐written Parquet tables\n",
        "        Dataset<Row> orderDetails = spark.read().parquet(\"/content/data/parquet/orderdetails\");\n",
        "        Dataset<Row> products    = spark.read().parquet(\"/content/data/parquet/products\");\n",
        "\n",
        "        // Join and aggregate\n",
        "        Dataset<Row> demand = orderDetails\n",
        "            .join(products.select(\"productCode\",\"productName\"), \"productCode\")\n",
        "            .groupBy(\"productCode\",\"productName\")\n",
        "            .agg(\n",
        "                countDistinct(\"orderNumber\").alias(\"orderCount\"),\n",
        "                sum(\"quantityOrdered\").alias(\"totalOrderedQty\"),\n",
        "                round(avg(\"priceEach\"), 2).alias(\"averagePriceEach\")\n",
        "            )\n",
        "            .orderBy(desc(\"totalOrderedQty\"));\n",
        "\n",
        "        // Show and write to Parquet\n",
        "        demand.show(false);\n",
        "        demand.write().mode(\"overwrite\")\n",
        "              .parquet(\"/content/output/processed/product_demand_summary.parquet\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNjdyEpCqD68",
        "outputId": "fdba11ad-8630-404f-9536-777cbe50e051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/project/src/ProductDemandAnalysis.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/project/src\n",
        "!javac -cp \"/content/spark-3.3.2-bin-hadoop3/jars/*\" ProductDemandAnalysis.java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6BY9ODiqTd2",
        "outputId": "21f43269-4750-4ba4-baab-30d753c498fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/project/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/spark-3.3.2-bin-hadoop3/bin/spark-submit \\\n",
        "  --class ProductDemandAnalysis \\\n",
        "  --master local \\\n",
        "  --conf \"spark.driver.extraClassPath=/content/project/src:/content/spark-3.3.2-bin-hadoop3/jars/*\" \\\n",
        "  --jars /content/spark-3.3.2-bin-hadoop3/jars/* \\\n",
        "  ProductDemandAnalysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5wANkc0xqVN2",
        "outputId": "2adbfc2c-463e-4718-a5e4-3f9fe75008b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/08/06 09:14:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 09:14:57 INFO SparkContext: Running Spark version 3.3.2\n",
            "25/08/06 09:14:57 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 09:14:57 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 09:14:57 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 09:14:57 INFO SparkContext: Submitted application: ProductDemandAnalysis\n",
            "25/08/06 09:14:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 09:14:57 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 09:14:57 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 09:14:57 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 09:14:57 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 09:14:57 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 09:14:57 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 09:14:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "25/08/06 09:14:58 INFO Utils: Successfully started service 'sparkDriver' on port 43215.\n",
            "25/08/06 09:14:58 INFO SparkEnv: Registering MapOutputTracker\n",
            "25/08/06 09:14:58 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 09:14:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 09:14:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 09:14:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 09:14:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-846dc239-2626-448d-b3ed-7f544bdf00b2\n",
            "25/08/06 09:14:58 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "25/08/06 09:14:58 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 09:14:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/06 09:14:59 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/06 09:14:59 INFO SparkContext: Added JAR file:///content/spark-3.3.2-bin-hadoop3/jars/activation-1.1.1.jar at spark://95a0e3787e8d:43215/jars/activation-1.1.1.jar with timestamp 1754471697187\n",
            "25/08/06 09:14:59 INFO SparkContext: Added JAR file:/content/spark-3.3.2-bin-hadoop3/jars/aircompressor-0.21.jar at spark://95a0e3787e8d:43215/jars/aircompressor-0.21.jar with timestamp 1754471697187\n",
            "25/08/06 09:14:59 INFO Executor: Starting executor ID driver on host 95a0e3787e8d\n",
            "25/08/06 09:14:59 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 09:14:59 INFO Executor: Fetching spark://95a0e3787e8d:43215/jars/activation-1.1.1.jar with timestamp 1754471697187\n",
            "25/08/06 09:14:59 INFO TransportClientFactory: Successfully created connection to 95a0e3787e8d/172.28.0.12:43215 after 74 ms (0 ms spent in bootstraps)\n",
            "25/08/06 09:14:59 INFO Utils: Fetching spark://95a0e3787e8d:43215/jars/activation-1.1.1.jar to /tmp/spark-f4bf516e-a02b-4b8b-933b-58e637ef4e46/userFiles-396dda91-4d59-476d-aa92-e48d5a6814a3/fetchFileTemp13741518470684345751.tmp\n",
            "25/08/06 09:14:59 INFO Executor: Adding file:/tmp/spark-f4bf516e-a02b-4b8b-933b-58e637ef4e46/userFiles-396dda91-4d59-476d-aa92-e48d5a6814a3/activation-1.1.1.jar to class loader\n",
            "25/08/06 09:14:59 INFO Executor: Fetching spark://95a0e3787e8d:43215/jars/aircompressor-0.21.jar with timestamp 1754471697187\n",
            "25/08/06 09:14:59 INFO Utils: Fetching spark://95a0e3787e8d:43215/jars/aircompressor-0.21.jar to /tmp/spark-f4bf516e-a02b-4b8b-933b-58e637ef4e46/userFiles-396dda91-4d59-476d-aa92-e48d5a6814a3/fetchFileTemp2929744068250919186.tmp\n",
            "25/08/06 09:14:59 INFO Executor: Adding file:/tmp/spark-f4bf516e-a02b-4b8b-933b-58e637ef4e46/userFiles-396dda91-4d59-476d-aa92-e48d5a6814a3/aircompressor-0.21.jar to class loader\n",
            "25/08/06 09:14:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42569.\n",
            "25/08/06 09:14:59 INFO NettyBlockTransferService: Server created on 95a0e3787e8d:42569\n",
            "25/08/06 09:14:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 09:14:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 95a0e3787e8d, 42569, None)\n",
            "25/08/06 09:14:59 INFO BlockManagerMasterEndpoint: Registering block manager 95a0e3787e8d:42569 with 434.4 MiB RAM, BlockManagerId(driver, 95a0e3787e8d, 42569, None)\n",
            "25/08/06 09:14:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 95a0e3787e8d, 42569, None)\n",
            "25/08/06 09:14:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 95a0e3787e8d, 42569, None)\n",
            "25/08/06 09:15:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 09:15:00 INFO SharedState: Warehouse path is 'file:/content/project/src/spark-warehouse'.\n",
            "25/08/06 09:15:02 INFO InMemoryFileIndex: It took 112 ms to list leaf files for 1 paths.\n",
            "25/08/06 09:15:03 INFO SparkContext: Starting job: parquet at ProductDemandAnalysis.java:12\n",
            "25/08/06 09:15:03 INFO DAGScheduler: Got job 0 (parquet at ProductDemandAnalysis.java:12) with 1 output partitions\n",
            "25/08/06 09:15:03 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at ProductDemandAnalysis.java:12)\n",
            "25/08/06 09:15:03 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:15:03 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:15:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at ProductDemandAnalysis.java:12), which has no missing parents\n",
            "25/08/06 09:15:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 103.1 KiB, free 434.3 MiB)\n",
            "25/08/06 09:15:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.1 KiB, free 434.3 MiB)\n",
            "25/08/06 09:15:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 95a0e3787e8d:42569 (size: 37.1 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:15:03 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:15:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at ProductDemandAnalysis.java:12) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:15:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:15:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:15:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 09:15:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1839 bytes result sent to driver\n",
            "25/08/06 09:15:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 976 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:15:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:15:05 INFO DAGScheduler: ResultStage 0 (parquet at ProductDemandAnalysis.java:12) finished in 1.383 s\n",
            "25/08/06 09:15:05 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:15:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 09:15:05 INFO DAGScheduler: Job 0 finished: parquet at ProductDemandAnalysis.java:12, took 1.495621 s\n",
            "25/08/06 09:15:05 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 95a0e3787e8d:42569 in memory (size: 37.1 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:15:09 INFO InMemoryFileIndex: It took 15 ms to list leaf files for 1 paths.\n",
            "25/08/06 09:15:09 INFO SparkContext: Starting job: parquet at ProductDemandAnalysis.java:13\n",
            "25/08/06 09:15:09 INFO DAGScheduler: Got job 1 (parquet at ProductDemandAnalysis.java:13) with 1 output partitions\n",
            "25/08/06 09:15:09 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at ProductDemandAnalysis.java:13)\n",
            "25/08/06 09:15:09 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:15:09 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:15:09 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at ProductDemandAnalysis.java:13), which has no missing parents\n",
            "25/08/06 09:15:09 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 103.1 KiB, free 434.3 MiB)\n",
            "25/08/06 09:15:09 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 37.1 KiB, free 434.3 MiB)\n",
            "25/08/06 09:15:09 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 95a0e3787e8d:42569 (size: 37.1 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:15:09 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:15:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at ProductDemandAnalysis.java:13) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:15:09 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:15:09 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4651 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:15:09 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 09:15:09 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1964 bytes result sent to driver\n",
            "25/08/06 09:15:09 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 98 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:15:09 INFO DAGScheduler: ResultStage 1 (parquet at ProductDemandAnalysis.java:13) finished in 0.178 s\n",
            "25/08/06 09:15:09 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:15:09 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:15:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 09:15:09 INFO DAGScheduler: Job 1 finished: parquet at ProductDemandAnalysis.java:13, took 0.191613 s\n",
            "25/08/06 09:15:10 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 95a0e3787e8d:42569 in memory (size: 37.1 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:15:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 09:15:11 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1)\n",
            "25/08/06 09:15:11 INFO FileSourceStrategy: Output Data Schema: struct<orderNumber: string, productCode: string, quantityOrdered: string, priceEach: string ... 2 more fields>\n",
            "25/08/06 09:15:12 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 09:15:12 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#10)\n",
            "25/08/06 09:15:12 INFO FileSourceStrategy: Output Data Schema: struct<productCode: string, productName: string>\n",
            "25/08/06 09:15:12 INFO CodeGenerator: Code generated in 323.617786 ms\n",
            "25/08/06 09:15:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)\n",
            "25/08/06 09:15:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 434.2 MiB)\n",
            "25/08/06 09:15:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 95a0e3787e8d:42569 (size: 34.8 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:15:13 INFO SparkContext: Created broadcast 2 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:15:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4210971 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:15:13 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:15:13 INFO DAGScheduler: Got job 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 09:15:13 INFO DAGScheduler: Final stage: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 09:15:13 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:15:13 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:15:13 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 09:15:13 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 13.6 KiB, free 434.2 MiB)\n",
            "25/08/06 09:15:13 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.2 MiB)\n",
            "25/08/06 09:15:13 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 95a0e3787e8d:42569 (size: 6.1 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:15:13 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:15:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:15:13 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:15:13 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4976 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:15:13 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 09:15:13 INFO FileScanRDD: Reading File path: file:///content/data/parquet/products/part-00000-6f591ce5-8ce6-4705-a661-9754a49f5377-c000.snappy.parquet, range: 0-16667, partition values: [empty row]\n",
            "25/08/06 09:15:13 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 09:15:13 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 09:15:14 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 5350 bytes result sent to driver\n",
            "25/08/06 09:15:14 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 803 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:15:14 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:15:14 INFO DAGScheduler: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.841 s\n",
            "25/08/06 09:15:14 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:15:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 09:15:14 INFO DAGScheduler: Job 2 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.853957 s\n",
            "25/08/06 09:15:14 INFO CodeGenerator: Code generated in 46.070863 ms\n",
            "25/08/06 09:15:14 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 16.0 MiB, free 418.1 MiB)\n",
            "25/08/06 09:15:14 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 418.1 MiB)\n",
            "25/08/06 09:15:14 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 95a0e3787e8d:42569 (size: 4.7 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:15:14 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:15:14 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 09:15:14 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1)\n",
            "25/08/06 09:15:14 INFO FileSourceStrategy: Output Data Schema: struct<orderNumber: string, productCode: string, quantityOrdered: string, priceEach: string ... 2 more fields>\n",
            "25/08/06 09:15:14 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 95a0e3787e8d:42569 in memory (size: 6.1 KiB, free: 434.4 MiB)\n",
            "25/08/06 09:15:14 INFO CodeGenerator: Code generated in 348.405611 ms\n",
            "25/08/06 09:15:14 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 201.5 KiB, free 418.0 MiB)\n",
            "25/08/06 09:15:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 417.9 MiB)\n",
            "25/08/06 09:15:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 95a0e3787e8d:42569 (size: 34.8 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:15:15 INFO SparkContext: Created broadcast 5 from show at ProductDemandAnalysis.java:27\n",
            "25/08/06 09:15:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4219095 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:15:15 INFO DAGScheduler: Registering RDD 11 (show at ProductDemandAnalysis.java:27) as input to shuffle 0\n",
            "25/08/06 09:15:15 INFO DAGScheduler: Got map stage job 3 (show at ProductDemandAnalysis.java:27) with 1 output partitions\n",
            "25/08/06 09:15:15 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (show at ProductDemandAnalysis.java:27)\n",
            "25/08/06 09:15:15 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:15:15 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:15:15 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[11] at show at ProductDemandAnalysis.java:27), which has no missing parents\n",
            "25/08/06 09:15:15 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 57.2 KiB, free 417.9 MiB)\n",
            "25/08/06 09:15:15 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 23.6 KiB, free 417.9 MiB)\n",
            "25/08/06 09:15:15 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 95a0e3787e8d:42569 (size: 23.6 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:15:15 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:15:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[11] at show at ProductDemandAnalysis.java:27) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:15:15 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:15:15 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:15:15 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 09:15:15 INFO CodeGenerator: Code generated in 59.65272 ms\n",
            "25/08/06 09:15:15 INFO CodeGenerator: Code generated in 30.497357 ms\n",
            "25/08/06 09:15:15 INFO CodeGenerator: Code generated in 16.640052 ms\n",
            "25/08/06 09:15:15 INFO CodeGenerator: Code generated in 23.857952 ms\n",
            "25/08/06 09:15:15 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails/part-00000-d0518666-5369-4332-9ddc-d7a9dd010872-c000.snappy.parquet, range: 0-24791, partition values: [empty row]\n",
            "25/08/06 09:15:15 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 09:15:16 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3931 bytes result sent to driver\n",
            "25/08/06 09:15:16 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 778 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:15:16 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:15:16 INFO DAGScheduler: ShuffleMapStage 3 (show at ProductDemandAnalysis.java:27) finished in 0.846 s\n",
            "25/08/06 09:15:16 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 09:15:16 INFO DAGScheduler: running: Set()\n",
            "25/08/06 09:15:16 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 09:15:16 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 09:15:16 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 09:15:16 INFO CodeGenerator: Code generated in 192.851997 ms\n",
            "25/08/06 09:15:16 INFO DAGScheduler: Registering RDD 14 (show at ProductDemandAnalysis.java:27) as input to shuffle 1\n",
            "25/08/06 09:15:16 INFO DAGScheduler: Got map stage job 4 (show at ProductDemandAnalysis.java:27) with 1 output partitions\n",
            "25/08/06 09:15:16 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (show at ProductDemandAnalysis.java:27)\n",
            "25/08/06 09:15:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "25/08/06 09:15:16 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:15:16 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[14] at show at ProductDemandAnalysis.java:27), which has no missing parents\n",
            "25/08/06 09:15:16 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 75.1 KiB, free 417.8 MiB)\n",
            "25/08/06 09:15:16 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 28.7 KiB, free 417.7 MiB)\n",
            "25/08/06 09:15:16 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 95a0e3787e8d:42569 (size: 28.7 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:15:16 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:15:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[14] at show at ProductDemandAnalysis.java:27) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:15:16 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:15:16 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (95a0e3787e8d, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:15:16 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)\n",
            "25/08/06 09:15:16 INFO ShuffleBlockFetcherIterator: Getting 1 (194.5 KiB) non-empty blocks including 1 (194.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 09:15:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 27 ms\n",
            "25/08/06 09:15:16 INFO CodeGenerator: Code generated in 35.433781 ms\n",
            "25/08/06 09:15:16 INFO CodeGenerator: Code generated in 23.872675 ms\n",
            "25/08/06 09:15:16 INFO CodeGenerator: Code generated in 27.349948 ms\n",
            "25/08/06 09:15:16 INFO CodeGenerator: Code generated in 29.6015 ms\n",
            "25/08/06 09:15:17 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 95a0e3787e8d:42569 in memory (size: 23.6 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:15:17 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 5718 bytes result sent to driver\n",
            "25/08/06 09:15:17 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 632 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:15:17 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:15:17 INFO DAGScheduler: ShuffleMapStage 5 (show at ProductDemandAnalysis.java:27) finished in 0.681 s\n",
            "25/08/06 09:15:17 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 09:15:17 INFO DAGScheduler: running: Set()\n",
            "25/08/06 09:15:17 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 09:15:17 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 09:15:17 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 09:15:17 INFO CodeGenerator: Code generated in 31.771531 ms\n",
            "25/08/06 09:15:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 09:15:17 INFO CodeGenerator: Code generated in 63.200592 ms\n",
            "25/08/06 09:15:17 INFO SparkContext: Starting job: show at ProductDemandAnalysis.java:27\n",
            "25/08/06 09:15:17 INFO DAGScheduler: Got job 5 (show at ProductDemandAnalysis.java:27) with 1 output partitions\n",
            "25/08/06 09:15:17 INFO DAGScheduler: Final stage: ResultStage 8 (show at ProductDemandAnalysis.java:27)\n",
            "25/08/06 09:15:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
            "25/08/06 09:15:17 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:15:17 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[18] at show at ProductDemandAnalysis.java:27), which has no missing parents\n",
            "25/08/06 09:15:17 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 63.7 KiB, free 417.8 MiB)\n",
            "25/08/06 09:15:17 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 26.3 KiB, free 417.7 MiB)\n",
            "25/08/06 09:15:17 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 95a0e3787e8d:42569 (size: 26.3 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:15:17 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:15:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[18] at show at ProductDemandAnalysis.java:27) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:15:17 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:15:17 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (95a0e3787e8d, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:15:17 INFO Executor: Running task 0.0 in stage 8.0 (TID 5)\n",
            "25/08/06 09:15:17 INFO ShuffleBlockFetcherIterator: Getting 1 (12.4 KiB) non-empty blocks including 1 (12.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 09:15:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms\n",
            "25/08/06 09:15:17 INFO Executor: Finished task 0.0 in stage 8.0 (TID 5). 10899 bytes result sent to driver\n",
            "25/08/06 09:15:17 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 142 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:15:17 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:15:17 INFO DAGScheduler: ResultStage 8 (show at ProductDemandAnalysis.java:27) finished in 0.176 s\n",
            "25/08/06 09:15:17 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:15:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
            "25/08/06 09:15:17 INFO DAGScheduler: Job 5 finished: show at ProductDemandAnalysis.java:27, took 0.200371 s\n",
            "25/08/06 09:15:17 INFO CodeGenerator: Code generated in 16.901794 ms\n",
            "25/08/06 09:15:17 INFO CodeGenerator: Code generated in 25.68976 ms\n",
            "+-----------+---------------------------------------+----------+---------------+----------------+\n",
            "|productCode|productName                            |orderCount|totalOrderedQty|averagePriceEach|\n",
            "+-----------+---------------------------------------+----------+---------------+----------------+\n",
            "|S18_3232   |1992 Ferrari 360 Spider red            |53        |1808.0         |152.34          |\n",
            "|S18_1342   |1937 Lincoln Berline                   |28        |1111.0         |92.1            |\n",
            "|S700_4002  |American Airlines: MD-11S              |28        |1085.0         |65.97           |\n",
            "|S18_3856   |1941 Chevrolet Special Deluxe Cabriolet|28        |1076.0         |95.77           |\n",
            "|S50_1341   |1930 Buick Marquette Phaeton           |28        |1074.0         |38.79           |\n",
            "|S18_4600   |1940s Ford truck                       |28        |1061.0         |107.98          |\n",
            "|S10_1678   |1969 Harley Davidson Ultimate Chopper  |28        |1057.0         |85.17           |\n",
            "|S12_4473   |1957 Chevy Pickup                      |28        |1056.0         |103.9           |\n",
            "|S18_2319   |1964 Mercedes Tour Bus                 |28        |1053.0         |111.16          |\n",
            "|S24_3856   |1956 Porsche 356A Coupe                |27        |1052.0         |128.26          |\n",
            "|S24_3949   |Corsair F4U ( Bird Cage)               |28        |1051.0         |59.71           |\n",
            "|S700_3167  |F/A 18 Hornet 1/72                     |28        |1047.0         |73.03           |\n",
            "|S18_1662   |1980s Black Hawk Helicopter            |28        |1040.0         |139.11          |\n",
            "|S18_2949   |1913 Ford Model T Speedster            |28        |1038.0         |93.42           |\n",
            "|S24_1578   |1997 BMW R 1100 S                      |28        |1033.0         |101.95          |\n",
            "|S10_4757   |1972 Alfa Romeo GTA                    |28        |1030.0         |124.25          |\n",
            "|S24_2300   |1962 Volkswagen Microbus               |28        |1029.0         |115.28          |\n",
            "|S12_2823   |2002 Suzuki XREO                       |28        |1028.0         |132.17          |\n",
            "|S700_2610  |The USS Constitution Ship              |27        |1020.0         |65.43           |\n",
            "|S12_1108   |2001 Ferrari Enzo                      |27        |1019.0         |187.1           |\n",
            "+-----------+---------------------------------------+----------+---------------+----------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "25/08/06 09:15:18 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 09:15:18 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1)\n",
            "25/08/06 09:15:18 INFO FileSourceStrategy: Output Data Schema: struct<orderNumber: string, productCode: string, quantityOrdered: string, priceEach: string ... 2 more fields>\n",
            "25/08/06 09:15:18 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 09:15:18 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#10)\n",
            "25/08/06 09:15:18 INFO FileSourceStrategy: Output Data Schema: struct<productCode: string, productName: string>\n",
            "25/08/06 09:15:18 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:15:18 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 201.3 KiB, free 417.5 MiB)\n",
            "25/08/06 09:15:18 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 417.5 MiB)\n",
            "25/08/06 09:15:18 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 95a0e3787e8d:42569 (size: 34.8 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:15:18 INFO SparkContext: Created broadcast 9 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:15:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4210971 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:15:18 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:15:18 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 09:15:18 INFO DAGScheduler: Final stage: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 09:15:18 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:15:18 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:15:18 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[22] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 09:15:18 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 13.6 KiB, free 417.5 MiB)\n",
            "25/08/06 09:15:18 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 417.5 MiB)\n",
            "25/08/06 09:15:18 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 95a0e3787e8d:42569 (size: 6.1 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:15:18 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:15:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[22] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:15:18 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:15:18 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 6) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4976 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:15:18 INFO Executor: Running task 0.0 in stage 9.0 (TID 6)\n",
            "25/08/06 09:15:18 INFO FileScanRDD: Reading File path: file:///content/data/parquet/products/part-00000-6f591ce5-8ce6-4705-a661-9754a49f5377-c000.snappy.parquet, range: 0-16667, partition values: [empty row]\n",
            "25/08/06 09:15:18 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 09:15:18 INFO Executor: Finished task 0.0 in stage 9.0 (TID 6). 5350 bytes result sent to driver\n",
            "25/08/06 09:15:18 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 6) in 49 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:15:18 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:15:18 INFO DAGScheduler: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.069 s\n",
            "25/08/06 09:15:18 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:15:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "25/08/06 09:15:18 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.076437 s\n",
            "25/08/06 09:15:18 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 16.0 MiB, free 401.5 MiB)\n",
            "25/08/06 09:15:18 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 401.5 MiB)\n",
            "25/08/06 09:15:18 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 95a0e3787e8d:42569 (size: 4.7 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:15:18 INFO SparkContext: Created broadcast 11 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 09:15:18 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 09:15:18 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#1)\n",
            "25/08/06 09:15:18 INFO FileSourceStrategy: Output Data Schema: struct<orderNumber: string, productCode: string, quantityOrdered: string, priceEach: string ... 2 more fields>\n",
            "25/08/06 09:15:18 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.5 KiB, free 401.3 MiB)\n",
            "25/08/06 09:15:18 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 401.3 MiB)\n",
            "25/08/06 09:15:18 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 95a0e3787e8d:42569 (size: 34.8 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:15:18 INFO SparkContext: Created broadcast 12 from parquet at ProductDemandAnalysis.java:29\n",
            "25/08/06 09:15:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4219095 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 09:15:18 INFO DAGScheduler: Registering RDD 26 (parquet at ProductDemandAnalysis.java:29) as input to shuffle 2\n",
            "25/08/06 09:15:18 INFO DAGScheduler: Got map stage job 7 (parquet at ProductDemandAnalysis.java:29) with 1 output partitions\n",
            "25/08/06 09:15:18 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (parquet at ProductDemandAnalysis.java:29)\n",
            "25/08/06 09:15:18 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 09:15:18 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:15:18 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[26] at parquet at ProductDemandAnalysis.java:29), which has no missing parents\n",
            "25/08/06 09:15:18 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 57.8 KiB, free 401.2 MiB)\n",
            "25/08/06 09:15:18 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 23.8 KiB, free 401.2 MiB)\n",
            "25/08/06 09:15:18 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 95a0e3787e8d:42569 (size: 23.8 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:15:18 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:15:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[26] at parquet at ProductDemandAnalysis.java:29) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:15:18 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:15:18 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 7) (95a0e3787e8d, executor driver, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:15:18 INFO Executor: Running task 0.0 in stage 10.0 (TID 7)\n",
            "25/08/06 09:15:18 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails/part-00000-d0518666-5369-4332-9ddc-d7a9dd010872-c000.snappy.parquet, range: 0-24791, partition values: [empty row]\n",
            "25/08/06 09:15:18 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 09:15:18 INFO Executor: Finished task 0.0 in stage 10.0 (TID 7). 3931 bytes result sent to driver\n",
            "25/08/06 09:15:18 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 7) in 238 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:15:18 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:15:18 INFO DAGScheduler: ShuffleMapStage 10 (parquet at ProductDemandAnalysis.java:29) finished in 0.252 s\n",
            "25/08/06 09:15:18 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 09:15:18 INFO DAGScheduler: running: Set()\n",
            "25/08/06 09:15:18 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 09:15:18 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 09:15:18 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 09:15:19 INFO DAGScheduler: Registering RDD 29 (parquet at ProductDemandAnalysis.java:29) as input to shuffle 3\n",
            "25/08/06 09:15:19 INFO DAGScheduler: Got map stage job 8 (parquet at ProductDemandAnalysis.java:29) with 1 output partitions\n",
            "25/08/06 09:15:19 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (parquet at ProductDemandAnalysis.java:29)\n",
            "25/08/06 09:15:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
            "25/08/06 09:15:19 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:15:19 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[29] at parquet at ProductDemandAnalysis.java:29), which has no missing parents\n",
            "25/08/06 09:15:19 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 75.4 KiB, free 401.1 MiB)\n",
            "25/08/06 09:15:19 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 28.9 KiB, free 401.1 MiB)\n",
            "25/08/06 09:15:19 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 95a0e3787e8d:42569 (size: 28.9 KiB, free: 434.1 MiB)\n",
            "25/08/06 09:15:19 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:15:19 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[29] at parquet at ProductDemandAnalysis.java:29) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:15:19 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:15:19 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 8) (95a0e3787e8d, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:15:19 INFO Executor: Running task 0.0 in stage 12.0 (TID 8)\n",
            "25/08/06 09:15:19 INFO ShuffleBlockFetcherIterator: Getting 1 (194.5 KiB) non-empty blocks including 1 (194.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 09:15:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\n",
            "25/08/06 09:15:19 INFO Executor: Finished task 0.0 in stage 12.0 (TID 8). 5718 bytes result sent to driver\n",
            "25/08/06 09:15:19 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 8) in 183 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:15:19 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:15:19 INFO DAGScheduler: ShuffleMapStage 12 (parquet at ProductDemandAnalysis.java:29) finished in 0.201 s\n",
            "25/08/06 09:15:19 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 09:15:19 INFO DAGScheduler: running: Set()\n",
            "25/08/06 09:15:19 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 09:15:19 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 09:15:19 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 09:15:19 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 09:15:19 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 95a0e3787e8d:42569 in memory (size: 28.7 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:15:19 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 95a0e3787e8d:42569 in memory (size: 34.8 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:15:19 INFO CodeGenerator: Code generated in 48.331265 ms\n",
            "25/08/06 09:15:19 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 95a0e3787e8d:42569 in memory (size: 23.8 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:15:19 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 95a0e3787e8d:42569 in memory (size: 34.8 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:15:19 INFO SparkContext: Starting job: parquet at ProductDemandAnalysis.java:29\n",
            "25/08/06 09:15:19 INFO DAGScheduler: Got job 9 (parquet at ProductDemandAnalysis.java:29) with 1 output partitions\n",
            "25/08/06 09:15:19 INFO DAGScheduler: Final stage: ResultStage 15 (parquet at ProductDemandAnalysis.java:29)\n",
            "25/08/06 09:15:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\n",
            "25/08/06 09:15:19 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:15:19 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[34] at parquet at ProductDemandAnalysis.java:29), which has no missing parents\n",
            "25/08/06 09:15:19 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 95a0e3787e8d:42569 in memory (size: 26.3 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:15:19 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 64.9 KiB, free 401.6 MiB)\n",
            "25/08/06 09:15:19 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 26.7 KiB, free 401.7 MiB)\n",
            "25/08/06 09:15:19 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 95a0e3787e8d:42569 (size: 26.7 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:15:19 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:15:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[34] at parquet at ProductDemandAnalysis.java:29) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:15:19 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:15:19 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 9) (95a0e3787e8d, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:15:19 INFO Executor: Running task 0.0 in stage 15.0 (TID 9)\n",
            "25/08/06 09:15:19 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 95a0e3787e8d:42569 in memory (size: 28.9 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:15:19 INFO ShuffleBlockFetcherIterator: Getting 1 (12.4 KiB) non-empty blocks including 1 (12.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 09:15:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms\n",
            "25/08/06 09:15:19 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 95a0e3787e8d:42569 in memory (size: 6.1 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:15:19 INFO CodeGenerator: Code generated in 16.49637 ms\n",
            "25/08/06 09:15:19 INFO Executor: Finished task 0.0 in stage 15.0 (TID 9). 10158 bytes result sent to driver\n",
            "25/08/06 09:15:19 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 9) in 175 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:15:19 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:15:19 INFO DAGScheduler: ResultStage 15 (parquet at ProductDemandAnalysis.java:29) finished in 0.243 s\n",
            "25/08/06 09:15:19 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:15:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
            "25/08/06 09:15:19 INFO DAGScheduler: Job 9 finished: parquet at ProductDemandAnalysis.java:29, took 0.259690 s\n",
            "25/08/06 09:15:19 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 95a0e3787e8d:42569 in memory (size: 4.7 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:15:20 INFO DAGScheduler: Registering RDD 35 (parquet at ProductDemandAnalysis.java:29) as input to shuffle 4\n",
            "25/08/06 09:15:20 INFO DAGScheduler: Got map stage job 10 (parquet at ProductDemandAnalysis.java:29) with 1 output partitions\n",
            "25/08/06 09:15:20 INFO DAGScheduler: Final stage: ShuffleMapStage 18 (parquet at ProductDemandAnalysis.java:29)\n",
            "25/08/06 09:15:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\n",
            "25/08/06 09:15:20 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:15:20 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[35] at parquet at ProductDemandAnalysis.java:29), which has no missing parents\n",
            "25/08/06 09:15:20 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 67.8 KiB, free 417.8 MiB)\n",
            "25/08/06 09:15:20 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 417.7 MiB)\n",
            "25/08/06 09:15:20 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 95a0e3787e8d:42569 (size: 27.4 KiB, free: 434.3 MiB)\n",
            "25/08/06 09:15:20 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:15:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[35] at parquet at ProductDemandAnalysis.java:29) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:15:20 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:15:20 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 10) (95a0e3787e8d, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:15:20 INFO Executor: Running task 0.0 in stage 18.0 (TID 10)\n",
            "25/08/06 09:15:20 INFO ShuffleBlockFetcherIterator: Getting 1 (12.4 KiB) non-empty blocks including 1 (12.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 09:15:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/06 09:15:20 INFO Executor: Finished task 0.0 in stage 18.0 (TID 10). 6627 bytes result sent to driver\n",
            "25/08/06 09:15:20 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 10) in 105 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:15:20 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:15:20 INFO DAGScheduler: ShuffleMapStage 18 (parquet at ProductDemandAnalysis.java:29) finished in 0.152 s\n",
            "25/08/06 09:15:20 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 09:15:20 INFO DAGScheduler: running: Set()\n",
            "25/08/06 09:15:20 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 09:15:20 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 09:15:20 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 09:15:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:15:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:15:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:15:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:15:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:15:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:15:20 INFO CodeGenerator: Code generated in 22.702408 ms\n",
            "25/08/06 09:15:20 INFO SparkContext: Starting job: parquet at ProductDemandAnalysis.java:29\n",
            "25/08/06 09:15:20 INFO DAGScheduler: Got job 11 (parquet at ProductDemandAnalysis.java:29) with 1 output partitions\n",
            "25/08/06 09:15:20 INFO DAGScheduler: Final stage: ResultStage 22 (parquet at ProductDemandAnalysis.java:29)\n",
            "25/08/06 09:15:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\n",
            "25/08/06 09:15:20 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 09:15:20 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[37] at parquet at ProductDemandAnalysis.java:29), which has no missing parents\n",
            "25/08/06 09:15:20 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 257.1 KiB, free 417.5 MiB)\n",
            "25/08/06 09:15:20 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 94.3 KiB, free 417.4 MiB)\n",
            "25/08/06 09:15:20 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 95a0e3787e8d:42569 (size: 94.3 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:15:20 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\n",
            "25/08/06 09:15:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[37] at parquet at ProductDemandAnalysis.java:29) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 09:15:20 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
            "25/08/06 09:15:20 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 95a0e3787e8d:42569 in memory (size: 27.4 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:15:20 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 11) (95a0e3787e8d, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "25/08/06 09:15:20 INFO Executor: Running task 0.0 in stage 22.0 (TID 11)\n",
            "25/08/06 09:15:20 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 95a0e3787e8d:42569 in memory (size: 26.7 KiB, free: 434.2 MiB)\n",
            "25/08/06 09:15:20 INFO ShuffleBlockFetcherIterator: Getting 1 (12.5 KiB) non-empty blocks including 1 (12.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 09:15:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
            "25/08/06 09:15:20 INFO CodeGenerator: Code generated in 15.035405 ms\n",
            "25/08/06 09:15:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:15:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:15:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:15:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 09:15:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 09:15:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 09:15:20 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 09:15:20 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 09:15:20 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
            "25/08/06 09:15:20 INFO ParquetOutputFormat: Validation is off\n",
            "25/08/06 09:15:20 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "25/08/06 09:15:20 INFO ParquetOutputFormat: Parquet properties are:\n",
            "Parquet page size to 1048576\n",
            "Parquet dictionary page size to 1048576\n",
            "Dictionary is true\n",
            "Writer version is: PARQUET_1_0\n",
            "Page size checking is: estimated\n",
            "Min row count for page size check is: 100\n",
            "Max row count for page size check is: 10000\n",
            "Truncate length for column indexes is: 64\n",
            "Truncate length for statistics min/max  is: 2147483647\n",
            "Bloom filter enabled: false\n",
            "Max Bloom filter size for a column is 1048576\n",
            "Bloom filter expected number of distinct values are: null\n",
            "Page row count limit to 20000\n",
            "Writing page checksums is: on\n",
            "25/08/06 09:15:20 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productCode\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"productName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"orderCount\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : false,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalOrderedQty\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"averagePriceEach\",\n",
            "    \"type\" : \"double\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productCode (STRING);\n",
            "  optional binary productName (STRING);\n",
            "  required int64 orderCount;\n",
            "  optional double totalOrderedQty;\n",
            "  optional double averagePriceEach;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 09:15:20 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 09:15:21 INFO FileOutputCommitter: Saved output of task 'attempt_202508060915205649218880138222895_0022_m_000000_11' to file:/content/output/processed/product_demand_summary.parquet/_temporary/0/task_202508060915205649218880138222895_0022_m_000000\n",
            "25/08/06 09:15:21 INFO SparkHadoopMapRedUtil: attempt_202508060915205649218880138222895_0022_m_000000_11: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 09:15:21 INFO Executor: Finished task 0.0 in stage 22.0 (TID 11). 8324 bytes result sent to driver\n",
            "25/08/06 09:15:21 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 11) in 614 ms on 95a0e3787e8d (executor driver) (1/1)\n",
            "25/08/06 09:15:21 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
            "25/08/06 09:15:21 INFO DAGScheduler: ResultStage 22 (parquet at ProductDemandAnalysis.java:29) finished in 0.709 s\n",
            "25/08/06 09:15:21 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 09:15:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
            "25/08/06 09:15:21 INFO DAGScheduler: Job 11 finished: parquet at ProductDemandAnalysis.java:29, took 0.726379 s\n",
            "25/08/06 09:15:21 INFO FileFormatWriter: Start to commit write Job f314b364-c7c5-43c1-bfd1-ad39a3f15637.\n",
            "25/08/06 09:15:21 INFO FileFormatWriter: Write Job f314b364-c7c5-43c1-bfd1-ad39a3f15637 committed. Elapsed time: 47 ms.\n",
            "25/08/06 09:15:21 INFO FileFormatWriter: Finished processing stats for write job f314b364-c7c5-43c1-bfd1-ad39a3f15637.\n",
            "25/08/06 09:15:21 INFO SparkUI: Stopped Spark web UI at http://95a0e3787e8d:4041\n",
            "25/08/06 09:15:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 09:15:21 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 09:15:21 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 09:15:21 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 09:15:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 09:15:21 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 09:15:21 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 09:15:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-f4bf516e-a02b-4b8b-933b-58e637ef4e46\n",
            "25/08/06 09:15:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-67385ac5-3bc8-4c1a-8c58-f1d2b7dcb65c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"VerifyDemand\").getOrCreate()\n",
        "\n",
        "spark.read.parquet(\"/content/output/processed/product_demand_summary.parquet\") \\\n",
        "     .select(\"productCode\",\"productName\",\"orderCount\",\"totalOrderedQty\",\"averagePriceEach\") \\\n",
        "     .show(20, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHthtD-BqXTJ",
        "outputId": "2ebd9eef-0802-48ab-aa64-bb873e9c4833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------------------------------+----------+---------------+----------------+\n",
            "|productCode|productName                            |orderCount|totalOrderedQty|averagePriceEach|\n",
            "+-----------+---------------------------------------+----------+---------------+----------------+\n",
            "|S18_3232   |1992 Ferrari 360 Spider red            |53        |1808.0         |152.34          |\n",
            "|S18_1342   |1937 Lincoln Berline                   |28        |1111.0         |92.1            |\n",
            "|S700_4002  |American Airlines: MD-11S              |28        |1085.0         |65.97           |\n",
            "|S18_3856   |1941 Chevrolet Special Deluxe Cabriolet|28        |1076.0         |95.77           |\n",
            "|S50_1341   |1930 Buick Marquette Phaeton           |28        |1074.0         |38.79           |\n",
            "|S18_4600   |1940s Ford truck                       |28        |1061.0         |107.98          |\n",
            "|S10_1678   |1969 Harley Davidson Ultimate Chopper  |28        |1057.0         |85.17           |\n",
            "|S12_4473   |1957 Chevy Pickup                      |28        |1056.0         |103.9           |\n",
            "|S18_2319   |1964 Mercedes Tour Bus                 |28        |1053.0         |111.16          |\n",
            "|S24_3856   |1956 Porsche 356A Coupe                |27        |1052.0         |128.26          |\n",
            "|S24_3949   |Corsair F4U ( Bird Cage)               |28        |1051.0         |59.71           |\n",
            "|S700_3167  |F/A 18 Hornet 1/72                     |28        |1047.0         |73.03           |\n",
            "|S18_1662   |1980s Black Hawk Helicopter            |28        |1040.0         |139.11          |\n",
            "|S18_2949   |1913 Ford Model T Speedster            |28        |1038.0         |93.42           |\n",
            "|S24_1578   |1997 BMW R 1100 S                      |28        |1033.0         |101.95          |\n",
            "|S10_4757   |1972 Alfa Romeo GTA                    |28        |1030.0         |124.25          |\n",
            "|S24_2300   |1962 Volkswagen Microbus               |28        |1029.0         |115.28          |\n",
            "|S12_2823   |2002 Suzuki XREO                       |28        |1028.0         |132.17          |\n",
            "|S700_2610  |The USS Constitution Ship              |27        |1020.0         |65.43           |\n",
            "|S12_1108   |2001 Ferrari Enzo                      |27        |1019.0         |187.1           |\n",
            "+-----------+---------------------------------------+----------+---------------+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}